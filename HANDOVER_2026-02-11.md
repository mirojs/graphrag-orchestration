# Handover — February 11, 2026

## Where We Are

### Route 2: Skeleton Enrichment — COMPLETE & DEPLOYED

The full sentence-level hybrid skeleton pipeline is live in production:

| Component | Status | Commit |
|---|---|---|
| Sentence extraction (spaCy) | ✅ Deployed | `8c13faf1` |
| Voyage-context-3 sentence embeddings | ✅ Deployed | `8c13faf1` |
| Neo4j Sentence nodes + vector index | ✅ Deployed | `8c13faf1` |
| RELATED_TO sentence edges (0.90/k=2) | ✅ Deployed | `5fcf14b3` |
| Strategy B graph traversal retrieval | ✅ Deployed | `4638a69f` |
| gpt-4.1-mini synthesis model | ✅ Deployed | `24c938c4` |
| Metadata tag cleanup (strip_skeleton_tags) | ✅ Deployed | `a71efb15` |
| Phase 3 reranker | ❌ Not needed | `97415141` (proof) |

**Production image:** `a71efb15-39` on `graphrag-api.salmonhill-df6033f3.swedencentral.azurecontainerapps.io`

**Key results:**
- Strategy B: +178% F1, +39% containment vs baseline
- gpt-4.1-mini: 11W/1L/5T vs gpt-5.1, +29% F1, ~10× cheaper
- All answers at vector search rank #1 — no reranker needed
- 5/5 production queries clean (zero metadata leakage)

### Route 3: Global Search — NO sentence enrichment yet

Route 3 uses a completely different retrieval pipeline:
1. Community matching → hub entity extraction
2. MENTIONS-based chunk fetch + BM25+Vector RRF hybrid
3. PPR scoring for chunk re-ranking (D3 distillation)
4. Token budget enforcement (D4)
5. Synthesis via `synthesize_with_graph_context()` (gpt-5.1)

**Existing Route 3 analysis files (read these first):**
- `ANALYSIS_ROUTE3_ARCHITECTURE_DEEP_DIVE_2026-02-10.md` — definitive architecture review, 6 design flaws identified
- `ANALYSIS_ROUTE3_CONTEXT_VS_OUTPUT_2026-02-10.md` — retrieval gaps (truncated tables, missing clauses)
- `ANALYSIS_ROUTE3_SYNTHESIS_LLM_COMPARISON_2026-02-08.md` — gpt-5.1 wins with 100% theme coverage

---

## Tomorrow's Work: Route 3 Sentence-Level Enrichment

### Methodology (same as Route 2)

```
Framework-level weakness → Process-step-level weakness → Resolve → Update code
```

### TODO List

#### Phase 1: Spot Framework-Level Weaknesses

- [ ] **1.1 Run Route 3 benchmark on test corpus (test-5pdfs-v2-fix2)**
  - Use same 17 questions (Q-V + Q-L) that benchmarked Route 2
  - Record baseline F1, containment, latency for Route 3
  - Script: adapt `scripts/benchmark_strategy_ab.py` for Route 3

- [ ] **1.2 Identify which questions Route 3 fails on**
  - Compare Route 3 vs Route 2 (now with skeleton) head-to-head
  - Classify failures: retrieval gap? synthesis gap? coverage gap?
  - Focus on questions where Route 2+skeleton wins but Route 3 loses

- [ ] **1.3 Analyze Route 3 context quality**
  - For failed questions, inspect `llm_context` (use `include_context: true`)
  - Check: is the answer-bearing text in the retrieved chunks at all?
  - If yes → synthesis problem. If no → retrieval problem.
  - Reference: `ANALYSIS_ROUTE3_CONTEXT_VS_OUTPUT_2026-02-10.md` already found gaps

#### Phase 2: Spot Process-Step-Level Weaknesses

- [ ] **2.1 Map each Route 3 stage to its failure mode**
  - Stage 3.1 (Community matching): are the right communities selected?
  - Stage 3.2 (Hub entities): are answer-bearing entities in the hub set?
  - Stage 3.3 (MENTIONS fetch): are the right chunks retrieved?
  - Stage 3.3.5 (BM25+Vector RRF): does hybrid search find what community seeding misses?
  - Stage 3.4 (PPR scoring): does re-ranking help or hurt?
  - Stage 3.5 (Synthesis): does gpt-5.1 extract answers from context?

- [ ] **2.2 Trace specific failures end-to-end**
  - Pick 3-5 worst Route 3 questions
  - For each: log community matches → hub entities → chunk IDs → synthesis input → output
  - Identify the exact stage where the answer gets lost

- [ ] **2.3 Check if sentence-level retrieval would help**
  - For questions where Route 3 retrieves the right *chunk* but wrong *answer*:
    sentence-level precision would help
  - For questions where Route 3 misses the *chunk* entirely:
    need to fix upstream (community matching or RRF) first
  - This determines whether skeleton enrichment is the right fix for Route 3

#### Phase 3: Resolve — Implement Sentence Enrichment for Route 3

- [ ] **3.1 Design the injection point**
  - Route 3's synthesis uses `synthesize_with_graph_context()` (different from Route 2's `synthesize()`)
  - Coverage chunks go through the same `coverage_chunks` parameter
  - Decision: inject skeleton chunks as coverage_chunks (like Route 2) or as a new stage?
  - Key file: `src/worker/hybrid_v2/routes/route_3_global.py`

- [ ] **3.2 Add skeleton enrichment stage to Route 3**
  - Add Stage 3.3.7 (or similar): skeleton sentence retrieval
  - Reuse `_retrieve_skeleton_sentences()` / `_retrieve_skeleton_graph_traversal()` from Route 2
  - Consider: should Route 3 use Strategy A (flat) or B (graph traversal)?
  - Wire into `skeleton_coverage_chunks` → pass to synthesizer

- [ ] **3.3 Handle the model question**
  - Route 3 uses gpt-5.1 for synthesis (100% theme coverage in benchmarks)
  - Route 3 queries are thematic/cross-document — may need gpt-5.1's reasoning
  - Test: does gpt-4.1-mini work for Route 3? (probably not — Route 3 is synthesis-heavy, not extraction)
  - May need different model strategy per route

- [ ] **3.4 Handle metadata tag stripping**
  - `strip_skeleton_tags()` already runs in `synthesize()` — need to verify it also runs in `synthesize_with_graph_context()`
  - If not, add it there too

#### Phase 4: Test & Deploy

- [ ] **4.1 Benchmark Route 3 + skeleton vs baseline Route 3**
  - Same 17 questions, measure F1 / containment / latency
  - Head-to-head comparison

- [ ] **4.2 Benchmark Route 3 + skeleton vs Route 2 + skeleton**
  - Are they converging? Or do they serve different query types?

- [ ] **4.3 Deploy if results are positive**
  - Same deploy flow: `deploy-graphrag.sh`
  - No new env vars needed (reuses SKELETON_ENRICHMENT_ENABLED)

- [ ] **4.4 Update architecture doc with Route 3 results**

---

## Key Files for Tomorrow

| File | Purpose |
|---|---|
| `src/worker/hybrid_v2/routes/route_3_global.py` | Route 3 handler — main edit target |
| `src/worker/hybrid_v2/pipeline/synthesis.py` | `synthesize_with_graph_context()` — distillation + synthesis |
| `src/worker/hybrid_v2/routes/route_2_local.py` | Reference: how skeleton enrichment was added to Route 2 |
| `src/worker/hybrid_v2/services/neo4j_store.py` | Sentence query functions (reuse for Route 3) |
| `ANALYSIS_ROUTE3_ARCHITECTURE_DEEP_DIVE_2026-02-10.md` | Pre-existing analysis of Route 3 weaknesses |
| `ANALYSIS_ROUTE3_CONTEXT_VS_OUTPUT_2026-02-10.md` | Known retrieval gaps |
| `ARCHITECTURE_HYBRID_SKELETON_2026-02-11.md` | Architecture doc with benchmark results |
| `scripts/benchmark_strategy_ab.py` | Benchmark template to adapt for Route 3 |
| `scripts/benchmark_synthesis_model.py` | Model comparison template |

## Critical Difference: Route 3 vs Route 2

Route 2 is **extraction-focused** — find the specific sentence, extract the answer. Sentence-level chunking was a perfect fit.

Route 3 is **synthesis-focused** — gather broad thematic context across documents, synthesize a report. Sentence-level precision may help with *specific facts within themes* but the primary weakness may be at the community/hub selection level, not the chunk level.

**Start with Phase 1 (benchmark + diagnosis) before assuming sentence enrichment is the right fix for Route 3.**
