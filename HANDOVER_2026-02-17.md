# Session Handover — 2026-02-17

**Last commit:** `5df6fd2a` on `main` (pushed)  
**Uncommitted change:** `scripts/benchmark_route3_global_search.py` — added `BENCH_SKIP_BEFORE` env var support  
**Session focus:** Fix Q-G10 PPR zero-result crash, investigate Q-G1 missing "deposit"/"forfeited" themes  

---

## What Was Completed Today

### 1. Q-G10 PPR Zero-Results Bug — Fixed & Deployed ✅

**Problem:** Q-G10 ("Summarize each document's main purpose in one sentence") returned 0 PPR entities and 0 chunks_raw in the deployed service, despite the PPR Cypher working fine locally.

**Root cause:** `logger.info("ppr_seeds_capped", original=n_seeds, ...)` in `async_neo4j_service.py` used structlog-style keyword args with stdlib `logging.getLogger(__name__)`. When Tier 2 produced 56 seeds > 25 cap, the log call threw `Logger._log() got an unexpected keyword argument 'original'`, caught silently by the try/except in route_5, producing 0 PPR evidence.

**Fix (commit `596bb5e4`):** Converted all 10 structlog-style logger calls to `%s` format across 4 files:
- `src/worker/services/async_neo4j_service.py` — 7 fixes (PPR seed-capping, adaptive limits, PPR complete, negative detection, entity doc coverage, semantic beam)
- `src/worker/hybrid_v2/services/neo4j_store.py` — 1 fix
- `src/worker/services/ingestion_service.py` — 1 fix
- `src/worker/services/vector_service.py` — 1 fix

### 2. Q-G10 Doc Group Pruning — Fixed & Deployed ✅

**Problem:** After the logger fix, PPR returned 30 entities but the Invoice doc was still missing. `doc_group_pruning` removed it (score 0.0094 vs top doc 0.6285, ratio 0.015 < 0.25 threshold).

**Fix (commit `10f75e28`):** In `_build_cited_context()` in `src/worker/hybrid_v2/pipeline/synthesis.py`, added cross-document query detection via regex (e.g., `(?:all|every|each)\s+(?:of\s+the\s+)?(?:documents?|...)`) that bypasses `doc_group_pruning` for those queries.

### 3. Benchmark Results After Fixes ✅

| Question | Before | After | Notes |
|----------|--------|-------|-------|
| Q-G10 | 83% (5/6) | **100% (6/6)** | Invoice doc now included |
| Q-G9 | 67% (4/6) | **83% (5/6)** | Collateral improvement |
| Q-G5 | 83% (5/6) | 83% (5/6) | Stable |
| Q-G6 | 100% | 100% | Stable |
| Q-G7 | 80% (4/5) | 80% (4/5) | Stable (slight LLM variance) |
| Q-G8 | 100% | 100% | Stable |
| All 9 negatives | PASS | PASS | Stable |

### 4. Q-G1 Investigation — Root Cause Found ⚠️

**Problem:** Q-G1 ("Across the agreements, list the termination/cancellation rules you can find") hits only 5/7 themes. Missing: **"deposit"** and **"forfeited"**.

**Source text (purchase contract):**
> "Customer may cancel within 3 business days for full refund. Afterward, deposit is forfeited."

**Root cause: Sentence extraction drops "Afterward, deposit is forfeited."**

The sentence splitter (spaCy `doc.sents`) correctly splits at the period after "refund." The second sentence "Afterward, deposit is forfeited." is only **4 words** — it fails the `SKELETON_MIN_SENTENCE_WORDS = 5` filter in `_is_noise_sentence()`.

| Filter | Value | Threshold | Result |
|--------|-------|-----------|--------|
| `min_chars` | 32 | 30 | PASS |
| `min_words` | 4 | 5 | **FAIL** ← dropped |
| alpha chars | 27 | 10 | PASS |

**Key code:**
- Filter function: `src/worker/services/sentence_extraction_service.py` line 82 (`_is_noise_sentence()`)
- Config: `src/core/config.py` line 75 (`SKELETON_MIN_SENTENCE_WORDS: int = 5`)
- The parent TextChunk DOES contain the full text including "Afterward, deposit is forfeited." — it's only the Sentence node that's missing

---

## What Is NOT Done — Todo List

### Immediate (Tomorrow Morning)

1. **Fix the min-word filter for Q-G1** — Options:
   - **Option A (simplest):** Lower `SKELETON_MIN_SENTENCE_WORDS` from 5 → 3 in `src/core/config.py`. Risk: may let in some noisy short fragments. Check by running extraction on all 5 PDFs to see how many extra sentences this admits.
   - **Option B (precise):** Make `_is_noise_sentence()` smarter — e.g., if a short sentence contains semantically meaningful words (not just fragments/headers), keep it. Could combine word count with alpha-char ratio.
   - **Option C (merge):** Instead of dropping short sentences, merge them with the preceding sentence. "Customer may cancel within 3 business days for full refund. Afterward, deposit is forfeited." becomes one sentence node.

2. **Re-ingest after fix** — After changing the filter, re-run sentence extraction for `test-5pdfs-v2-fix2`:
   - Delete existing Sentence nodes: `MATCH (s:Sentence {group_id: 'test-5pdfs-v2-fix2'}) DETACH DELETE s`
   - Re-run ingestion or targeted sentence extraction script
   - Verify "Afterward, deposit is forfeited." appears as a Sentence node

3. **Re-benchmark Q-G1** — Run:
   ```bash
   BENCH_SKIP_BEFORE=Q-G1 python scripts/benchmark_route3_global_search.py \
     --force-route unified_search --include-context
   ```
   Check if Q-G1 improves from 5/7 → 6/7 or 7/7.

4. **Commit benchmark script change** — `scripts/benchmark_route3_global_search.py` has uncommitted `BENCH_SKIP_BEFORE` env var addition.

### Next Priority

5. **Audit other dropped sentences** — Run the extraction pipeline on all 5 PDFs and check how many sentences of 3–4 words would be admitted by lowering the threshold. Evaluate whether any are genuine noise vs. valuable content.

6. **Check other failing benchmarks** — Q-G5 (83%) and Q-G7 (80%) still have gaps. Investigate whether the same sentence-drop issue affects those, or if the cause is different (entity resolution, PPR coverage, synthesis LLM).

7. **Re-run decomposition experiment** (carried from 2026-02-15) — The 2×2 experiment (F/Fs/G/Gs) must be re-run with the fixed vector index. Previous Fs/Gs results are invalid. File: `scripts/experiment_decomposition.py`.

8. **Document structure as seed source** (carried from 2026-02-15) — Use section title embeddings as additional PPR seeds.

---

## Key Files

| File | Purpose |
|---|---|
| `src/worker/services/sentence_extraction_service.py` | Sentence splitting with spaCy, noise filtering (`_is_noise_sentence()`) |
| `src/core/config.py` | `SKELETON_MIN_SENTENCE_WORDS = 5` (line 75) — the filter that drops the sentence |
| `src/worker/services/async_neo4j_service.py` | PPR logic, entity queries — logger fixes applied today |
| `src/worker/hybrid_v2/pipeline/synthesis.py` | `_build_cited_context()` — doc_group_pruning bypass added today |
| `src/worker/hybrid_v2/services/neo4j_store.py` | `upsert_sentences_batch()` — creates Sentence nodes in Neo4j |
| `scripts/benchmark_route3_global_search.py` | Benchmark script with `BENCH_SKIP_BEFORE` support (uncommitted) |

## Neo4j State

- **Group:** `test-5pdfs-v2-fix2` (368 Entity nodes, 12 Section nodes)
- **Vector index:** `entity_embedding_v2` — 368 entities (other groups archived as `:EntityArchived`)
- **Missing Sentence:** "Afterward, deposit is forfeited." — not in graph due to 5-word minimum filter
- **URI:** `neo4j+s://a86dcf63.databases.neo4j.io`

## Key Insights

1. **stdlib logger ≠ structlog** — All `logger.info("msg", key=val)` calls must use `logger.info("msg key=%s", val)` format. Applies everywhere in the codebase.
2. **Cross-doc queries need special handling** — `doc_group_pruning` drops low-scoring docs, but cross-doc summary/comparison queries need all docs. Detection regex in `_build_cited_context()`.
3. **Sentence min-word filter too aggressive** — `SKELETON_MIN_SENTENCE_WORDS = 5` drops genuinely important 4-word sentences like "Afterward, deposit is forfeited." This is a systemic issue that may affect retrieval quality for other queries too.
4. **TextChunks have full text, Sentences don't** — The parent TextChunk contains "Afterward, deposit is forfeited." but sentence-based retrieval can't find it because the Sentence node was never created. This creates a retrieval gap that only affects sentence-level search paths.
