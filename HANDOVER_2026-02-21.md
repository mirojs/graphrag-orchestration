# Handover — 2026-02-21

## Current State

Route 5 Unified Search benchmark: **54/57 (94.7%)** on the 19-question R4 question bank.
Up from **51/57 (89.5%)** at the start of today's session.

All changes are committed and deployed. Branch: `main`. No staged changes.

---

## What Was Done Today

### Bugs Fixed

1. **Missing `source` field in sentence evidence** (commit `fa2e478`)
   - File: `src/worker/hybrid_v2/routes/route_5_unified.py` ~line 735
   - Bug: `_retrieve_sentence_evidence()` Cypher returns `sent.source` but the evidence dict never included it. So `ev.get("source")` in `_denoise_sentences()` always returned `None`, and the `_STRUCTURED_SOURCES` bypass for `source="signature_party"` sentences never fired. Signature block sentences were always hard-removed by the denoiser.
   - Fix: Added `"source": r.get("source", "")` to the evidence dict (1-line fix).

2. **Signature block content not reaching LLM context** (commits `920ae4a`, `c6b576d`, `fca2606`, `fd8cbc3`)
   - File: `src/worker/hybrid_v2/routes/route_5_unified.py` ~lines 886-962, 418-426, 459-467
   - Problem: Purchase contract Exhibit A (signature block with "Contoso Ltd.", "Fabrikam Inc.", date "04/30/2025") was never retrieved for entity-counting queries (Q-D8) because vector search ranks signature text poorly for those query types, and PPR chunk allocation deprioritizes them.
   - Fix: Added `_retrieve_signature_chunks()` method that queries Neo4j directly for TextChunks containing "Authorized Representative" or "Signed this" text. These chunks are merged into `coverage_chunks` alongside sentence evidence before synthesis.
   - Sub-bug found during implementation: The Cypher used `PART_OF` edge but hybrid_v2 schema uses `IN_DOCUMENT` for TextChunk→Document relationships. Fixed in `fd8cbc3`.

3. **Reverted counterproductive entity-tracking prompt** (commit `fa2e478`)
   - File: `src/worker/hybrid_v2/pipeline/synthesis.py`
   - The `_entity_tracking_patterns` regex list, `is_entity_tracking_query` detection, and "IMPORTANT for Entity-Document Tracking" guidance block were causing Q-D8 regressions (hardcoded entity names in the prompt biased the LLM). Removed entirely.

### Commits (chronological)

```
fa2e478  fix(retrieval): pass sentence source field to denoiser
920ae4a  feat(retrieval): add guaranteed signature block retrieval for Route 5
c6b576d  fix(retrieval): fetch parent TextChunks for signature block coverage
fca2606  fix(retrieval): use content-based matching for signature block chunks
fd8cbc3  fix(retrieval): use IN_DOCUMENT edge (not PART_OF) for chunk→document
```

---

## Score Trajectory

| State | Q-D3 | Q-D7 | Q-D8 | Total | Notes |
|-------|------|------|------|-------|-------|
| Start of day (entity prompt active) | 2/3 | 1/3 | 0/3 | 51/57 | Entity prompt hurt Q-D8 |
| + Source fix + prompt revert (`fa2e478`) | — | 3/3 | — | — | Q-D7 fixed |
| + Sentence retrieval (`920ae4a`) | — | — | — | — | Wrong granularity (sentences too short) |
| + TextChunk via PART_OF (`c6b576d`) | — | — | — | — | Wrong edge type |
| + Content-based (`fca2606`) | — | — | — | — | Still had PART_OF bug |
| **Final: + IN_DOCUMENT fix (`fd8cbc3`)** | **2/3** | **3/3** | **1/3** | **54/57** | Exhibit A in 3/3 Q-D8 runs |

---

## LLM Judge Results — Final Benchmark

Source: `benchmarks/route5_unified_r4questions_20260221T110522Z.json`
Eval:   `benchmarks/route5_unified_r4questions_20260221T110522Z.eval.md`

| QID | Score | Notes |
|-----|-------|-------|
| Q-D1 | 3/3 | Emergency defect notification |
| Q-D2 | 3/3 | Confirmed reservations on termination |
| Q-D3 | 2/3 | Time windows — misses 60-day repair window |
| Q-D4 | 3/3 | Insurance limits |
| Q-D5 | 3/3 | Coverage start/end |
| Q-D6 | 3/3 | Price matching |
| Q-D7 | 3/3 | Latest date (was 1/3) |
| Q-D8 | 1/3 | Entity document count (was 0/3) |
| Q-D9 | 3/3 | Fee structures |
| Q-D10 | 3/3 | Risk allocation |
| Q-N1–N10 | 27/27 | All negative tests pass |
| **Total** | **54/57** | **94.7%** |

---

## Remaining Issues — TODO

### 1. Q-D8 still scores 1/3 — LLM miscounts entity appearances

- **Status:** Retrieval solved, synthesis problem remains.
- **Detail:** The Exhibit A signature block (with "Contoso Ltd." and "Fabrikam Inc.") is now in the LLM context for ALL 3 runs. Run 1 correctly answers "tied at 4 each". Runs 2-3 give contradictory reasoning — the LLM says "Contoso Ltd. appears in more" but then counts Fabrikam at 5 and concludes Fabrikam wins.
- **Root cause:** This is a synthesis/reasoning limitation. The LLM has the right data but miscounts across 5 document sections. Possible approaches:
  - Add a structured entity-document matrix to the prompt context
  - Pre-compute entity-to-document mappings in the retrieval layer and inject a summary table
  - Prompt engineering to instruct explicit enumeration before comparison
- **Risk:** Any prompt changes here could affect other questions. Entity-tracking prompt was already tried and reverted because it caused regressions.

### 2. Q-D3 still scores 2/3 — missing 60-day repair window

- **Status:** Stable at 2/3 across sessions. Not a regression.
- **Detail:** The "60-day repair window after defect report" timeframe from the warranty document is consistently missed. The LLM gets most other timeframes correct.
- **Possible approach:** This likely requires either better chunk selection for the warranty document or additional retrieval that specifically targets "repair window" / "defect report" text.

### 3. Unstaged script changes (not blocking)

Two script files have uncommitted modifications:
- `scripts/benchmark_route3_global_search.py` — Adds Route 7 (HippoRAG 2) as a `--force-route` option
- `scripts/evaluate_route4_reasoning.py` — Fixes multi-line ground truth parsing (was only capturing first line of multi-line expected answers)

These are utility improvements; commit when convenient.

### 4. Route 7 (HippoRAG 2) type annotation (not blocking)

- `src/worker/hybrid_v2/routes/route_7_hipporag2.py` has a return type annotation change (`Dict[str, List[Dict]]` -> `List[Dict]`) that was deliberately not staged. Minor cleanup.

---

## Key Files Reference

| File | Purpose |
|------|---------|
| `src/worker/hybrid_v2/routes/route_5_unified.py` | Route 5 main pipeline — PPR + sentence vector search + signature block retrieval |
| `src/worker/hybrid_v2/pipeline/synthesis.py` | LLM synthesis prompt construction |
| `src/worker/hybrid_v2/pipeline/chunk_filters.py` | Noise filters, min-content penalty, filled-label exemption |
| `src/worker/hybrid_v2/indexing/dual_index.py` | Defines graph schema including `(:TextChunk)-[:IN_DOCUMENT]->(:Document)` |
| `src/worker/hybrid_v2/indexing/text_store.py` | TextChunk storage and `IN_DOCUMENT` relationship creation |
| `scripts/benchmark_route5_unified_r4_questions.py` | 19-question benchmark runner |
| `scripts/evaluate_route4_reasoning.py` | LLM judge evaluation (gpt-5.1) |
| `benchmarks/route5_unified_r4questions_*.json` | Benchmark result files |

## Key Architecture Notes

- **Sentence denoising** (`_denoise_sentences()` in route_5_unified.py) hard-removes sentences matching patterns like "signature", "signed this", "authorized representative". The bypass is via `_STRUCTURED_SOURCES` set containing `"signature_party"`. For the bypass to work, the `source` field must be present in the evidence dict (this was the bug fixed in `fa2e478`).
- **Signature block chunk retrieval** (`_retrieve_signature_chunks()`) runs a content-based Cypher query rather than relying on `source="signature_party"` tagging, because the purchase contract's Exhibit A page was not tagged with that source during indexing. The method uses `chunk.text CONTAINS 'Authorized Representative' OR chunk.text CONTAINS 'Signed this'`.
- **Coverage chunks** are merged with sentence evidence and passed to the synthesizer alongside PPR-retrieved entity chunks. They go through MD5 + Jaccard deduplication (threshold 0.92) against entity-retrieved chunks.
- **hybrid_v2 graph schema**: `(:TextChunk)-[:IN_DOCUMENT]->(:Document)`, NOT `PART_OF`. The `PART_OF` edge is used for `(:Sentence)-[:PART_OF]->(:TextChunk)`.

---

## How to Run Benchmarks

```bash
# Full 19-question benchmark with 3 repeats (includes LLM context for debugging)
python3 scripts/benchmark_route5_unified_r4_questions.py --repeats 3 --include-context

# Targeted single-question benchmark
python3 scripts/benchmark_route5_unified_r4_questions.py --repeats 3 --include-context --filter-qid Q-D8

# LLM judge evaluation
python3 scripts/evaluate_route4_reasoning.py benchmarks/route5_unified_r4questions_<timestamp>.json

# Deploy to Azure Container Apps
az containerapp update -n graphrag-api -g graphrag-rg --image <acr>.azurecr.io/graphrag-api:latest
```

---

# Session 2 — Route 7 (True HippoRAG 2) Deployment, Benchmarking & Cross-Route Evaluation

## Current State

- **Route 7 deployed and live** on Azure Container Apps (CI/CD auto-deployed on push to main)
- **Route 7 on Q-D questions:** 57/57 (100%) LLM judge across all 3 tested configurations
- **Route 7 on Q-G global questions:** 52/57 (91.2%) LLM judge — first cross-route test
- **Route 6 on Q-G global questions (fresh 3-repeat):** 53/57 (93.0%) LLM judge
- Architecture doc updated with full Section 33 (Route 7)
- Eval script updated to support Q-G ground truth (was previously Q-D/Q-N only)

## What Was Done (Session 2)

### 1. Route 7 Deployment & Bug Fix

- CI/CD auto-deployed Route 7 on push to main
- **HTTP 500 bug:** `_fetch_chunks_by_ids()` returned `Dict[str, List[Dict]]` but synthesizer expects `List[Dict]`. Iterating a dict yields string keys → `'str' object has no attribute 'get'`
- **Fix (commit `9e21c6f`):** Changed return to flat `List[Dict]`, returning `chunks_list` directly
- Redeployed successfully via CI/CD

### 2. Route 7 Benchmarks on Q-D Questions (19-question R4 bank)

All scored **57/57 (100%)** on gpt-5.1 LLM judge:

| Config | LLM Judge | Containment | P50 Latency | Neg Pass |
|--------|-----------|-------------|-------------|----------|
| R7 vanilla (d=0.5) | 57/57 | 0.91 | 3,438 ms | 9/9 |
| R7 Phase 2 all-ON | 57/57 | 0.90 | 5,949 ms | 9/9 |
| R7 damping=0.85 | 57/57 | 0.92 | 5,153 ms | 9/9 |
| R5 baseline | 51/57 | 0.81 | 9,105 ms | 9/9 |

Route 7 vanilla is **2.6x faster** than Route 5 and **+10.5 pp** better on LLM judge.

### 3. Architecture Doc Updated

- Added Section 33 to `ARCHITECTURE_DESIGN_LAZY_HIPPO_HYBRID.md` (commit `741cb10`)
- Covers: architecture diagram, file inventory, Phase 2 flags, damping analysis, bug fix, all benchmark results, ablation, repeatability

### 4. Cross-Route Evaluation — Route 7 vs Route 6 on Q-G Global Questions

This is the first time the LLM judge has been applied to Q-G (global/thematic) questions.

**Eval script fix required:** `scripts/evaluate_route4_reasoning.py` regex was `Q-[DNS]\d+` — changed to `Q-[DGNLS]\d+` to match Q-G questions. Also added multi-line expected answer parsing for Q-G entries (they have bulleted sub-items).

#### Route 7 on Q-G questions (1 repeat)

| QID | Score | Issue |
|-----|-------|-------|
| Q-G1 | 3/3 | |
| Q-G2 | 3/3 | |
| Q-G3 | **2/3** | Minor invoice total phrasing |
| Q-G4 | **1/3** | Missed PMA monthly statement |
| Q-G5 | 3/3 | |
| Q-G6 | **1/3** | Misattributed entity roles |
| Q-G7 | 3/3 | |
| Q-G8 | 3/3 | |
| Q-G9 | 3/3 | |
| Q-G10 | 3/3 | |
| Q-N1..N10 | 27/27 | |
| **Total** | **52/57 (91.2%)** | |

#### Route 6 on Q-G questions (fresh 3-repeat, same day)

| QID | Score | Issue |
|-----|-------|-------|
| Q-G1 | 3/3 | |
| Q-G2 | 3/3 | |
| Q-G3 | 3/3 | |
| Q-G4 | 3/3 | |
| Q-G5 | **2/3** | Missing legal fee recovery |
| Q-G6 | **1/3** | Misattributed entity roles |
| Q-G7 | **2/3** | Missing PMA 60-day notice |
| Q-G8 | 3/3 | |
| Q-G9 | 3/3 | |
| Q-G10 | 3/3 | |
| Q-N1..N10 | 27/27 | (LLM judge passed all; automated checker flagged Q-N8/N9/N10 as FAIL) |
| **Total** | **53/57 (93.0%)** | |

#### Cross-Route Summary

| Metric | Route 7 (Q-G) | Route 6 (Q-G) | Route 7 (Q-D) | Route 5 (Q-D) |
|--------|---------------|---------------|---------------|---------------|
| LLM Judge | 52/57 (91.2%) | 53/57 (93.0%) | **57/57 (100%)** | 51/57 (89.5%) |
| Avg P50 Latency | 15,102 ms | 5,612 ms | 3,438 ms | 9,105 ms |
| Avg Resp Length | 3,171 chars | ~1,900 chars | 1,154 chars | 1,150 chars |

### 5. Automated Metrics — Route 6 Fresh 3-Repeat

- Q-G6: 88% theme coverage (7/8) — missing `pumper` term
- Q-G10: 83% theme coverage (5/6) — missing `scope of work` phrase
- Q-N8, Q-N9, Q-N10: automated NEGATIVE_TEST FAIL (but LLM judge scored 3/3 — the automated checker is too strict on response format)

## Benchmark Files Generated (Session 2)

| File | Description |
|------|-------------|
| `benchmarks/route7_hipporag2_r4questions_20260221T101924Z.json` | R7 vanilla, 1 repeat, smoke test |
| `benchmarks/route7_hipporag2_r4questions_20260221T102100Z.json` + `.eval.md` | R7 vanilla d=0.5, 3 repeats — **57/57** |
| `benchmarks/route7_hipporag2_r4questions_20260221T103332Z.json` + `.eval.md` | R7 Phase 2 all-ON — **57/57** |
| `benchmarks/route7_hipporag2_r4questions_20260221T103757Z.json` + `.eval.md` | R7 damping=0.85 — **57/57** |
| `benchmarks/route5_unified_r4questions_20260221T102102Z.json` + `.eval.md` | R5 regression — **51/57** |
| `benchmarks/route7_global_search_20260221T105836Z.json` + `_eval_input.eval.md` | R7 on Q-G, 1 repeat — **52/57** |
| `benchmarks/route6_global_search_20260221T112104Z.json` + `_eval_input.eval.md` | R6 on Q-G, 3 repeats — **53/57** |

## Commits (Session 2)

```
9e21c6f  fix(route7): pass flat chunk list to synthesizer instead of wrapped dict
741cb10  docs: add Route 7 (True HippoRAG 2) architecture and benchmark results
```

---

## TODO — Pick Up Tomorrow

### 1. Investigate Route 6 Q-G score discrepancy
- **Issue:** User recalls Route 6 previously scoring 57/57 on Q-G global questions, but fresh 3-repeat eval today shows 53/57. Need to determine if:
  - A prior deployment/config produced better results (different synthesis model, different REDUCE prompt state?)
  - The LLM judge ground truth for Q-G is too strict (multi-line expected answers parsed correctly?)
  - Q-G5 (dispute resolution), Q-G6 (entity roles), Q-G7 (notice mechanisms) are genuine Route 6 weaknesses or eval artifacts
- **Action:** Find the previous 57/57 eval file or re-run with the specific deployment/config that achieved it

### 2. Route 7 Q-G improvement
- **Q-G4 (1/3):** Route 7 missed PMA monthly statement reporting obligation — triple embeddings don't encode "reporting" concepts well. Consider adding community seeds (Phase 2) specifically for global questions.
- **Q-G6 (1/3):** Both Route 6 and Route 7 misattribute entity-to-document roles. Shared synthesis problem — consider prompt engineering for entity enumeration queries.

### 3. Commit script changes
- `scripts/benchmark_route3_global_search.py` — Added `hipporag2_search` as `--force-route` option
- `scripts/evaluate_route4_reasoning.py` — Fixed Q-G ground truth: regex `Q-[DNS]` → `Q-[DGNLS]`, multi-line expected answer parsing

### 4. Record cross-route results in architecture doc
- Section 33 only covers Q-D results. Add Q-G cross-route comparison (Route 7 vs Route 6) once the Route 6 baseline discrepancy is resolved.

### 5. Phase 2 ablation on Q-G questions
- Phase 2 features (structural seeds, community seeds) may help Route 7 on global questions where vanilla Phase 1 scored 52/57. Worth testing once Q-G eval baseline is confirmed.

### 6. Remaining Route 5 issues (from Session 1)
- Q-D8 still 1/3 — LLM miscounts entities despite correct retrieval
- Q-D3 still 2/3 — missing 60-day repair window

---

## Key Files Reference (Session 2 additions)

| File | Purpose |
|------|---------|
| `src/worker/hybrid_v2/routes/route_7_hipporag2.py` | Route 7 handler — PPR + triple linking + DPR |
| `src/worker/hybrid_v2/retrievers/triple_store.py` | Triple embedding store + recognition memory filter |
| `src/worker/hybrid_v2/retrievers/hipporag2_ppr.py` | Passage-node PPR (Entity↔Passage weighted walk) |
| `scripts/benchmark_route7_hipporag2.py` | Route 7 Q-D benchmark script |
| `scripts/benchmark_route3_global_search.py` | Q-G benchmark (supports `--force-route hipporag2_search`) |
| `scripts/evaluate_route4_reasoning.py` | LLM judge (now supports Q-G ground truth) |
| `ARCHITECTURE_DESIGN_LAZY_HIPPO_HYBRID.md` | Section 33 = Route 7 full documentation |

## How to Run Cross-Route Q-G Benchmarks

```bash
# Get token
export GRAPHRAG_API_TOKEN=$(az account get-access-token --scope "api://b68b6881-80ba-4cec-b9dd-bd2232ec8817/.default" --query accessToken -o tsv)

# Route 7 on Q-G questions
python3 scripts/benchmark_route3_global_search.py --force-route hipporag2_search --repeats 3 --timeout 180

# Route 6 on Q-G questions
python3 scripts/benchmark_route3_global_search.py --force-route concept_search --repeats 3 --timeout 180

# LLM judge (need to convert format first — global search JSON uses different structure)
# Convert: python3 -c "import json; ..." (see session notes)
# Then: python3 scripts/evaluate_route4_reasoning.py benchmarks/<converted>_eval_input.json
```
