# Route 2 Architecture Deep Dive ‚Äî Sequential Improvement Experiment

**Date:** 2026-02-10  
**Objective:** Systematically analyze and improve Route 2 (Local Search) noise sources, one improvement at a time, with evaluation after each change.  
**Baseline Build:** commit `b7a299c0` (post-denoising Feb 9, Steps 4‚Äì12)  
**Test Group:** `test-5pdfs-v2-fix2`  
**Benchmark:** 10 positive (Q-L1‚ÄìQ-L10) + 9 negative (Q-N1‚ÄìQ-N10) questions

---

## 1. Architecture Overview: LazyGraphRAG + HippoRAG 2 Hybrid

The system combines two complementary retrieval paradigms:

| Component | Strength | Weakness (compensated by the other) |
|---|---|---|
| **LazyGraphRAG** | Structural topic clustering (GDS Louvain communities), eager index-time LLM summaries, semantic community matching | Can't do entity-level precision retrieval ‚Äî returns thematic clusters, not specific facts |
| **HippoRAG 2** | Entity-focused PPR traversal ‚Äî deterministic graph pathfinding from seed entities; traverses KNN `SEMANTICALLY_SIMILAR` edges | Cold-start problem with ambiguous queries (needs clear entity seeds to start PPR) |

### How They Work Together in Route 2

Route 2 is **primarily HippoRAG 2**, with LazyGraphRAG providing supporting infrastructure:

| LazyGraphRAG contribution to Route 2 | HippoRAG 2 contribution to Route 2 |
|---|---|
| GDS Louvain `community_id` property on Entity nodes ‚Üí used for PPR seed augmentation (adds top-degree community peers) | Core retrieval engine: PPR from NER entities through 5 graph paths |
| `:Community` nodes with LLM summaries + Voyage embeddings ‚Üí injected into NER prompt as context so gpt-4o can extract entity names that exist in the graph | Entity‚ÜíTextChunk traversal via MENTIONS edges |
| Community-aware seed augmentation (+5 peers from same Louvain cluster) | KNN entity‚Üîentity edges traversed in PPR Path 3 (similarity ‚â• 0.60) |
| | KNN section‚Üîsection edges traversed in PPR Path 2 (similarity ‚â• 0.50) |

### Two Louvain Implementations (Same Partition, Two Access Patterns)

| Step | What it produces | Query-time usage in Route 2 |
|---|---|---|
| **Step 8** ‚Äî GDS Louvain (`_run_gds_graph_algorithms()`) | `community_id` integer property on each Entity node | PPR seed augmentation: `get_community_peers()` queries `seed.community_id` to find top-degree peers |
| **Step 9** ‚Äî Materialization (`_materialize_louvain_communities()`) | Reads Step 8 `community_id` ‚Üí creates `:Community` nodes with LLM summaries + Voyage embeddings + `BELONGS_TO` edges | NER prompt grounding: community titles+summaries injected into gpt-4o NER prompt |

Both derive from the same GDS Louvain run. Step 9 is a materialization layer on top of Step 8.

---

## 2. Route 2 Pipeline ‚Äî Step-by-Step with Noise Analysis

```
Query
  ‚îÇ
  ‚ñº
Step 1: NER Entity Extraction (gpt-4o)       [intent.py]
  ‚îÇ  ~5 seed entity names
  ‚ñº
Step 2: Seed Resolution (6-strategy cascade)  [tracing.py]
  ‚îÇ  resolved Neo4j node IDs
  ‚ñº
Step 3: PPR Graph Traversal (HippoRAG 2)     [async_neo4j_service.py]
  ‚îÇ  ~13 entities (top_k=15, budget=0.8)
  ‚ñº
Step 4: Text Chunk Retrieval + Denoising      [synthesis.py ‚Üí _retrieve_text_chunks()]
  ‚îÇ  ~40 raw ‚Üí ~10 deduped chunks
  ‚ñº
Step 5: Context Building (token budget)       [synthesis.py ‚Üí _build_cited_context()]
  ‚îÇ  ~11.6K tokens (32K budget at 36% utilization)
  ‚ñº
Step 6: LLM Synthesis (gpt-5.1)              [synthesis.py ‚Üí _generate_response()]
  ‚îÇ  cited response
  ‚ñº
Post: Negative detection (0 chunks ‚Üí "not found")
```

### PPR 5-Path Traversal (Step 3)

| Path | Edge Types | Weight Env Var | Default |
|---|---|---|---|
| Path 1: Entity-to-Entity | All edges except MENTIONS, SIMILAR_TO, APPEARS_IN_SECTION | `PPR_WEIGHT_ENTITY` | 1.0 |
| Path 2: Section thematic hops | MENTIONS ‚Üí IN_SECTION ‚Üí **SEMANTICALLY_SIMILAR** (Section‚ÜîSection) ‚Üí IN_SECTION ‚Üí MENTIONS | `PPR_WEIGHT_SECTION` | 1.0 |
| Path 3: Entity KNN similarity | **SEMANTICALLY_SIMILAR** or **SIMILAR_TO** (Entity‚ÜîEntity) | `PPR_WEIGHT_SIMILAR` | 1.0 |
| Path 4: Cross-document | APPEARS_IN_SECTION ‚Üí **SHARES_ENTITY** ‚Üí HAS_HUB_ENTITY | `PPR_WEIGHT_SHARES` | 1.0 |
| Path 5: Hub entities | APPEARS_IN_SECTION ‚Üí **HAS_HUB_ENTITY** | `PPR_WEIGHT_HUB` | 1.0 |

All weights default to 1.0 ‚Äî no tuning has been done.

---

## 3. Noise Source Attribution

| # | Source | Pipeline Step | Mechanism | Current Mitigation | Residual Gap |
|---|---|---|---|---|---|
| **N1** | Broad NER entities | Step 1 | "PROPERTY MANAGEMENT AGREEMENT" pulls in everything from that doc | Generic seed blocklist | No query-intent filtering |
| **N2** | PPR graph expansion | Step 3 | Fee question reaches Warranty, Disclaimers via document-level edges | Relevance budget 0.8 (keeps 13/15) | Budget too generous; no score-gap pruning |
| **N3** | PPR scores discarded | Step 4 | Every entity gets uniform `limit_per_entity=12` regardless of PPR score | MD5 dedup collapses duplicates using highest score | Score-weighted allocation not implemented |
| **N4** | Multi-entity chunk duplication | Step 4 | Same chunk via multiple entities ‚Üí 75% duplicates | **MD5 content-hash dedup** (dominant fix, +67% F1) | Near-duplicate detection not implemented |
| **N5** | Cross-document copies | Index-time | Same doc indexed as 5+ copies ‚Üí 20√ó redundancy | MD5 dedup catches content-identical copies | Semantic near-dedup not implemented |

---

## 4. Baseline Metrics (Post-Feb 9 Denoising)

| Metric | Value |
|---|---|
| Containment (positive) | 10/10 |
| F1 (avg) | 0.261 |
| Chunks to LLM (avg) | 9.9 |
| Context tokens (avg) | 11,597 |
| Latency (avg) | 5,920 ms |
| Negative pass | 9/9 |
| Raw chunks before dedup (avg) | 40.0 |
| Dedup ratio (avg) | 75.3% |
| Noise filters triggered | 0 |
| Token budget utilization | 36% |

Source: `route2_local_search_20260209T162951Z.json` (baseline, all measures ON)

---

## 5. Improvement Queue

| # | Improvement | Target Noise Source | Env Toggle | Expected Impact | Status |
|---|---|---|---|---|---|
| **1** | PPR score-weighted chunk allocation | N3 (scores discarded) | `DENOISE_SCORE_WEIGHTED=1` (enable) | Raw chunks ~40‚Üí~20; F1 ‚Üë | ‚úÖ KEPT (‚àí0.002 F1, ‚àí21% tokens) |
| **1.5** | Community-aware PPR entity filtering | N2 (graph expansion) | `DENOISE_COMMUNITY_FILTER=1` (enable) | Cross-community noise entities penalized | ‚úÖ KEPT (‚àí0.008 F1, ‚àí8% tokens) |
| **2** | PPR score-gap pruning | N2 (graph expansion) | `DENOISE_SCORE_GAP=1` (enable) | Entity count ~13‚Üí~6-8 | üîß Next |
| **3** | Query-intent NER prompt | N1 (broad NER) | Prompt change only | Seed count ~5‚Üí~2-3 targeted | ‚è≥ Queued |
| **4** | Semantic near-dedup (embedding cosine) | N5 (cross-doc copies) | `DENOISE_SEMANTIC_DEDUP=1` (enable) | Catches formatting-variant duplicates | ‚è≥ Queued |
| **5** | Vector chunk safety net | N/A (coverage gap) | `DENOISE_VECTOR_FALLBACK=1` (enable) | Insurance for NER misses | ‚è≥ Queued |

---

## 6. Experiment Results

### 6.1 Improvement #1: PPR Score-Weighted Chunk Allocation

**Hypothesis:** Allocating chunks proportional to PPR score instead of uniform `limit_per_entity=12` will reduce raw chunk count and improve context focus. High-PPR entities get more chunks; low-PPR entities get fewer (min 1).

**Implementation:**
- Modified `_retrieve_text_chunks()` in `synthesis.py`
- Formula: `entity_budget = max(1, round(score / max_score √ó limit_per_entity))`
- Top-scoring entity gets full 12 chunks; noise entity with 1/50th the score gets 1 chunk
- Toggle: `DENOISE_SCORE_WEIGHTED=1` to enable (off by default for safe rollout)
- Fallback: if all scores are 0 or equal, reverts to uniform allocation

**Results:**

| Metric | Baseline (all ON) | + Score-Weighted | Delta |
|---|---|---|---|
| Containment | 10/10 | 10/10 | ‚Äî |
| F1 (avg) | 0.261 | 0.259 | ‚àí0.002 |
| Precision (avg) | 0.155 | 0.155 | ‚Äî |
| Recall (avg) | 0.988 | 0.988 | ‚Äî |
| Chunks to LLM (avg) | 9.9 | 8.4 | ‚àí15% |
| Raw chunks (pre-dedup) | 40.0 | 36.4 | ‚àí9% |
| Context tokens (avg) | 12,651 | 10,031 | ‚àí21% |
| Latency (avg) | 5,920 ms | 6,610 ms | +12%* |
| Negative pass | 9/9 | 9/9 | ‚Äî |

*\*Latency increase dominated by Q-L1 cold start (14,491ms vs 6,081ms). Excluding Q-L1: 5,902ms ‚Üí 5,734ms (‚àí3%).*

**Per-question detail:**

| QID | Base F1 | New F1 | Œî F1 | Base P | New P | Base chunks | New chunks | Base tokens | New tokens |
|---|---|---|---|---|---|---|---|---|---|
| Q-L1 | 0.304 | 0.275 | ‚àí0.029 | 0.179 | 0.159 | 10 | 8 | 12,744 | 9,252 |
| Q-L2 | 0.062 | 0.082 | +0.020 | 0.032 | 0.043 | 10 | 10 | 12,744 | 12,744 |
| Q-L3 | 0.412 | 0.424 | +0.012 | 0.259 | 0.269 | 10 | 8 | 12,744 | 9,252 |
| Q-L4 | 0.133 | 0.120 | ‚àí0.013 | 0.071 | 0.064 | 9 | 9 | 11,414 | 11,414 |
| Q-L5 | 0.222 | 0.189 | ‚àí0.033 | 0.125 | 0.104 | 8 | 8 | 11,891 | 11,891 |
| Q-L6 | 0.291 | 0.188 | ‚àí0.103 | 0.170 | 0.104 | 10 | 9 | 12,744 | 11,323 |
| Q-L7 | 0.197 | 0.241 | +0.044 | 0.109 | 0.137 | 8 | 9 | 11,891 | 11,323 |
| Q-L8 | 0.275 | 0.275 | +0.000 | 0.163 | 0.163 | 10 | 8 | 12,744 | 9,252 |
| Q-L9 | 0.432 | 0.485 | +0.053 | 0.276 | 0.320 | 12 | 10 | 13,798 | 10,306 |
| Q-L10 | 0.286 | 0.312 | +0.026 | 0.167 | 0.185 | 12 | 5 | 13,798 | 3,556 |

**Winners:** Q-L9 (+0.053), Q-L7 (+0.044), Q-L10 (+0.026), Q-L2 (+0.020), Q-L3 (+0.012)
**Losers:** Q-L6 (‚àí0.103), Q-L5 (‚àí0.033), Q-L1 (‚àí0.029), Q-L4 (‚àí0.013)
**Neutral:** Q-L8 (+0.000)

**Observations:**
1. **F1 essentially flat** (‚àí0.002, well within noise). 5 winners vs 4 losers, roughly balanced.
2. **Context tokens reduced 21%** ‚Äî significant cost saving and leaves headroom for future improvements.
3. **Q-L6 is the big loser** (‚àí0.103): dropped from 10‚Üí9 chunks, lost 1 relevant chunk. This question asks about specific monetary terms where the missing chunk likely contained key data.
4. **Q-L10 dramatic token reduction** (13,798‚Üí3,556, ‚àí74%): budget cut 12‚Üí5 chunks, yet F1 *improved* (+0.026), confirming noise dilution was real.
5. **Recall unchanged** (0.988 both): score-weighting doesn't lose ground-truth entities, only adjusts chunk depth.
6. All entity budgets still ‚â•1 (minimum guaranteed), so no entity is completely silenced.

**Verdict:** ‚úÖ **KEEP ‚Äî neutral F1, 21% token reduction.** Score-weighted allocation doesn't improve F1 alone but reduces context noise by 21%, creating headroom for downstream improvements. The Q-L6 regression warrants monitoring. Proceed to Improvement #1.5.

**Benchmark file:** `benchmarks/route2_local_search_20260210T030934Z.json`

---

### 6.2 Improvement #1.5: Community-Aware PPR Entity Filtering

**Hypothesis:** Entities from different Louvain communities than the query's primary topic add noise. Penalizing off-community entities' scores by 0.3√ó before score-weighted allocation will reduce cross-topic noise chunks.

**Implementation:**
- Already implemented in `synthesis.py` L828‚ÄìL870 (toggle: `DENOISE_COMMUNITY_FILTER=1`)
- Batch-fetches `community_id` for all PPR entities via `get_entity_communities()`
- Identifies **target community** by majority-vote among top-3 PPR-scored entities
- Multiplies off-community entity scores by `COMMUNITY_PENALTY` (default 0.3)
- Cascading effect: penalized entities get fewer chunks via score-weighted allocation, and their chunks sort lower in context

**Results:**

| Metric | + #1 Score-Weighted | + #1.5 Community Filter | Delta |
|---|---|---|---|
| Containment | 10/10 | 10/10 | ‚Äî |
| F1 (avg) | 0.259 | 0.252 | ‚àí0.008 |
| Precision (avg) | 0.155 | 0.153 | ‚àí0.002 |
| Recall (avg) | 0.988 | 1.000 | +0.012 |
| Chunks to LLM (avg) | 8.4 | 7.8 | ‚àí7% |
| Raw chunks (pre-dedup) | 36.4 | 34.4 | ‚àí6% |
| Context tokens (avg) | 10,031 | 9,242 | ‚àí8% |
| Latency (avg) | 6,610 ms | 7,077 ms | +7%* |
| Negative pass | 9/9 | 9/9 | ‚Äî |

*\*Latency increase dominated by Q-L1 cold start (18,108ms). Excluding Q-L1: median ~5,900ms.*

**Per-question detail:**

| QID | Base F1 | New F1 | Œî F1 | Base P | New P | Base chunks | New chunks | Base tokens | New tokens |
|---|---|---|---|---|---|---|---|---|---|
| Q-L1 | 0.275 | 0.304 | +0.029 | 0.159 | 0.179 | 8 | 7 | 9,252 | 7,831 |
| Q-L2 | 0.082 | 0.085 | +0.003 | 0.043 | 0.044 | 10 | 9 | 12,744 | 11,323 |
| Q-L3 | 0.424 | 0.424 | +0.000 | 0.269 | 0.269 | 8 | 7 | 9,252 | 7,831 |
| Q-L4 | 0.120 | 0.136 | +0.016 | 0.064 | 0.073 | 9 | 9 | 11,414 | 11,414 |
| Q-L5 | 0.189 | 0.200 | +0.011 | 0.104 | 0.111 | 8 | 7 | 11,891 | 10,470 |
| Q-L6 | 0.188 | 0.219 | +0.031 | 0.104 | 0.123 | 9 | 9 | 11,323 | 11,323 |
| Q-L7 | 0.241 | 0.230 | ‚àí0.011 | 0.137 | 0.130 | 9 | 9 | 11,323 | 11,323 |
| Q-L8 | 0.275 | 0.275 | +0.000 | 0.163 | 0.163 | 8 | 7 | 9,252 | 7,831 |
| Q-L9 | 0.485 | 0.356 | ‚àí0.129 | 0.320 | 0.216 | 10 | 7 | 10,306 | 6,538 |
| Q-L10 | 0.312 | 0.286 | ‚àí0.026 | 0.185 | 0.167 | 5 | 7 | 3,556 | 6,538 |

**Winners:** Q-L6 (+0.031), Q-L1 (+0.029), Q-L4 (+0.016), Q-L5 (+0.011), Q-L2 (+0.003)
**Losers:** Q-L9 (‚àí0.129), Q-L10 (‚àí0.026), Q-L7 (‚àí0.011)
**Neutral:** Q-L3 (+0.000), Q-L8 (+0.000)

**Community Filter Behaviour:**

| QID | Target Community | In-Target | Penalised | Notable Penalised Entities |
|---|---|---|---|---|
| Q-L1 | 23 (Property Mgmt) | 9 | 4 | ‚Äî |
| Q-L4 | 104 (Purchase Contract) | 5 | 8 | Heavy penalty ‚Äî most entities cross-community |
| Q-L6 | 23 (Property Mgmt) | 8 | 5 | Builder, Buyer/Owner penalised |
| Q-L9 | 104 (Purchase Contract) | 10 | 3 | Fabrikam 8‚Üí2, Bayfront 8‚Üí2 |
| Q-L10 | 104 (Purchase Contract) | 10 | 3 | Same as Q-L9 |

**Q-L9 Regression Analysis:**
- Community filter correctly penalized off-topic entities (Fabrikam 8‚Üí2, Bayfront 8‚Üí2)
- Chunks reduced 10‚Üí7, tokens 10,306‚Üí6,538 (‚àí37%)
- **Recall unchanged (1.0)** ‚Äî the correct answer is still retrieved
- Precision dropped 0.320‚Üí0.216 due to LLM synthesis variance (more verbose response with fewer context chunks)
- Single-repeat run; Q-L9 had highest baseline F1 (0.485) so most room for regression noise

**Observations:**
1. **F1 near-flat** (‚àí0.008) with 5 winners vs 3 losers. Within single-repeat noise margin.
2. **Context tokens further reduced 8%** on top of #1's 21% reduction. Cumulative: baseline 12,651 ‚Üí 9,242 (‚àí27%).
3. **Recall improved** from 0.988 ‚Üí 1.000 ‚Äî community filtering may help precision of entity selection.
4. Community filter penalizes 3‚Äì8 entities per query, with heavier impact on cross-document questions (Q-L4: 8 penalised).
5. Filter correctly identifies document-topic communities (23=Property Mgmt agreement, 104=Purchase contract/invoice).
6. Q-L6 improvement (+0.031) reverses the Q-L6 regression from #1 ‚Äî community filter helped recover the lost relevant chunk.

**Verdict:** ‚úÖ **KEEP ‚Äî neutral F1, further 8% token reduction, better recall.** Community filter adds meaningful topic-aware penalization. Q-L9 regression is LLM synthesis noise (recall=1.0), not retrieval degradation. Cumulative token savings now 27% vs original baseline. Proceed to Improvement #2.

**Benchmark file:** `benchmarks/route2_local_search_20260210T052425Z.json`

---

### 6.3 Improvement #2: PPR Score-Gap Pruning

*(Pending)*

---

### 6.4 Improvement #3: Query-Intent NER Prompt

*(Pending)*

---

### 6.5 Improvement #4: Semantic Near-Dedup

*(Pending)*

---

### 6.6 Improvement #5: Vector Chunk Safety Net

*(Pending)*

---

## 7. Cumulative Results Summary

| Improvement | Containment | F1 (avg) | Chunks (avg) | Raw Chunks | Latency | Neg Pass | Verdict |
|---|---|---|---|---|---|---|---|
| Baseline (Feb 9) | 10/10 | 0.261 | 9.9 | 40.0 | 5,920 ms | 9/9 | ‚Äî |
| + #1 Score-weighted | 10/10 | 0.259 | 8.4 | 36.4 | 6,610 ms | 9/9 | ‚úÖ KEEP |
| + #1.5 Community filter | 10/10 | 0.252 | 7.8 | 34.4 | 7,077 ms | 9/9 | ‚úÖ KEEP |
| + #2 Score-gap pruning | | | | | | | |
| + #3 NER intent prompt | | | | | | | |
| + #4 Semantic dedup | | | | | | | |
| + #5 Vector safety net | | | | | | | |

---

## 8. Conclusions & Recommendations

*(Will be written after all experiments complete)*
