# Handover — 2026-02-09

## Current State

### Deployment
- **Revision:** `graphrag-api--0000020` — image `76b59a5-98` — **Running**
- **HEAD:** `76b59a50` on `main` (all commits pushed)
- **API URL:** `https://graphrag-api.salmonhill-df6033f3.swedencentral.azurecontainerapps.io`
- **Resource Group:** `rg-graphrag-feature` / Subscription `3adfbe7c-9922-40ed-b461-ec798989a3fa`
- **ACR:** `graphragacr12153`
- **Test Group:** `test-5pdfs-v2-fix2`
- **Ablation toggles:** All restored to OFF (all measures enabled)
- **Tests:** 247 passed, 53 skipped (3 integration files skip due to pydantic collection errors — pre-existing)

### Git Log (Session Commits)
```
76b59a50  analysis: Route 2 prompt review and synthesis LLM comparison post-denoising
a4b88946  fix: skip temperature param for gpt-5-mini/gpt-5-nano (only support default=1)
4f795d1a  fix: dedup ablation toggle was no-op due to dict-key implicit dedup
1044b8e0  fix: merge retrieval_stats into context_stats when Route 2 pre-fetches chunks
f06d7385  feat: per-measure attribution instrumentation + ablation toggles
5924415c  feat: expose context_stats in API response metadata
b7a299c0  fix: unpack _retrieve_text_chunks tuple in route_2_local.py
b580b9c6  Steps 11-12: PPR path weight tuning + community-aware seeding
d276454a  Step 10: Noise filters — form-label, bare-heading, min-content
92ea67c0  Step 8: Normalize KNN config consistency
30df3e85  feat: implement context de-noising pipeline Phase 1 (Steps 4-7)
3105b53d  docs: apply 13 architecture corrections
```

---

## What Was Accomplished

### Phase 1: Context De-Noising Pipeline (Steps 4–12)
1. **Step 4 — Chunk dedup** (`synthesis.py`): SHA-256 content-hash dedup across entity-fetched chunks. Removes 75% of raw chunks on average.
2. **Step 5 — Token budget guard** (`synthesis.py`): Configurable `SYNTHESIS_TOKEN_BUDGET` (default 32K) trims lowest-scored chunks.
3. **Step 6 — PPR score propagation** (`synthesis.py`): Entity PPR scores propagate to their chunks via `_score_for_entity()`.
4. **Step 7 — Score-ranked ordering** (`synthesis.py`): Chunks sorted by propagated PPR score before budget trim.
5. **Step 8 — KNN config normalisation** (`synthesis.py`): Consistent `NORMALIZED_KNN_K` between Route 2 PPR and Route 4 beam search.
6. **Step 10 — Noise filters** (`chunk_filters.py`): Three composable penalty multipliers — form-label (0.05×), bare-heading (0.10×), min-content (0.20×). 23 unit tests.
7. **Steps 11–12 — PPR tuning** (`synthesis.py`): Configurable path weights (`PPR_WEIGHT_ENTITY/SECTION/SIMILAR/SHARES/HUB`) + community-aware seeding (`PPR_COMMUNITY_SEED_AUGMENT`).

### Instrumentation & Observability
8. **`context_stats`** in API metadata: `chunks_before_budget`, `chunks_after_budget`, `context_tokens`, `final_context_tokens`, `token_budget`, `budget_enabled`, `num_doc_groups`.
9. **`retrieval` sub-dict**: `chunks_raw`, `chunks_after_dedup`, `duplicates_removed`, `dedup_ratio`, `noise_filters` breakdown, `dedup_enabled`, `noise_enabled`.
10. **Ablation env-var toggles**: `DENOISE_DISABLE_DEDUP=1`, `DENOISE_DISABLE_NOISE=1`, `DENOISE_DISABLE_BUDGET=1` — disable individual measures for A/B testing.

### Benchmarking & Analysis
11. **Route 2 benchmark (3 runs):** 10/10 positive containment, 9/9 negative pass on all runs.
12. **Effectiveness analysis** (`ANALYSIS_ROUTE2_DENOISING_EFFECTIVENESS_2026-02-09.md`): Chunks −74%, latency −42%, F1 +67%, Q-L8/Q-L10 flipped from FAIL to PASS.
13. **Ablation study** (`ANALYSIS_ROUTE2_PRECISION_ABLATION_2026-02-09.md`): Dedup is the dominant measure (F1 −26% when disabled). Noise filters latent on clean corpus. Budget guard backstop only.

### Bug Fixes
14. Route 2 HTTP 500 — tuple unpacking mismatch after `_retrieve_text_chunks` return changed.
15. `retrieval_stats` missing in Route 2 — pre-fetch path skipped merge; fixed in route handler.
16. Dedup ablation toggle was no-op — dict-key implicit dedup; fixed to use list-based dedup.
17. gpt-5-mini/gpt-5-nano temperature param skip (only support default=1).

---

## Key Files Modified

| File | Changes |
|---|---|
| `src/worker/hybrid_v2/pipeline/synthesis.py` | Dedup, budget, PPR scores, score ordering, context_stats, ablation toggles |
| `src/worker/hybrid_v2/pipeline/chunk_filters.py` | New file — noise filter penalties with per-filter attribution |
| `src/worker/hybrid_v2/routes/route_2_local.py` | 3-tuple unpack, retrieval_stats merge into context_stats |
| `src/worker/hybrid_v2/routes/route_3_global.py` | context_stats passthrough to metadata |
| `src/worker/hybrid_v2/routes/route_4_drift.py` | context_stats passthrough to metadata |
| `tests/unit/test_chunk_filters.py` | 23 tests for noise filters |
| `tests/unit/test_synthesis.py` | Updated for new return types |
| `ARCHITECTURE_DESIGN_LAZY_HIPPO_HYBRID.md` | 13 architecture corrections applied |

---

## Benchmark File Inventory

| Label | File |
|---|---|
| Pre-change baseline | `benchmarks/route2_local_search_20260208T152919Z.json` |
| Post-change baseline (all ON) | `benchmarks/route2_local_search_20260209T162951Z.json` |
| Ablation: Dedup OFF | `benchmarks/route2_local_search_20260209T170957Z.json` |
| Ablation: Noise OFF | `benchmarks/route2_local_search_20260209T172507Z.json` |
| Ablation: Budget OFF | `benchmarks/route2_local_search_20260209T173020Z.json` |

---

## TODO — Next Session

### High Priority
- [ ] **Route 3 (global/thematic) benchmark** — Run `scripts/benchmark_route3_global_search.py` or `benchmark_route3_thematic.py`. Context_stats instrumentation is already wired in route_3_global.py.
- [ ] **Route 4 (drift/multi-hop) benchmark** — Run `scripts/benchmark_route4_drift_multi_hop.py`. Context_stats already wired in route_4_drift.py.
- [ ] **Test on messier corpus** — Upload documents with OCR artifacts, form labels, or fragmented layouts to validate noise filters actually trigger. Current 5-PDF set is too clean (0 chunks penalised).

### Medium Priority
- [ ] **Steps 13–16: FastRP structural embeddings** — Next phase from ARCHITECTURE_DESIGN_LAZY_HIPPO_HYBRID.md. FastRP graph embeddings in Neo4j to complement text embeddings for retrieval.
- [ ] **Ablation on larger corpus** — The 5-PDF test set is small. Run ablation study on a larger document set to get more variance and validate noise filter contributions.
- [ ] **PPR weight tuning experiments** — The env vars (`PPR_WEIGHT_ENTITY`, `PPR_WEIGHT_SECTION`, etc.) are all at default 1.0. Experiment with different weights to see if retrieval quality improves.

### Low Priority / Cleanup
- [ ] **Fix pydantic collection errors** — `test_indexing_fallback.py`, `test_indexing_pipeline.py`, `test_route3_hybrid.py` fail to collect due to pydantic validation errors. Pre-existing, not from our changes.
- [ ] **Remove ablation env vars from production** — The `DENOISE_DISABLE_*` vars are set to empty strings. Consider removing them entirely from the container app config once ablation testing is complete.
- [ ] **Step 9 (skipped)** — Review architecture plan to confirm Step 9 status. Steps 1–8, 10–12 are implemented.

---

## Environment Reference

| Item | Value |
|---|---|
| API URL | `https://graphrag-api.salmonhill-df6033f3.swedencentral.azurecontainerapps.io` |
| Auth scope | `api://b68b6881-80ba-4cec-b9dd-bd2232ec8817/.default` |
| Test group | `test-5pdfs-v2-fix2` |
| Token budget | `SYNTHESIS_TOKEN_BUDGET=32000` (default) |
| PPR weights | All default 1.0 (`PPR_WEIGHT_ENTITY/SECTION/SIMILAR/SHARES/HUB`) |
| Community seed | `PPR_COMMUNITY_SEED_AUGMENT=5` (default) |
| CI/CD | `.github/workflows/deploy.yml` — triggers on push to `main` for `src/**` |
| ACR image tag format | `<7-char-sha>-<run_number>` |
