# Session Handover — 2026-02-10

**Last commit:** `d2cc988e` on `main` (pushed to remote)  
**Session focus:** Citation explosion fix, doc-scope retrieval, chunking strategy analysis  

---

## What Was Completed Today

### 1. Citation Explosion Structural Fix ✅

**Problem:** Sentence markers `[1a], [1b], [2a]` from DI `language_spans` leaked through Route 3 synthesis, causing the LLM to hallucinate hundreds of citations (360+ markers in context).

**Fix (committed d2cc988e):**
- Added `strip_sentence_markers()` utility + `_SENTENCE_CITATION_RE` regex at module level in `synthesis.py`
- Applied structurally in **both** Route 2 (`synthesize()`) and Route 3 (`synthesize_with_graph_context()`)
- Added `sentence_citation_map.clear()` in both paths
- Added per-chunk sentence caps: `ROUTE2_MAX_SENTENCES_PER_CHUNK=8`, `ROUTE3_MAX_SENTENCES_PER_CHUNK=8`
- Updated `scripts/benchmark_route3_community_prompt.py` to import shared function instead of local copy

**Files changed:**
- `src/worker/hybrid_v2/pipeline/synthesis.py` — structural strip + sentence caps
- `scripts/benchmark_route3_community_prompt.py` — import shared utility

### 2. Doc-Scope Retrieval (Prior Session, Verified) ✅

IDF-weighted entity→document voting, doc-filtered Cypher, doc-coherence denoising (Pass 7). Benchmark results:

| Combo | F1 | Precision | Recall | Ctx Chars |
|---|---|---|---|---|
| baseline+gpt-5-mini | 0.479 | 0.579 | 0.456 | 26,640 |
| doc_scope+gpt-5-mini | 0.492 | 0.642 | 0.440 | 13,808 |
| baseline+gpt-4.1-mini | 0.501 | 0.459 | 0.579 | 26,640 |
| doc_scope+gpt-4.1-mini | 0.521 | 0.487 | 0.585 | 13,808 |

### 3. Chunking & Embedding Pipeline Research ✅

Full codebase audit of how documents are chunked and embedded. Key findings:

- **Table row linearizations exist in metadata but are NEVER embedded** (`_extract_table_row_sentences()`)
- **All content types embedded identically** — no content-type routing
- **DI `language_spans` are noisy** — ~40% are table cells, form labels, headers
- **avg chunk size 641 tokens** — too coarse for precision-critical queries
- **Chunk noise filters** are post-retrieval soft penalties only (don't fix root cause)

### 4. Chunking Strategy Analysis Document ✅

Created `ANALYSIS_CHUNKING_STRATEGY_DEEP_DIVE_2026-02-10.md` covering:
- Full current architecture documentation
- Two improvement options with pros/cons
- Four mature industry strategies (references)
- Phase 0 experiment design
- Production scale estimates
- Decision framework by customer domain

---

## What Is NOT Done — Todo List

### Immediate (Tomorrow Morning)

- [ ] **Build Phase 0 experiment script** (`scripts/experiment_chunking_strategy.py`)
  - Pull DI paragraphs + table row linearizations from chunk metadata in Neo4j
  - Embed with voyage-context-3 `contextualized_embed()`
  - Run benchmark queries as cosine similarity top-k
  - Compare F1/precision/recall across 4 variants (baseline, paragraphs+tables, filtered sentences, hybrid)
  - ~2-3 hours of work

### Short-Term (This Week)

- [ ] **Implement Option 1: Table row promotion** (if Phase 0 shows improvement)
  - Promote table row sentences from metadata → standalone TextChunk nodes
  - Embed separately with voyage-context-3
  - Link via PART_OF → parent chunk
  - Add `chunk_type = "table_row"` property
  - Update retrieval to include table-row chunks
  - ~2-3 days

- [ ] **Reduce max_tokens from 1500 → 800** (if Phase 0 supports smaller chunks)
  - Change `SectionChunkConfig.max_tokens` in `chunker.py`
  - Re-index test corpus
  - Re-benchmark
  - ~1 day

- [ ] **Add content-type embedding prefixes** (experimental)
  - Prepend "Table row: " for table-row chunks
  - Prepend "Key-value: " for KVP chunks
  - Measure impact
  - ~0.5 day

### Medium-Term (Next Sprint)

- [ ] **Implement Option 2: Sentence-level retrieval** (if Phase 0 shows significant improvement)
  - New Sentence node type in Neo4j
  - Sentence extraction pipeline with DI noise filtering
  - Sentence vector index (`sentence_embeddings_v2`)
  - Parent-chunk expansion in retrieval path
  - ~1-2 weeks

- [ ] **Production deployment of chunking improvements**
  - Full re-index of all customer corpora
  - Update retrieval paths for new node types
  - Benchmark against production traffic

### Backlog

- [ ] **DI sentence noise filtering pipeline** — needed for both Option 1 and Option 2
- [ ] **spaCy integration for clean sentence boundaries** — alternative to DI language_spans
- [ ] **LLM-based sentence segmentation** — highest quality, highest cost option
- [ ] **Contextual header stacking** — prepend section_path to chunk text before embedding
- [ ] **Benchmark expansion** — need more queries, especially table-specific and domain-specific

---

## Environment Quick Reference

```
Neo4j:       neo4j+s://a86dcf63.databases.neo4j.io  (user=neo4j)
Azure OpenAI: graphrag-openai-8476.openai.azure.com  (AAD auth)
Voyage AI:   voyage-context-3, 2048-dim
Test corpus: test-5pdfs-v2-fix2 (5 docs, 18 chunks, 347 sentences)

Production env vars:
  DOC_SCOPE_ENABLED=1
  DOC_SCOPE_MIN_SCORE=1.5
  DENOISE_DOC_COHERENCE=1
  DOC_COHERENCE_PENALTY=0.2
  ROUTE2_MAX_SENTENCES_PER_CHUNK=8
  ROUTE3_MAX_SENTENCES_PER_CHUNK=8
  DENOISE_SCORE_WEIGHTED=1
  DENOISE_COMMUNITY_FILTER=1
  COMMUNITY_PENALTY=0.3
  DENOISE_SCORE_GAP=1
  SCORE_GAP_THRESHOLD=0.5
  SCORE_GAP_MIN_KEEP=6
  DENOISE_SEMANTIC_DEDUP=1
  SEMANTIC_DEDUP_THRESHOLD=0.92
```

## Key Files to Know

| File | What It Does |
|---|---|
| `src/worker/hybrid_v2/pipeline/synthesis.py` | Stage 3 synthesis (Route 2 + Route 3), citation fix, doc-scope |
| `src/worker/hybrid_v2/indexing/section_chunking/chunker.py` | Section-aware chunking (100-1500 tokens) |
| `src/worker/services/document_intelligence_service.py` | DI processing, table linearization, sentence extraction |
| `src/worker/hybrid_v2/embeddings/voyage_embed.py` | Voyage contextual embedding with bin-packing |
| `src/worker/hybrid_v2/pipeline/chunk_filters.py` | Post-chunking noise filters |
| `src/worker/hybrid_v2/indexing/lazygraphrag_pipeline.py` | Pipeline entry point for chunking |
| `ANALYSIS_CHUNKING_STRATEGY_DEEP_DIVE_2026-02-10.md` | Full analysis doc (created today) |

## Git State

```
Branch: main
HEAD:   8c024300 — "Fix NameError: remove stray _community_prompt_used references (Step 5 cleanup)"
         ↑ replaces a1dc27d2 via force-push (bad commit had 5 files, only 1 was intended)
Prior:  5505fae0 — "Route 3 denoise: default ON, skip Step 5, fix deploy safety"
Status: UNSTAGED local changes exist (from parallel session — do NOT commit yet):
  M  src/worker/hybrid_v2/pipeline/synthesis.py  (Document-Scoped Retrieval helpers)
  M  src/worker/hybrid_v2/indexing/text_store.py  (entity document coverage)
  M  src/worker/services/async_neo4j_service.py  (additional async methods)
  ??  ARCHITECTURE_DOCUMENT_AWARE_RETRIEVAL_2026-02-10.md
  ??  scripts/benchmark_route2_prompt_model_comparison.py
Deployed image: 8c024300-95 on both graphrag-api and graphrag-worker
```

---

## Session 2 Update (evening, same day)

### What Was Done

1. **Route 3 denoising defaults hardened** (commit `5505fae0`)
   - D1-D4 Python defaults changed `"0"` → `"1"` (always-on)
   - Shell defaults added to `deploy-graphrag.sh`
   - `ROUTE3_DENOISE_COMMUNITY_PROMPT` removed (Step 5 abandoned)
   - Deployed successfully

2. **NameError bugfix** (commit `8c024300`)
   - `_community_prompt_used` was referenced but never defined — leftover from Step 5
   - Fixed with `git add -p` to stage only the 2-line fix
   - Force-pushed to replace bad commit `a1dc27d2`
   - Redeployed

3. **Context vs. Output analysis** → see `ANALYSIS_ROUTE3_CONTEXT_VS_OUTPUT_2026-02-10.md`
   - Called API for Q-G1, Q-G3, Q-G6, Q-G10 with `include_context=true`
   - **Finding 1:** Retrieval gaps — PMA fee table truncated (3/7 items lost),
     warranty transferability missing, Contoso Lifts LLC missing
   - **Finding 2:** Co-occurrence preamble wastes ~500-650 tokens per query
   - **Finding 3:** LLM over-explains what's absent, repeats across output sections

### Route 3 Denoising Final Status

| Step | Env Var | Default | Status |
|------|---------|---------|--------|
| D1 Hash dedup | `ROUTE3_DENOISE_DEDUP` | `"1"` | ✅ Active |
| D2 Noise filter | `ROUTE3_DENOISE_NOISE_FILTER` | `"1"` | ✅ Active |
| D3 PPR scoring | `ROUTE3_DENOISE_PPR_SCORING` | `"1"` | ✅ Active |
| D4 Token budget | `ROUTE3_DENOISE_TOKEN_BUDGET` | `"1"` | ✅ Active |
| D5 Community prompt | — | — | ❌ Abandoned (1 win, 5 losses, avg -0.014) |

---

## Updated TODO List for Next Session

### From Context vs. Output Analysis (NEW — highest value)

- [ ] **P1: Fix PMA AGENT'S FEES chunk truncation**
  - 3 of 7 fee items (10% repair, $75/mo advertising, $35/hr scheduling) lost
  - Investigate: query Neo4j for actual PMA chunks, check section boundaries
  - Where: `src/worker/hybrid_v2/indexing/` chunking logic
  - Impact: High — fixes worst question (Q-G3)

- [ ] **P2: Strip co-occurrence relationships from context preamble**
  - "Entity → Entity: Co-occur in N chunk(s)" lines provide zero synthesis value
  - Where: context assembly in `synthesis.py` (`_build_graph_context_for_synthesis` or similar)
  - Action: Remove relationships section, keep entity descriptions
  - Impact: Medium — frees ~500 tokens/query for real content

- [ ] **P3: Prompt tuning to reduce answer verbosity**
  - LLM spends paragraphs on absence; repeats facts across Answer/Supporting/CrossRef
  - Where: `_generate_graph_response()` in `synthesis.py` (L800+)
  - Action: Add "Focus on stated facts. Mention absence briefly. No cross-section repetition."
  - Impact: Medium — more concise, actionable answers

- [ ] **P4: Investigate remaining retrieval gaps**
  - Warranty transferability clause (Q-G1) — check Neo4j for chunk existence
  - "Contoso Lifts LLC" entity (Q-G6) — check entity extraction
  - Impact: Low — 1 fact each

### From Earlier Session (carried forward)

- [ ] **Build Phase 0 chunking experiment script** (from Session 1 analysis)
- [ ] **Implement table row promotion** (if Phase 0 positive)
- [ ] **Add ROUTE3_DENOISE_* vars to `infra/main.bicep`** (low priority, defaults protect us)

---

*Start tomorrow with P1 (PMA chunk truncation investigation) — it's the highest-impact single fix.*
