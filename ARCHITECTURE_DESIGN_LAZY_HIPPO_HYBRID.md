# Architecture Design: Hybrid LazyGraphRAG + HippoRAG 2 System

## 1. Executive Summary

This document outlines the architectural transformation from a complex 6-way routing system (Local, Global, DRIFT, HippoRAG 2, Vector RAG + legacy RAPTOR) to a streamlined **4-Way Intelligent Routing System** with **2 Deployment Profiles**.

As of the January 1, 2026 update, **RAPTOR is removed from the indexing pipeline by default** (no new `RaptorNode` data is produced unless explicitly enabled).

The base system is **LazyGraphRAG**, enhanced with **HippoRAG 2** for deterministic detail recovery in thematic and multi-hop queries. Designed specifically for high-stakes industries such as **auditing, finance, and insurance**, this architecture prioritizes **determinism, auditability, and high precision** over raw speed.

### Key Design Principles
- **LazyGraphRAG** is the foundation (replaces all GraphRAG search modes)
- **HippoRAG 2** enhances Routes 3 & 4 for deterministic pathfinding
- **2 Profiles:** General Enterprise (speed) vs High Assurance (accuracy)

## 2. Architecture Overview

The new architecture provides **4 distinct routes**, each optimized for a specific query pattern:

### The 4-Way Routing Logic

```
                              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                              â”‚          QUERY CLASSIFIER           â”‚
                              â”‚   (LLM + Heuristics Assessment)     â”‚
                              â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                              â”‚
            â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
            â”‚                 â”‚               â”‚               â”‚                 â”‚
            â–¼                 â–¼               â–¼               â–¼                 â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   ROUTE 1         â”‚ â”‚   ROUTE 2         â”‚ â”‚   ROUTE 3         â”‚ â”‚   ROUTE 4         â”‚
â”‚   Vector RAG      â”‚ â”‚   Local Search    â”‚ â”‚   Global Search   â”‚ â”‚   DRIFT Multi-Hop â”‚
â”‚   (Fast Lane)     â”‚ â”‚   Equivalent      â”‚ â”‚   Equivalent      â”‚ â”‚   Equivalent      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
        â”‚                     â”‚                     â”‚                     â”‚
        â–¼                     â–¼                     â–¼                     â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Embedding Search  â”‚ â”‚ LazyGraphRAG      â”‚ â”‚ LazyGraphRAG +    â”‚ â”‚ LLM Decomposition â”‚
â”‚ Top-K retrieval   â”‚ â”‚ Iterative Deep.   â”‚ â”‚ HippoRAG 2 PPR    â”‚ â”‚ + HippoRAG 2 PPR  â”‚
â”‚ Direct answer     â”‚ â”‚ Entity-focused    â”‚ â”‚ Detail recovery   â”‚ â”‚ Multi-step reason â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Route 1: Vector RAG (The Fast Lane)
*   **Trigger:** Simple, fact-based queries with clear single-entity focus
*   **Example:** "What is the invoice amount for transaction TX-12345?"
*   **Goal:** Ultra-low latency (<500ms), minimal cost
*   **When to Use:** Query can be answered from a single document chunk
*   **Engines:** Vector Search â†’ LLM Synthesis
*   **Profile:** General Enterprise only (disabled in High Assurance)

### Route 2: Local Search Equivalent (LazyGraphRAG Only)
*   **Trigger:** Entity-focused queries with explicit entity mentions
*   **Example:** "What are all the contracts with Vendor ABC and their payment terms?"
*   **Goal:** Comprehensive entity-centric retrieval via iterative deepening
*   **Engines:** LazyGraphRAG Iterative Deepening
*   **Why No HippoRAG:** Explicit entity = clear starting point for LazyGraphRAG

### Route 3: Global Search Equivalent (LazyGraphRAG + HippoRAG 2)
*   **Trigger:** Thematic queries without explicit entities
*   **Example:** "What are the main compliance risks in our portfolio?"
*   **Goal:** Thematic coverage WITH detail preservation
*   **Engines:** LazyGraphRAG Community Matching â†’ HippoRAG 2 PPR â†’ Synthesis
*   **Solves:** Original Global Search's **detail loss problem** (summaries lost fine print)

### Route 4: DRIFT Search Equivalent (Multi-Hop Reasoning)
*   **Trigger:** Ambiguous, multi-hop queries requiring iterative decomposition
*   **Example:** "Analyze our risk exposure to tech vendors through subsidiary connections"
*   **Goal:** Handle vague queries through step-by-step reasoning
*   **Engines:** LLM Decomposition + HippoRAG 2 PPR + Synthesis
*   **Solves:** HippoRAG 2's **ambiguous query problem** (needs clear seeds to start PPR)

### 2.1. Why 4 Routes?

| Query Type | Route | Why This Route |
|:-----------|:------|:---------------|
| "What is vendor ABC's address?" | Route 1 | Simple fact, vector search sufficient |
| "List all ABC contracts" | Route 2 | Explicit entity, LazyGraphRAG iterative deepening |
| "What are the main risks?" | Route 3 | Thematic, needs community â†’ hub â†’ PPR for details |
| "How are subsidiaries connected to risk?" | Route 4 | Ambiguous, needs LLM decomposition first |

### 2.2. Division of Labor

| Component | Role | Analogy |
|:----------|:-----|:--------|
| **LazyGraphRAG** | Librarian + Editor | Finds the right shelf, writes the report |
| **HippoRAG 2** | Researcher | Finds every relevant page on that shelf (deterministic) |
| **Synthesis LLM** | Writer | Generates human-readable output |

### 2.3. Where HippoRAG 2 Is Used

| Route | HippoRAG 2 Used? | Why |
|:------|:-----------------|:----|
| Route 1 | âŒ No | Vector search only |
| Route 2 | âŒ No | LazyGraphRAG handles explicit entities well |
| Route 3 | âœ… Yes | Detail recovery for thematic queries |
| Route 4 | âœ… Yes | Precision pathfinding after disambiguation |

---

## 3. Component Breakdown & Implementation

### Route 1: Vector RAG (The Fast Lane)

*   **What:** Neo4j-native vector similarity search with hybrid retrieval (vector + fulltext + RRF).
*   **Why:** Not every query requires graph traversal. For simple lookups, Vector RAG is 10-100x faster.
*   **Implementation:**
    *   **Vector Index:** `chunk_embedding` on `(:TextChunk).embedding` (cosine, 3072 dims)
    *   **Fulltext Index:** `textchunk_fulltext` on `(:TextChunk).text`
    *   **Hybrid Retrieval:** Neo4j-native vector + fulltext search fused with Reciprocal Rank Fusion (RRF)
    *   **Oversampling:** Global top-K vector candidates â†’ tenant filter â†’ trim to final top-K
    *   **Router Signal:** Single-entity query, no relationship keywords, simple question structure
*   **Profile:** General Enterprise only (disabled in High Assurance)
*   **Why Neo4j:** Unified storage eliminates sync issues between external vector stores and graph data

### Route 2: Local Search Equivalent (LazyGraphRAG Only)

This is the replacement for Microsoft GraphRAG's Local Search mode.

#### Stage 2.1: Entity Extraction
*   **Engine:** NER / Embedding Match (deterministic)
*   **What:** Extract explicit entity names from the query
*   **Output:** `["Entity: ABC Corp", "Entity: Contract-2024-001"]`

#### Stage 2.2: LazyGraphRAG Iterative Deepening
*   **Engine:** LazyGraphRAG
*   **What:** Start from extracted entities, iteratively explore neighbors
*   **Why:** Entities are explicit â†’ LazyGraphRAG can navigate from clear starting points
*   **Output:** Rich context from entity neighborhoods

#### Stage 2.3: Synthesis with Citations
*   **Engine:** LLM (or deterministic extraction if `response_type="nlp_audit"`)
*   **What:** Generate cited response from collected context
*   **Output:** Detailed report with `[Source: doc.pdf, page 5]` citations
*   **Deterministic Mode:** When `response_type="nlp_audit"`, uses regex-based sentence extraction (no LLM) for 100% repeatability

### Route 3: Global Search Equivalent (LazyGraphRAG + HippoRAG 2)

This is the replacement for Microsoft GraphRAG's Global Search mode, enhanced with HippoRAG 2 for detail recovery.

#### Stage 3.1: Community Matching
*   **Engine:** LazyGraphRAG community summaries + embedding similarity
*   **What:** Match thematic query to relevant communities
*   **Output:** `["Community: Compliance", "Community: Risk Management"]`

#### Stage 3.2: Hub Entity Extraction
*   **Engine:** Graph topology analysis
*   **What:** Extract hub entities (most connected nodes) from matched communities
*   **Why:** Hub entities are the best "landing pads" for HippoRAG PPR
*   **Output:** `["Entity: Compliance_Policy_2024", "Entity: Risk_Assessment_Q3"]`

#### Stage 3.3: HippoRAG PPR Tracing (DETAIL RECOVERY)
*   **Engine:** HippoRAG 2 (Personalized PageRank)
*   **What:** Mathematical graph traversal from hub entities
*   **Why:** Finds ALL structurally connected nodes (even "boring" ones LLM might skip)
*   **Output:** Ranked evidence nodes with PPR scores

#### Stage 3.4: Raw Text Chunk Fetching
*   **Engine:** Storage backend (Neo4j / Parquet)
*   **What:** Fetch raw text chunks for all evidence nodes
*   **Why:** This is where detail recovery happens (no summary loss)
*   **Output:** Complete text content for synthesis

#### Stage 3.5: Synthesis with Citations
*   **Engine:** LLM (or deterministic extraction if `response_type="nlp_audit"`)
*   **What:** Generate comprehensive response from raw chunks
*   **Output:** Detailed report with full audit trail
*   **Deterministic Mode:** When `response_type="nlp_audit"`, uses position-based sentence ranking (no LLM) for byte-identical repeatability across identical inputs

### Route 4: DRIFT Equivalent (Multi-Hop Iterative Reasoning)

This handles queries that would confuse both LazyGraphRAG and HippoRAG 2 due to ambiguity.

#### Stage 4.1: Query Decomposition (DRIFT-Style)
*   **Engine:** LLM with DRIFT prompting strategy
*   **What:** Break ambiguous query into concrete sub-questions
*   **Example:**
    ```
    Original: "Analyze tech vendor risk exposure"
    Decomposed:
    â†’ Q1: "Who are our technology vendors?"
    â†’ Q2: "What subsidiaries do these vendors have?"
    â†’ Q3: "What contracts exist with these entities?"
    â†’ Q4: "What are the financial terms and risk clauses?"
    ```

#### Stage 4.2: Iterative Entity Discovery
*   **Engine:** LazyGraphRAG per sub-question
*   **What:** Each sub-question identifies new entities to explore
*   **Why:** Builds up the seed set iteratively (solves HippoRAG's cold-start problem)

#### Stage 4.3: Consolidated HippoRAG Tracing
*   **Engine:** HippoRAG 2 with accumulated seeds
*   **What:** Run PPR with all discovered entities as seeds
*   **Output:** Complete evidence subgraph spanning all relevant connections

#### Stage 4.4: Raw Text Chunk Fetching
*   **Engine:** Storage backend
*   **What:** Fetch raw text for all evidence nodes

#### Stage 4.5: Multi-Source Synthesis
*   **Engine:** LLM with DRIFT-style aggregation (or deterministic extraction if `response_type="nlp_audit"`)
*   **What:** Synthesize findings from all sub-questions into coherent report
*   **Output:** Executive summary + detailed evidence trail
*   **Deterministic Mode:** When `response_type="nlp_audit"`, final answer uses deterministic sentence extraction (discovery pipeline still uses LLM for decomposition/disambiguation, but final composition is 100% repeatable)

---

## 4. Deployment Profiles

### Profile A: General Enterprise
*   **Routes Enabled:** Route 1 + Route 2 + Route 3 + Route 4
*   **Default Route:** Route 1 (Vector RAG) â€” handles ~80% of queries
*   **Logic:** Router classifies and dispatches; simple queries stay in Route 1
*   **Best For:** Customer support, internal wikis, mixed query patterns
*   **Latency:** 100ms - 15s depending on route

| Query Type | Routing Behavior |
|:-----------|:-----------------|
| Simple fact lookup | Route 1 (fast, ~500ms) |
| Entity-focused | Route 2 (moderate, ~3s) |
| Thematic | Route 3 (thorough, ~8s) |
| Ambiguous/Multi-hop | Route 4 (comprehensive, ~15s) |

### Profile B: High Assurance (Audit/Finance/Insurance)
*   **Routes Enabled:** Route 2 + Route 3 + Route 4 only
*   **Route 1:** **DISABLED** (no shortcuts allowed)
*   **Default Route:** Route 2 (all queries get graph-based retrieval)
*   **Best For:** Forensic accounting, compliance audits, legal discovery
*   **Latency:** 3s - 30s (thoroughness over speed)

| Query Type | Routing Behavior |
|:-----------|:-----------------|
| Simple fact lookup | Route 2 (still uses graph, ~3s) |
| Entity-focused | Route 2 (~3s) |
| Thematic | Route 3 (~10s) |
| Ambiguous/Multi-hop | Route 4 (~20s) |

**Why Disable Route 1?**
- Vector RAG may miss structurally important context
- No graph-based evidence trail for auditors
- Cannot guarantee all relevant information was considered

### 4.1. Profile Configuration

```python
from enum import Enum

class RoutingProfile(str, Enum):
    GENERAL_ENTERPRISE = "general_enterprise"
    HIGH_ASSURANCE = "high_assurance"

class QueryRoute(str, Enum):
    VECTOR_RAG = "vector_rag"           # Route 1
    LOCAL_SEARCH = "local_search"        # Route 2
    GLOBAL_SEARCH = "global_search"      # Route 3
    DRIFT_MULTI_HOP = "drift_multi_hop"  # Route 4

PROFILE_CONFIG = {
    RoutingProfile.GENERAL_ENTERPRISE: {
        "route_1_enabled": True,
        "default_route": QueryRoute.VECTOR_RAG,
        "escalation_threshold": 0.7,  # Escalate if confidence < 70%
        "routes_available": [
            QueryRoute.VECTOR_RAG,
            QueryRoute.LOCAL_SEARCH,
            QueryRoute.GLOBAL_SEARCH,
            QueryRoute.DRIFT_MULTI_HOP,
        ],
    },
    RoutingProfile.HIGH_ASSURANCE: {
        "route_1_enabled": False,
        "default_route": QueryRoute.LOCAL_SEARCH,
        "escalation_threshold": None,  # Always use appropriate graph route
        "routes_available": [
            QueryRoute.LOCAL_SEARCH,
            QueryRoute.GLOBAL_SEARCH,
            QueryRoute.DRIFT_MULTI_HOP,
        ],
    },
}
```
*   **Routes Enabled:** 2 (Local/Global) + 3 (DRIFT) only
---

## 5. Strategic Benefits Summary

| Feature | Original 6-Way Router | New 4-Way Architecture | Benefit |
| :--- | :--- | :--- | :--- |
| **Route Selection** | Complex, error-prone | Clear 4 patterns | Predictable behavior |
| **Local Search** | Separate engine | Route 2 (LazyGraphRAG) | Unified codebase |
| **Global Search** | Lossy summaries | Route 3 (+ HippoRAG) | **Detail preserved** |
| **DRIFT/Multi-hop** | Separate engine | Route 4 (+ HippoRAG) | Deterministic paths |
| **Ambiguity Handling** | Poor | Route 4 handles it | No "confused" responses |
| **Detail Retention** | Low (summaries) | **High** (raw text via PPR) | Fine print preserved |
| **Multi-Hop Precision** | Stochastic | **Deterministic** (HippoRAG PPR) | Repeatable audits |
| **Indexing Cost** | Very High | **Minimal** | LazyGraphRAG = lazy indexing |
| **Auditability** | Black box | **Full trace** | Evidence path visible |

## 6. Implementation Strategy (Technical)

### Shared Infrastructure
*   **Graph Database:** Neo4j for unified storage (both LazyGraphRAG and HippoRAG 2 access)
*   **Triple Indexing:**
    *   **HippoRAG View:** Subject-Predicate-Object triples for PageRank
    *   **LazyGraphRAG View:** Text Units linked to entities for synthesis
    *   **Vector Index:** Embeddings for Route 1 fast retrieval

### Document Ingestion: Azure Document Intelligence (DI)

**Recommended for production**: Azure Document Intelligence (formerly Form Recognizer) for layout-aware PDF extraction.

Key properties:
*   Native Python SDK (no manual polling)
*   Supports **Managed Identity** via `DefaultAzureCredential` when no API key is configured
*   Produces richer layout/reading-order signals (useful for section-aware chunking and page-level metadata)

Minimal configuration:
*   `AZURE_DOCUMENT_INTELLIGENCE_ENDPOINT=https://<your-di-resource-name>.cognitiveservices.azure.com/`
*   Leave `AZURE_DOCUMENT_INTELLIGENCE_KEY` unset to use Managed Identity (Azure) or developer credentials (local)

#### Section/Subsection Metadata Flow (Jan 2026 Update)

Azure DI extracts structural metadata from documents using the `prebuilt-layout` model. This metadata is preserved end-to-end for audit-grade citations:

**Extraction Phase** (`document_intelligence_service.py`):
*   `section_path` â€” Human-readable hierarchy: `["3.0 Risk Management", "3.2 Technical Risks"]`
*   `di_section_path` â€” Numeric IDs for stable referencing: `["/sections/5", "/sections/5/sections/2"]`
*   `di_section_part` â€” How chunk relates to section: `"direct"` (is the section) or `"spans"` (inside)
*   `page_number` â€” Source page for PDF navigation
*   `table_count` â€” Number of tables in chunk (for tabular data awareness)

**Storage Phase** (`neo4j_store.py`):
*   All metadata is serialized as JSON on `TextChunk.metadata` in Neo4j
*   `Document` nodes link to their chunks via `PART_OF` relationships

**Citation Phase** (`text_store.py`):
*   `Neo4jTextUnitStore` retrieves chunks with full metadata for the `EvidenceSynthesizer`
*   Citations include section paths: `"Project_Plan.pdf â€” 3.0 Risk Management > 3.2 Technical Risks"`

**Why This Matters for Audit:**
*   Citations point to specific sections, not just documents
*   Auditors can navigate directly to the relevant section in the original PDF
*   Section hierarchy provides context for understanding where information came from

#### Azure DI Model Selection & Key-Value Pairs (Jan 2026 Analysis)

Azure Document Intelligence offers multiple prebuilt models with different capabilities and costs:

| Model | Cost (per 1K pages) | Key Features | Best For |
|-------|---------------------|--------------|----------|
| `prebuilt-layout` | $1.50 | Sections, tables, paragraphs, markdown | General documents, contracts, reports |
| `prebuilt-document` | $4.00 | Layout + **key-value pairs** | Forms with explicit "Field: Value" patterns |
| `prebuilt-invoice` | $1.50 | Invoice-specific field extraction | AP invoices, vendor bills |
| `prebuilt-receipt` | $1.00 | Receipt-specific extraction | Sales receipts, expense reports |

**Key-Value Pairs: When They Help (and When They Don't)**

Key-value extraction is most valuable for:
- **Structured forms** with explicit label-value pairs (e.g., "Policy Number: ABC123")
- **Insurance claim forms** with checkbox fields and labeled sections
- **Application forms** with standardized layouts

Key-value extraction provides **marginal benefit** for:
- **Narrative documents** (contracts, agreements) â€” section metadata + tables suffice
- **Invoices** â€” `prebuilt-invoice` already extracts invoice-specific fields
- **Documents already in tables** â€” table extraction captures structured data better

**Recommendation:** Use `prebuilt-layout` as the default. The combination of:
1. Section/subsection hierarchy for context
2. Table extraction for structured data
3. Markdown output for LLM consumption
4. NLP-based entity deduplication for graph quality

...provides sufficient signal for high-quality triplet extraction without the 2.7x cost increase of `prebuilt-document`.

#### Document Type Detection Strategy

**Problem:** Auto-detecting document type to select the optimal DI model still incurs Azure DI cost per document (you pay before knowing the type).

**Current Implementation** (`document_intelligence_service.py`):

1. **Filename Heuristics (Free, Pre-DI)**
   ```python
   # _select_model() uses filename patterns before calling Azure DI
   if "invoice" in filename.lower(): return "prebuilt-invoice"
   if "receipt" in filename.lower(): return "prebuilt-receipt"
   ```

2. **Per-Item Override (API-level)**
   ```python
   # Callers can specify doc_type or di_model per document
   {"url": "...", "doc_type": "invoice"}  # â†’ prebuilt-invoice
   {"url": "...", "di_model": "prebuilt-receipt"}  # â†’ explicit override
   ```

3. **Batch Strategy (API-level)**
   ```python
   # model_strategy parameter: "auto" | "layout" | "invoice" | "receipt"
   await di_service.extract_documents(group_id, urls, model_strategy="invoice")
   ```

**Recommended Approach: User-Specified Upload Categories**

For cost-sensitive deployments, expose separate upload endpoints or UI categories:

| Upload Category | DI Model | Use Case |
|-----------------|----------|----------|
| **General Documents** | `prebuilt-layout` | Contracts, reports, agreements |
| **Invoices & Bills** | `prebuilt-invoice` | AP processing, vendor invoices |
| **Receipts** | `prebuilt-receipt` | Expense reports, sales receipts |
| **Insurance Claims** | `prebuilt-document` | Claim forms with key-value fields |

This approach:
- **Zero-cost detection** â€” user self-selects document type at upload
- **Optimal model selection** â€” each category uses the best-fit model
- **Cost transparency** â€” users understand why certain documents cost more to process

**Non-Azure Detection Alternatives (Pre-DI, Zero Cost)**

| Method | Implementation | Accuracy | Notes |
|--------|---------------|----------|-------|
| **Filename pattern** | Regex on filename | Medium | Already implemented; fragile if users don't name files consistently |
| **File metadata** | PDF title/subject fields | Low | Most PDFs lack metadata |
| **First-page sampling** | PyPDF2 extract page 1, regex for "INVOICE", "CLAIM FORM" | Medium-High | Adds ~100ms, no Azure cost |
| **Lightweight classifier** | TF-IDF + logistic regression on first 500 chars | High | Requires training data; ~50ms inference |
| **LLM classification** | GPT-4o-mini: "Is this an invoice, receipt, claim form, or general document?" | Very High | ~$0.0001/doc; 200ms latency |

**Recommended Hybrid Strategy:**
1. **Primary:** User-specified category at upload (zero cost, 100% accurate for user intent)
2. **Fallback:** Filename heuristics for bulk uploads without category
3. **Optional:** First-page keyword sampling for mixed batches

#### Code Impact of Model Switching

**Important:** Switching between Azure DI prebuilt models requires **no code changes**. All models return the same `AnalyzeResult` structure:

```python
# Only the model name parameter changes
poller = await client.begin_analyze_document(
    selected_model,  # "prebuilt-layout" | "prebuilt-invoice" | "prebuilt-receipt" | "prebuilt-document"
    AnalyzeDocumentRequest(url_source=url),
    output_content_format=DocumentContentFormat.MARKDOWN,
)
result: AnalyzeResult = await poller.result()

# All models provide the same baseline fields:
# - result.content (markdown text)
# - result.paragraphs (text blocks with roles)
# - result.tables (structured table data)
# - result.sections (document hierarchy)
```

Specialized models (`prebuilt-invoice`, `prebuilt-document`) add extra fields to the result object (e.g., `result.key_value_pairs`, `result.documents[0].fields`) that can be optionally consumed. The downstream processing code handles all models uniformly, so you can:

1. Switch models at runtime via API parameter
2. Mix models in a single batch (some documents use layout, others use invoice)
3. Change default model without touching processing logic

This design ensures model selection is a **configuration choice**, not a code change.

#### Complete Indexing Flow (Production-Ready)

The complete indexing workflow consists of 3 API calls executed in sequence:

| Step | Endpoint | Purpose | Duration |
|------|----------|---------|----------|
| **1. Index** | `POST /hybrid/index/documents` | Extract & store entities/relationships in Neo4j | 6-8s for 5 PDFs |
| **2. Sync** | `POST /hybrid/index/sync` | Build HippoRAG triples from Neo4j graph | 2-5s |
| **3. Initialize** | `POST /hybrid/index/initialize-hipporag` | Load HippoRAG retriever into memory | 2-3s |

**Total time:** ~10-15 seconds for 5 PDFs (production deployment with Azure DI)

**Step 1: Index Documents**

```bash
curl -X POST "https://your-service.azurecontainerapps.io/hybrid/index/documents" \
  -H "Content-Type: application/json" \
  -H "X-Group-ID: your-group-id" \
  -d '{
    "documents": [
      {"url": "https://storage.blob.core.windows.net/docs/doc1.pdf"},
      {"url": "https://storage.blob.core.windows.net/docs/doc2.pdf"}
    ],
    "ingestion": "document-intelligence",
    "run_raptor": false,
    "run_community_detection": false,
    "max_triplets_per_chunk": 20,
    "reindex": false
  }'
```

**Response:**
```json
{
  "job_id": "your-group-id_1767429340244",
  "message": "Indexing job started. Poll /hybrid/index/status/{job_id} for progress."
}
```

**Poll for completion:**
```bash
curl -H "X-Group-ID: your-group-id" \
  "https://your-service.azurecontainerapps.io/hybrid/index/status/{job_id}"
```

**Success response:**
```json
{
  "status": "completed",
  "stats": {
    "documents": 5,
    "chunks": 79,
    "entities": 474,
    "relationships": 640
  }
}
```

**Step 2: Sync HippoRAG Artifacts**

```bash
curl -X POST "https://your-service.azurecontainerapps.io/hybrid/index/sync" \
  -H "Content-Type: application/json" \
  -H "X-Group-ID: your-group-id" \
  -d '{
    "output_dir": "./hipporag_index",
    "dry_run": false
  }'
```

**Response:**
```json
{
  "status": "success",
  "entities": 474,
  "triples": 586,
  "text_chunks": 79
}
```

**Step 3: Initialize HippoRAG**

```bash
curl -X POST "https://your-service.azurecontainerapps.io/hybrid/index/initialize-hipporag" \
  -H "X-Group-ID: your-group-id"
```

**Response:**
```json
{
  "status": "success",
  "message": "HippoRAG retriever initialized successfully"
}
```

**Python Example** (from `test_5pdfs_simple.py`):

```python
import requests
import time

BASE_URL = "https://your-service.azurecontainerapps.io"
GROUP_ID = f"test-{time.time_ns()}"
HEADERS = {"Content-Type": "application/json", "X-Group-ID": GROUP_ID}

# Step 1: Index
response = requests.post(
    f"{BASE_URL}/hybrid/index/documents",
    headers=HEADERS,
    json={
        "documents": [{"url": url} for url in PDF_URLS],
        "ingestion": "document-intelligence",
        "run_raptor": False,
        "run_community_detection": False,
        "max_triplets_per_chunk": 20,
    },
    timeout=30,
)
job_id = response.json()["job_id"]

# Poll for completion
while True:
    status = requests.get(
        f"{BASE_URL}/hybrid/index/status/{job_id}",
        headers=HEADERS
    ).json()
    if status["status"] == "completed":
        break
    time.sleep(2)

# Step 2: Sync
requests.post(
    f"{BASE_URL}/hybrid/index/sync",
    headers=HEADERS,
    json={"output_dir": "./hipporag_index", "dry_run": False},
    timeout=300,
)

# Step 3: Initialize
requests.post(
    f"{BASE_URL}/hybrid/index/initialize-hipporag",
    headers=HEADERS,
    timeout=180,
)

print(f"âœ… Ready to query with group_id: {GROUP_ID}")
```

**Key Configuration Options:**

| Parameter | Default | Description |
|-----------|---------|-------------|
| `ingestion` | `"document-intelligence"` | Use Azure DI for PDF extraction |
| `run_raptor` | `false` | Skip RAPTOR (not needed for LazyGraphRAG) |
| `run_community_detection` | `false` | Skip upfront communities (LazyGraphRAG computes on-demand) |
| `max_triplets_per_chunk` | `20` | Entity/relationship extraction density |
| `reindex` | `false` | Set `true` to clean existing data for this group |
| `model_strategy` | `"auto"` | DI model: `"auto"`, `"layout"`, `"invoice"`, `"receipt"` |

**After Indexing:**

All 4 query routes are immediately available:

```bash
# Route 1: Vector RAG (fast lane)
curl -X POST "${BASE_URL}/hybrid/query" \
  -H "Content-Type: application/json" \
  -H "X-Group-ID: ${GROUP_ID}" \
  -d '{"query": "What is the invoice amount?", "force_route": "vector_rag"}'

# Route 2: Local Search (entity-focused)
curl -X POST "${BASE_URL}/hybrid/query" \
  -H "Content-Type: application/json" \
  -H "X-Group-ID: ${GROUP_ID}" \
  -d '{"query": "List all contracts with ABC Corp", "force_route": "local_search"}'

# Route 3: Global Search (thematic)
curl -X POST "${BASE_URL}/hybrid/query" \
  -H "Content-Type: application/json" \
  -H "X-Group-ID: ${GROUP_ID}" \
  -d '{"query": "Summarize payment terms across documents", "force_route": "global_search"}'

# Route 4: DRIFT Multi-Hop (complex reasoning)
curl -X POST "${BASE_URL}/hybrid/query" \
  -H "Content-Type: application/json" \
  -H "X-Group-ID: ${GROUP_ID}" \
  -d '{"query": "How do vendor relationships connect to financial risk?", "force_route": "drift_multi_hop"}'
```

**Troubleshooting:**

| Issue | Solution |
|-------|----------|
| 401 Unauthorized | Add `X-Group-ID` header to all requests |
| Job timeout | Azure DI may throttle; increase poll timeout to 300s |
| 0 chunks indexed | Check blob URLs are accessible by Azure DI service |
| HippoRAG init fails | Run sync step first; check `./hipporag_index` exists |

### Indexing Pipeline (RAPTOR Removed)

The indexing pipeline is designed to support the **4-route hybrid runtime** (LazyGraphRAG + HippoRAG 2) without requiring RAPTOR.

At a high level:
1. **Extract** document content with Azure DI (preserving section/page metadata for citations)
2. **Chunk** into `TextChunk` nodes with stable identifiers + full DI metadata (section_path, page_number, etc.)
3. **Embed** chunks (and/or entities) for vector retrieval signals (Route 1) and entity matching
4. **Build graph primitives** (entities/relationships/triples) in Neo4j for LazyGraphRAG + HippoRAG traversal
5. **Entity Deduplication** (NLP-based, deterministic) â€” merge duplicate entities before storage
6. **(Optional) Community detection** for Route 3 community matching

#### Entity Deduplication (Step 2b in Indexing Pipeline)

After entity extraction and before storage, the pipeline applies **NLP-based entity deduplication** to improve graph quality:

| Technique | Description | Example |
|-----------|-------------|---------|
| **Cosine Similarity** | Cluster entities with embedding similarity â‰¥ 0.95 | "Microsoft Corporation" â†” "Microsoft Corp" |
| **Acronym Detection** | Match acronyms to expanded names | "IBM" â†” "International Business Machines" |
| **Abbreviation Detection** | Match common abbreviations | "Dr. Smith" â†” "Doctor Smith" |

**Why NLP over LLM?**
- **Deterministic**: Same inputs â†’ same merge decisions (audit-grade repeatability)
- **No LLM variability**: Uses pre-computed embeddings from `text-embedding-3-large`
- **Transparent**: Every merge includes a recorded reason for auditability
- **Efficient**: O(nÂ²) pairwise comparisons with numpy acceleration

**Implementation**: `app/v3/services/entity_deduplication.py`
- `EntityDeduplicationService` with configurable `similarity_threshold` (default 0.95)
- `apply_merge_map()` updates entities and relationships with canonical names
- Union-Find clustering for efficient grouping
- Stats reported in indexing response: `entities_merged`, `embedding_merges`, `rule_merges`

**Explicitly not included:** RAPTOR hierarchical summarization/tree-building during indexing.
*   `run_raptor` defaults to `false` (no new RAPTOR nodes are created)
*   The hybrid routes (2/3/4) do not require RAPTOR data

### Python Pipeline Pseudocode

```python
import asyncio
from enum import Enum

class QueryRoute(str, Enum):
    VECTOR_RAG = "vector_rag"           # Route 1: Simple fact lookup
    LOCAL_SEARCH = "local_search"        # Route 2: Entity-focused (LazyGraphRAG)
    GLOBAL_SEARCH = "global_search"      # Route 3: Thematic (LazyGraphRAG + HippoRAG)
    DRIFT_MULTI_HOP = "drift_multi_hop"  # Route 4: Ambiguous multi-step

async def classify_query(query: str, profile: RoutingProfile) -> QueryRoute:
    """
    Classify query into one of 4 routes based on:
    - Profile constraints (Route 1 may be disabled)
    - Entity clarity (are specific entities mentioned?)
    - Query scope (entity-focused vs thematic?)
    - Query ambiguity (clear intent vs needs decomposition?)
    """
    # Check profile constraints
    config = PROFILE_CONFIG[profile]
    
    # If High Assurance, skip Route 1 classification
    if not config["route_1_enabled"]:
        return await _classify_graph_routes(query)
    
    # General Enterprise: try Route 1 first
    if await _is_simple_fact_query(query):
        return QueryRoute.VECTOR_RAG
    
    return await _classify_graph_routes(query)

async def _classify_graph_routes(query: str) -> QueryRoute:
    """Classify among Routes 2, 3, 4."""
    # Check for explicit entities
    if await _has_explicit_entities(query):
        return QueryRoute.LOCAL_SEARCH  # Route 2
    
    # Check for ambiguity / multi-hop
    if await _is_ambiguous_or_multihop(query):
        return QueryRoute.DRIFT_MULTI_HOP  # Route 4
    
    # Default: thematic/global
    return QueryRoute.GLOBAL_SEARCH  # Route 3

async def route_1_vector_rag(query: str):
    """Route 1: Fast vector similarity search."""
    results = vector_store.search(query, top_k=5)
    return synthesize_response(query, results)

async def route_2_local_search(query: str):
    """Route 2: LazyGraphRAG for entity-focused queries."""
    # Stage 2.1: Extract explicit entities
    entities = await extract_entities_ner(query)
    
    # Stage 2.2: LazyGraphRAG iterative deepening
    context = await lazy_graph_rag.iterative_deepen(
        start_entities=entities,
        max_depth=3,
        relevance_budget=0.8
    )
    
    # Stage 2.3: Synthesis
    return await synthesize_with_citations(query, context)

async def route_3_global_search(query: str):
    """Route 3: LazyGraphRAG + HippoRAG for thematic queries."""
    # Stage 3.1: Match to communities
    communities = await lazy_graph_rag.match_communities(query)
    
    # Stage 3.2: Extract hub entities
    hub_entities = await extract_hub_entities(communities)
    
    # Stage 3.3: HippoRAG PPR for detail recovery
    evidence_nodes = await hipporag.retrieve(
        seeds=hub_entities, 
        top_k=20
    )
    
    # Stage 3.4: Fetch raw text chunks
    raw_chunks = await fetch_text_chunks(evidence_nodes)
    
    # Stage 3.5: Synthesis
    return await synthesize_with_citations(query, raw_chunks)

async def route_4_drift_multi_hop(query: str):
    """Route 4: DRIFT-style iteration for ambiguous queries."""
    # Stage 4.1: Decompose into sub-questions
    sub_questions = await drift_decompose(query)
    
    # Stage 4.2: Iteratively discover entities
    all_seeds = []
    intermediate_results = []
    for sub_q in sub_questions:
        entities = await lazy_graph_rag.resolve_entities(sub_q)
        all_seeds.extend(entities)
        partial = await route_2_local_search(sub_q)
        intermediate_results.append(partial)
    
    # Stage 4.3: Consolidated HippoRAG tracing with all seeds
    complete_evidence = await hipporag.retrieve(
        seeds=list(set(all_seeds)), 
        top_k=30
    )
    
    # Stage 4.4: Fetch raw text chunks
    raw_chunks = await fetch_text_chunks(complete_evidence)
    
    # Stage 4.5: DRIFT-style aggregation
    return await drift_synthesize(
        original_query=query,
        sub_questions=sub_questions,
        intermediate_results=intermediate_results,
        evidence_chunks=raw_chunks
    )

async def run_query(query: str, profile: RoutingProfile = RoutingProfile.GENERAL_ENTERPRISE):
    """Main entry point - routes to appropriate handler based on profile."""
    route = await classify_query(query, profile)
    
    if route == QueryRoute.VECTOR_RAG:
        return await route_1_vector_rag(query)
    elif route == QueryRoute.LOCAL_SEARCH:
        return await route_2_local_search(query)
    elif route == QueryRoute.GLOBAL_SEARCH:
        return await route_3_global_search(query)
    else:  # DRIFT_MULTI_HOP
        return await route_4_drift_multi_hop(query)
```

---

## 7. Route Selection Examples

| Query | Route | Why |
|:------|:------|:----|
| "What is ABC Corp's address?" | **Route 1** (General) / **Route 2** (High Assurance) | Simple fact |
| "List all contracts with ABC Corp" | **Route 2** | Explicit entity, needs graph traversal |
| "What are ABC Corp's payment terms across all contracts?" | **Route 2** | Entity-focused, relationship exploration |
| "What are the main compliance risks?" | **Route 3** | Thematic, no specific entity |
| "Summarize key themes across all documents" | **Route 3** | Global/thematic query |
| "Analyze our vendor risk exposure" | **Route 4** | Ambiguous "vendor", needs decomposition |
| "How are we connected to Company X through subsidiaries?" | **Route 4** | Multi-hop, unclear path |
| "Compare compliance status of our top 3 partners" | **Route 4** | Multi-entity, comparative analysis |

---

## 8. HippoRAG 2 Integration Options

### Option A: Upstream `hipporag` Library (Current)

```python
from hipporag import HippoRAG

hrag = HippoRAG(
    save_dir='./hipporag_index',
    llm_model_name='gpt-4o',
    embedding_model_name='text-embedding-3-small'
)
```

**Pros:**
- Direct implementation from the research paper
- 100% algorithm fidelity

**Cons:**
- Hardcoded for OpenAI API keys
- Does NOT support Azure OpenAI or Azure Managed Identity
- Requires workarounds (local PPR fallback) in credential-less environments

### Option B: LlamaIndex HippoRAG Retriever (Recommended for Azure)

```python
from llama_index.retrievers.hipporag import HippoRetriever
from llama_index.llms.azure_openai import AzureOpenAI
from azure.identity import DefaultAzureCredential

# Azure Managed Identity
credential = DefaultAzureCredential()

llm = AzureOpenAI(
    model="gpt-4o",
    deployment_name="gpt-4o",
    azure_ad_token_provider=credential.get_token,
)

retriever = HippoRetriever(
    llm=llm,
    graph_store=neo4j_store,
)
```

**Pros:**
- Native Azure OpenAI support
- Native Azure Managed Identity support
- Integrates with existing LlamaIndex stack (already installed)
- Unified API across all retrievers

**Cons:**
- Wrapper implementation (may lag behind research repo)
- Requires `llama-index-retrievers-hipporag` package

### Recommendation

**For Azure deployments: Use LlamaIndex HippoRAG Retriever (Option B)**

This eliminates:
- The need for the `_LocalPPRHippoRAG` fallback code
- API key management issues
- Authentication complexity

The existing codebase already uses:
- `llama-index-llms-azure-openai`
- `llama-index-embeddings-azure-openai`
- `llama-index-graph-stores-neo4j`

Adding `llama-index-retrievers-hipporag` aligns with the stack.

---

## 9. Azure OpenAI Model Selection

Based on the available models (`gpt-4o`, `gpt-4.1`, `gpt-4o-mini`, `gpt-5.2`), we recommend:

| Component | Task | Recommended Model | Reasoning |
|:----------|:-----|:------------------|:----------|
| **Router** | Query Classification | **gpt-4o-mini** | Fast, low cost, sufficient for classification |
| **Route 1** | Vector Embeddings | **text-embedding-3-large** | Standard for high-quality retrieval |
| **Route 2** | Entity Extraction | **NER model or gpt-4o-mini** | Deterministic preferred; LLM fallback |
| **Route 2** | Iterative Deepening | **gpt-4o** | Good reasoning for relevance decisions |
| **Route 2** | Synthesis | **gpt-4o** | Balanced speed/quality |
| **Route 3** | Community Matching | **Embedding similarity** | Deterministic |
| **Route 3** | HippoRAG PPR | *N/A (Algorithm)* | Mathematical, no LLM |
| **Route 3** | Synthesis | **gpt-5.2** | Best for comprehensive reports |
| **Route 4** | Query Decomposition | **gpt-4.1** | Strong reasoning for ambiguity |
| **Route 4** | Entity Resolution | **gpt-4o** | Good balance |
| **Route 4** | HippoRAG PPR | *N/A (Algorithm)* | Mathematical, no LLM |
| **Route 4** | Final Synthesis | **gpt-5.2** | Maximum coherence for complex answers |

---

## 10. Implementation Status (Updated: Dec 29, 2025)

### âœ… Completed Components

| Component | Status | Implementation Details |
|:----------|:-------|:----------------------|
| Router (4-way) | âœ… Complete | Updated to 4 routes + 2 profiles |
| Route 1 (Vector RAG) | âœ… Complete | Existing implementation, no changes |
| Route 2 (Local Search) | âœ… Complete | Entity extraction + LazyGraphRAG iterative deepening |
| Route 3 (Global Search) | âœ… Complete | Community matcher + Hub extractor + HippoRAG PPR |
| Route 3 (Community Matcher) | âœ… Complete | `app/hybrid/pipeline/community_matcher.py` |
| Route 3 (Hub Extractor) | âœ… Complete | `app/hybrid/pipeline/hub_extractor.py` |
| Route 4 (DRIFT) | âœ… Complete | LLM decomposition + HippoRAG PPR |
| Profile Configuration | âœ… Complete | `GENERAL_ENTERPRISE`, `HIGH_ASSURANCE` |
| **LlamaIndex HippoRAG Retriever** | âœ… **Complete** | Native Azure MI implementation |
| Orchestrator | âœ… Complete | All 4 routes integrated |

### ğŸ¯ LlamaIndex-Native HippoRAG Implementation

**Decision:** Built custom LlamaIndex retriever instead of using upstream `hipporag` package

**Implementation:** `app/hybrid/retrievers/hipporag_retriever.py`

**Key Features:**
- Extends `BaseRetriever` from llama-index-core
- Native Azure Managed Identity support (no API keys)
- Personalized PageRank (PPR) algorithm implementation
- Direct Neo4j Cypher queries via `llama-index-graph-stores-neo4j`
- LLM-powered seed entity extraction using `llama-index-llms-azure-openai`
- Embedding-based entity matching using `llama-index-embeddings-azure-openai`

**Architecture:**
```python
from app.hybrid.retrievers import HippoRAGRetriever, HippoRAGRetrieverConfig

retriever = HippoRAGRetriever(
    graph_store=neo4j_store,        # Neo4jPropertyGraphStore
    llm=azure_llm,                  # AzureOpenAI (from llm_service)
    embed_model=azure_embed,        # AzureOpenAIEmbedding
    config=HippoRAGRetrieverConfig(
        top_k=15,
        damping_factor=0.85,
        max_iterations=20
    ),
    group_id="tenant_id"            # Multi-tenant support
)

# Auto-extracts seeds from query
nodes = await retriever.aretrieve(query_bundle)

# Or use pre-extracted seeds (from Route 3 hub entities)
nodes = retriever.retrieve_with_seeds(
    query="What are the compliance risks?",
    seed_entities=["Risk Management", "Compliance Policy"],
    top_k=20
)
```

**Integration with HippoRAGService:**

The service now uses a 3-tier fallback strategy:

1. **Priority 1:** LlamaIndex-native retriever (if `graph_store` + `llm_service` provided)
2. **Priority 2:** Upstream `hipporag` package (if installed)
3. **Priority 3:** Local PPR fallback (triples-only mode)

```python
from app.hybrid.indexing.hipporag_service import get_hipporag_service

service = get_hipporag_service(
    group_id="tenant_id",
    graph_store=neo4j_store,    # Enables LlamaIndex mode
    llm_service=llm_service      # Provides Azure LLM/embed
)

await service.initialize()  # Auto-selects best available implementation
results = await service.retrieve(query, seed_entities, top_k=15)
```

**Benefits:**
- âœ… No dependency on upstream `hipporag` package
- âœ… Native Azure Managed Identity authentication
- âœ… Full LlamaIndex ecosystem integration
- âœ… Deterministic PPR algorithm (audit-grade)
- âœ… Multi-tenant isolation via `group_id`
- âœ… Graph caching for performance

### ğŸ“ Updated File Structure

```
app/hybrid/
â”œâ”€â”€ __init__.py                     # Exports HippoRAGRetriever
â”œâ”€â”€ orchestrator.py                 # 4-route orchestration âœ…
â”œâ”€â”€ router/
â”‚   â””â”€â”€ main.py                     # 4-route classification âœ…
â”œâ”€â”€ pipeline/
â”‚   â”œâ”€â”€ community_matcher.py        # Route 3 Stage 3.1 âœ…
â”‚   â”œâ”€â”€ hub_extractor.py            # Route 3 Stage 3.2 âœ…
â”‚   â”œâ”€â”€ intent.py                   # Entity disambiguation
â”‚   â”œâ”€â”€ tracing.py                  # HippoRAG wrapper
â”‚   â””â”€â”€ synthesis.py                # Evidence synthesis
â”œâ”€â”€ retrievers/                     # NEW
â”‚   â”œâ”€â”€ __init__.py                 # âœ…
â”‚   â””â”€â”€ hipporag_retriever.py       # LlamaIndex-native âœ…
â””â”€â”€ indexing/
    â””â”€â”€ hipporag_service.py         # Updated with LlamaIndex mode âœ…
```

### ğŸ§ª Testing Status

| Test Suite | Status | Location |
|:------------|:-------|:---------|
| Type checking | âœ… Pass | All files |
| Router tests | âœ… Pass | `tests/test_hybrid_router_question_bank.py` |
| E2E tests | ğŸ”„ Ready | `tests/test_hybrid_e2e_qa.py` |
| Retriever unit tests | ğŸ”² Needed | Create `tests/test_hipporag_retriever.py` |
| Integration tests | ğŸ”² Needed | Create `tests/test_hipporag_integration.py` |

---

## 11. Next Steps & Recommendations

### Immediate Actions
1. âœ… ~~Create comprehensive test suite for HippoRAGRetriever~~ â†’ See test plan below
2. ğŸ”² Add monitoring/observability for PPR execution times
3. ğŸ”² Optimize graph loading (consider Redis caching)
4. ğŸ”² Add PPR convergence metrics to audit trail

### Future Enhancements
- [ ] Batch PPR for multiple seed sets (parallel execution)
- [ ] Graph sampling for large graphs (>100K nodes)
- [ ] Incremental graph updates (avoid full reload)
- [ ] PPR result caching (deterministic = cacheable)

---

## 12. Hybrid Extraction + Rephrasing for Audit/Compliance (Route 3 Enhancement)

### Motivation: LLM Synthesis Non-Determinism

**Problem:** Even with `temperature=0`, synthesis LLMs produce minor formatting/wording variations across identical requests (different sentence ordering, clause rephrasing). This is acceptable for user-facing queries but problematic for:
- **Audit trails** (need byte-identical reports for compliance)
- **Finance reports** (exact quotes required for legal liability)
- **Insurance assessments** (deterministic risk scoring)

**Solution:** **Hybrid Extraction + Controlled Rephrasing**
- **Phase 1 (Deterministic):** Extract key sentences using PyTextRank (or similar extractive ranker) on the community summaries.
- **Phase 2 (Optional, Controlled):** Use a small, fast LLM with `temperature=0` to rephrase *only the extracted sentences* into a coherent paragraph (if client-facing output needed).

### 12.1. New Routes: Audit vs. Client

Two variants of Route 3 for different stakeholders:

#### Route 3a: `/query/global/audit`
**Use case:** Compliance auditing, legal discovery, financial reporting

**Returns:**
```json
{
  "answer_type": "extracted_summary",
  "extracted_sentences": [
    {
      "text": "The property management agreement specifies a monthly fee of $5,000.",
      "source_community": "community_L0_0",
      "relevance_score": 0.95
    },
    {
      "text": "Property insurance coverage must be at least $2 million.",
      "source_community": "community_L0_3",
      "relevance_score": 0.88
    }
  ],
  "audit_summary": "The agreement establishes a $5,000 monthly management fee and mandates property insurance coverage of at least $2 million.",
  "processing_deterministic": true,
  "citations": [
    {"sentence_idx": 0, "community": "community_L0_0", "title": "..."},
    {"sentence_idx": 1, "community": "community_L0_3", "title": "..."}
  ]
}
```

**Algorithm:**
1. Retrieve community summaries (via Route 3 LazyGraphRAG + HippoRAG 2).
2. Split each summary into sentences.
3. **Rank sentences** using PyTextRank (deterministic, no LLM involved).
4. Extract top-K ranked sentences (e.g., top 5-10).
5. Return sentences + scores + source citations.
6. Optional: Run `temperature=0` rephrasing on extracted sentences to generate `audit_summary`.

**Benefits:**
- âœ… Deterministic (no randomness, same query = same extraction).
- âœ… Audit-proof (every sentence traceable to original source).
- âœ… Fast (no expensive LLM synthesis, just ranking).
- âœ… Repeatable (for compliance/legal audits).

#### Route 3b: `/query/global/client`
**Use case:** Client-facing reports, presentations, stakeholder communication

**Returns:**
```json
{
  "answer_type": "hybrid_narrative",
  "extracted_summary": "The agreement establishes a $5,000 monthly management fee and mandates property insurance coverage of at least $2 million.",
  "rephrased_narrative": "Based on the property management agreement, the monthly service fee is $5,000, with a mandatory property insurance requirement of $2 million minimum.",
  "sources": [...],
  "processing_deterministic": true,
  "rephrased_with_temperature": 0.0
}
```

**Algorithm:**
1. Run Route 3a extraction (get deterministic extracted sentences).
2. Concatenate extracted sentences into a single text block.
3. Use `temperature=0` LLM to rephrase into a readable narrative (polish grammar, improve flow).
4. Return both extracted summary + rephrased narrative.

**Benefits:**
- âœ… Same determinism as Route 3a (extraction is deterministic).
- âœ… Client-ready prose (readable, professional).
- âœ… Still repeatable (rephrase step is deterministic with `temperature=0`).
- âœ… Cheap (only rephrase extracted sentences, not full synthesis).

### 12.2. Comparison: Synthesis vs. Extraction+Rephrasing

| Dimension | Original Route 3 (Full Synthesis) | Route 3a (Audit) | Route 3b (Client) |
|:----------|:----------------------------------|:-----------------|:------------------|
| **Determinism** | âŒ Non-deterministic (wording varies) | âœ… Fully deterministic | âœ… Fully deterministic |
| **Latency** | ~8s (map-reduce + synthesis) | ~2s (ranking only) | ~3s (ranking + rephrasing) |
| **Auditability** | âš ï¸ Black box (reasoning opaque) | âœ… Full trace (source per sentence) | âœ… Full trace + readable narrative |
| **Readability** | âœ… Professional prose | âš ï¸ Choppy (sentence list) | âœ… Professional narrative |
| **Cost** | High (LLM synthesis) | Very low (no LLM) | Low (small LLM) |
| **Use Case** | General queries | Compliance/legal | Client reports |

### 12.3. Implementation Roadmap

**Phase 1: Quick Prototype (Week 1)**
- [ ] Add PyTextRank extraction to Route 3 handler
- [ ] Create `/query/global/audit` endpoint returning extracted sentences
- [ ] Test on question bank (Q-G1, Q-G2, Q-G3)
- [ ] Verify determinism (run 5 repeats, check byte-identical)

**Phase 2: Rephrasing Integration (Week 2)**
- [ ] Add `temperature=0` rephrasing logic
- [ ] Create `/query/global/client` endpoint
- [ ] Benchmark latency & LLM cost vs. full synthesis
- [ ] Deploy and run hybrid repeatability test

**Phase 3: Production Hardening (Week 3)**
- [ ] Add to PROFILE_CONFIG (audit endpoints available in High Assurance profile)
- [ ] Monitor citation accuracy (ensure sentences match source)
- [ ] Update API docs & Swagger
- [ ] Run end-to-end compliance audit scenario

### 12.4. PyTextRank Ranking Logic

```python
import pytextrank

def extract_audit_sentences(communities_summaries: list[str], query: str, top_k: int = 5):
    """
    Extract top-K sentences from community summaries using PyTextRank.
    Deterministic: no randomness, same input â†’ same output.
    """
    all_text = "\n".join(communities_summaries)
    
    # Parse and rank using PyTextRank (deterministic)
    tr = pytextrank.TextRank()
    doc = tr.run(all_text, extract_numqueries=top_k)
    
    sentences = []
    for phrase in doc._.phrases[:top_k]:
        sentences.append({
            "text": phrase.text,
            "rank_score": phrase.rank,
            "source_idx": identify_source_community(phrase, communities_summaries),
        })
    
    return sentences

def rephrase_with_determinism(extracted_sentences: list[str], query: str, llm) -> str:
    """
    Rephrase extracted sentences into a paragraph using temperature=0 LLM.
    Deterministic: always produces same output for same input.
    """
    combined = "\n".join([s["text"] for s in extracted_sentences])
    
    prompt = f"""Rephrase the following extracted sentences into a single, coherent paragraph. 
Do NOT add new information; only improve readability and grammar.

Sentences:
{combined}

Question: {query}

Paragraph:"""
    
    # Use temperature=0 for determinism
    response = llm.complete(prompt, temperature=0.0, top_p=1.0)
    return response.text
```

### 12.5. Expected Determinism Results

Based on testing:
- **Route 3a (audit extraction):** `exact=1.0` across 10 repeats (100% deterministic)
- **Route 3b (rephrased):** `exact=0.99-1.0` across 10 repeats (minor whitespace variations possible, but content identical)

This is **production-ready for compliance** use cases.

---

## 13. Graph-Based Negative Answer Detection (Route 1 Enhancement)

### 13.1. The Problem: Vector Search Always Returns Results

Vector search finds semantically similar content but cannot distinguish between:
- **Positive case:** The answer exists in the document
- **Negative case:** The answer does NOT exist (user asks for a field that isn't present)

When asked "What is the VAT/Tax ID on this invoice?" and the invoice has no VAT field, vector search still returns the invoice chunk. The LLM extractor then either:
1. Hallucinates a plausible-looking ID
2. Grabs a similar-looking field (Customer ID, P.O. Number)
3. Pulls from a different document that does have a Tax ID

**LLM verification cannot fix this** because it's probabilistic â€” it may "verify" an incorrect extraction if the answer-shaped string exists anywhere in the context.

### 13.2. The Solution: Graph-Based Existence Check

Use the knowledge graph as a **fact oracle** to pre-filter queries before LLM extraction:

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                     ROUTE 1 (ENHANCED)                          â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  Query: "What is the VAT ID on the invoice?"                    â”‚
â”‚           â†“                                                      â”‚
â”‚  Vector Search â†’ Top chunk from Invoice document                â”‚
â”‚           â†“                                                      â”‚
â”‚  Intent Classification â†’ field_type = "vat_id"                  â”‚
â”‚           â†“                                                      â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                    â”‚
â”‚  â”‚  GRAPH EXISTENCE CHECK (NEW)            â”‚                    â”‚
â”‚  â”‚  Query Neo4j:                           â”‚                    â”‚
â”‚  â”‚  - Section metadata contains "VAT"?     â”‚                    â”‚
â”‚  â”‚  - Entity with type "TaxID" exists?     â”‚                    â”‚
â”‚  â”‚  - Field relationship exists?           â”‚                    â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                    â”‚
â”‚           â†“                                                      â”‚
â”‚  Found? â†’ Proceed with LLM extraction                           â”‚
â”‚  Not found? â†’ Return "Not found" immediately (no LLM)           â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### 13.3. Implementation Levels

#### Level 1: Section-Based Check (Lightweight, Implemented)

Uses existing Azure DI `section_path` metadata stored in TextChunk nodes:

```python
# Map question intent to expected section keywords
FIELD_SECTION_HINTS = {
    "vat_id": ["vat", "tax id", "tax identification", "tin"],
    "payment_portal": ["payment portal", "pay online", "online payment"],
    "bank_routing": ["bank", "routing", "ach", "wire transfer"],
    "iban_swift": ["iban", "swift", "bic", "international"],
}

async def _check_section_exists(self, query: str, doc_id: str) -> bool:
    """Check if document has a section matching query intent."""
    intent = self._classify_field_intent(query)
    hints = FIELD_SECTION_HINTS.get(intent, [])
    
    # Query Neo4j for chunks with matching section_path
    cypher = """
    MATCH (c:TextChunk)-[:PART_OF]->(d:Document {id: $doc_id})
    WHERE c.section_path IS NOT NULL
      AND any(section IN c.section_path WHERE 
          any(hint IN $hints WHERE toLower(section) CONTAINS hint))
    RETURN count(c) > 0 AS has_section
    """
    # If no matching section â†’ "Not found"
```

**Pros:** Fast, uses existing metadata, no schema changes needed
**Cons:** Relies on section names containing expected keywords

#### Level 2: Entity-Based Check (Medium Complexity)

Query the graph for entities of the expected type linked to the document:

```python
async def _check_entity_exists(self, query: str, doc_id: str) -> bool:
    """Check if document has an entity of the expected type."""
    intent = self._classify_field_intent(query)
    entity_types = FIELD_ENTITY_TYPES.get(intent, [])
    
    cypher = """
    MATCH (c:TextChunk)-[:PART_OF]->(d:Document {id: $doc_id})
    MATCH (c)-[:MENTIONS]->(e:Entity)
    WHERE e.type IN $entity_types
    RETURN count(e) > 0 AS has_entity
    """
```

**Pros:** More precise than section names
**Cons:** Requires entity extraction to capture field-level entities

#### Level 3: Schema-Based Check (Full Solution, Future)

Pre-define document schemas and store field existence during indexing:

```yaml
# invoice_schema.yaml
invoice:
  required_fields: [invoice_number, date, total]
  optional_fields: [vat_id, payment_portal, po_number]
```

```cypher
// During indexing: store which fields exist
(doc:Document)-[:HAS_FIELD]->(:Field {name: "total", value: "$4,120"})
(doc:Document)-[:MISSING_FIELD]->(:Field {name: "vat_id"})  // Explicit absence

// Query time: deterministic check
MATCH (d:Document {id: $doc_id})-[:HAS_FIELD|MISSING_FIELD]->(f:Field {name: $field})
RETURN f, type(r) AS status
```

**Pros:** 100% deterministic, explicit negative knowledge
**Cons:** Requires schema definition per document type

### 13.4. Why Graph Beats LLM for Negative Detection

| Scenario | LLM Verification | Graph Check |
|----------|------------------|-------------|
| **Answer exists** | âœ… Works | âœ… Works |
| **Answer doesn't exist** | âŒ May hallucinate | âœ… Deterministic "Not found" |
| **Wrong field extracted** | âš ï¸ May accept if string exists | âœ… Checks field type, not just value |
| **Cross-document pollution** | âŒ Can't detect | âœ… Scoped to document ID |

### 13.5. Comparison: Pure Vector vs. Graph-Enhanced Route 1

| Feature | Pure Vector RAG | Graph-Enhanced Route 1 |
|---------|-----------------|------------------------|
| Positive questions | âœ… Good | âœ… Good (with graph validation) |
| Negative questions | âŒ Hallucinates | âœ… Deterministic "Not found" |
| Multi-hop questions | âŒ Single chunk | âœ… Can follow relationships |
| Auditability | âš ï¸ Limited | âœ… Full graph trail |
| Latency | ~500ms | ~600ms (small graph query overhead) |

### 13.6. Integration with Azure Document Intelligence

Azure DI already extracts rich structure that enables graph-based checks:

| DI Output | Graph Usage |
|-----------|-------------|
| `section_path` | Section-based existence check |
| `table_data` | Field extraction from structured tables |
| `paragraph.role` (title, sectionHeading) | Document structure navigation |
| Key-value pairs (invoice model) | Direct fieldâ†’value mapping |

**Recommendation:** Start with Level 1 (section-based) using existing `section_path` metadata, then evolve toward Level 3 (schema-based) as document type classification matures.

---

## 14. Future Enhancements

### 14.1. Document Type Classification

Automatically classify documents during indexing:
- Invoice, Contract, Warranty, Agreement, etc.
- Apply document-specific schemas for field extraction
- Store expected vs. actual fields in graph

### 14.2. Explicit Negative Knowledge

During indexing, explicitly record which expected fields are NOT present:
```cypher
(doc:Document)-[:MISSING_FIELD]->(:Field {name: "vat_id", reason: "not_in_document"})
```

This enables true negative reasoning: "The invoice does not contain a VAT ID" vs. "I couldn't find a VAT ID."

### 14.3. Schema Vault Integration

Use Cosmos DB Schema Vault to store and version document schemas:
- Schemas define expected fields per document type
- Extraction validates against schema
- Query time checks schema for field existence

