"""
Stage 3: Synthesis & Evidence Validation (The "Analyst")

Uses LazyGraphRAG's Iterative Deepening to fetch raw text chunks
based on the evidence nodes from Stage 2, then synthesizes a
comprehensive, cited response.

Model Selection:
- Final Synthesis (Route 2/3): HYBRID_SYNTHESIS_MODEL (gpt-5.2) - Best coherence
- Intermediate Synthesis (Route 3): HYBRID_INTERMEDIATE_MODEL (gpt-4o) - Speed/quality balance
"""

from typing import List, Tuple, Optional, Dict, Any, TYPE_CHECKING
import hashlib
import os
import re
import structlog

from src.worker.hybrid_v2.services.extraction_service import ExtractionService
from src.worker.hybrid_v2.pipeline.enhanced_graph_retriever import EnhancedGraphContext
from src.worker.hybrid_v2.pipeline.chunk_filters import apply_noise_filters

logger = structlog.get_logger(__name__)

# ---------------------------------------------------------------------------
# Sentence-marker stripping — shared between production and benchmarks
# ---------------------------------------------------------------------------
# Matches Azure DI language_span citation markers: [1a], [2b], [12z] etc.
# These are generated by _build_cited_context / synthesize_with_graph_context
# when language_spans are available.  When the downstream code re-groups
# context (e.g. community-regrouping), the per-chunk sentence markers become
# meaningless and cause the LLM to over-cite (60+ markers per claim).
# Stripping converts context back to block-level [N] citations only.
_SENTENCE_CITATION_RE = re.compile(r"^\[\d+[a-z]\]\s*", re.MULTILINE)

# Matches skeleton retrieval metadata tags injected by route_2_local.py:
#   [Skeleton: paragraph, sim=0.751]   (Strategy A)
#   [Skeleton-B: related_to, sim=0.890] (Strategy B)
# These are useful for debugging but must be stripped before LLM synthesis
# because smaller models (gpt-4.1-mini) sometimes copy them into responses.
_SKELETON_TAG_RE = re.compile(r"\[Skeleton(?:-B)?:\s*[^\]]*\]\s*")


def strip_sentence_markers(text: str) -> str:
    """Remove sentence-level citation markers ([1a], [2b] …) from *text*.

    Leaves block-level markers ([1], [2] …) intact so the LLM can still
    cite at the chunk level.  Intended to be called **before** any context
    re-grouping (community-based, theme-based, etc.) that would scramble
    per-chunk sentence ordering.

    >>> strip_sentence_markers("[1a] First sentence.\\n[1b] Second.")
    'First sentence.\\nSecond.'
    """
    return _SENTENCE_CITATION_RE.sub("", text)


def strip_skeleton_tags(text: str) -> str:
    """Remove skeleton retrieval metadata tags from context before LLM synthesis.

    Tags like ``[Skeleton: paragraph, sim=0.751]`` are injected by
    route_2_local.py for structured logging.  Smaller models (gpt-4.1-mini)
    sometimes copy these verbatim into responses, polluting user output.
    Stripping is safe — the same information lives in chunk metadata.

    >>> strip_skeleton_tags("[Skeleton: paragraph, sim=0.751] The rent is $500.")
    'The rent is $500.'
    >>> strip_skeleton_tags("[Skeleton-B: related_to, sim=0.890] Fee is 15%.")
    'Fee is 15%.'
    """
    return _SKELETON_TAG_RE.sub("", text)


class EvidenceSynthesizer:
    """
    Takes evidence nodes from HippoRAG and generates a comprehensive
    response with citations using LazyGraphRAG's text retrieval.
    
    Model Selection:
    - Final answers: HYBRID_SYNTHESIS_MODEL (gpt-5.2) for maximum coherence
    - Route 3 intermediate steps: HYBRID_INTERMEDIATE_MODEL (gpt-4o) for speed
    
    Key Features:
    - Fetches RAW text chunks (not summaries) for full detail retention.
    - Enforces citation requirements for auditability.
    - Uses configurable "relevance budget" for precision control.
    """
    
    def __init__(
        self, 
        llm_client: Optional[Any],
        text_unit_store: Optional[Any] = None,
        relevance_budget: float = 0.8
    ):
        """
        Args:
            llm_client: The LLM client for synthesis.
            text_unit_store: Store containing raw text chunks.
            relevance_budget: 0.0-1.0, higher = more thorough (slower).
        """
        self.llm = llm_client
        self.text_store = text_unit_store
        self.relevance_budget = relevance_budget

    # Default token budget for LLM context window (configurable via SYNTHESIS_TOKEN_BUDGET env var).
    # Chunks are added in PPR-score order until this limit is reached.
    DEFAULT_TOKEN_BUDGET: int = 32_000

    _REFUSAL_MESSAGE = "Not found in the provided documents."

    def _detect_missing_field_refusal(
        self,
        query: str,
        text_chunks: List[Dict[str, Any]],
        response_type: str,
    ) -> Optional[str]:
        if response_type not in {"summary", "detailed_report"}:
            return None
        q = (query or "").casefold()
        if not q:
            return None

        evidence_text = " ".join(
            chunk.get("text", "")
            for chunk in text_chunks
            if isinstance(chunk, dict)
        )
        ev = evidence_text.casefold()

        def _has_any(terms: Tuple[str, ...]) -> bool:
            return any(t in ev for t in terms)

        if "california" in q and ("governed" in q or "law" in q or "laws" in q):
            if not self._has_governing_law_jurisdiction(evidence_text, "california"):
                return "california_law_missing"

        if "routing number" in q and "routing number" not in ev:
            return "routing_number_missing"

        if any(t in q for t in ("swift", "iban", "bic")) and not _has_any(("swift", "iban", "bic")):
            return "swift_iban_bic_missing"

        if "vat" in q and "vat" not in ev:
            return "vat_missing"

        if "tax id" in q and "tax id" not in ev:
            return "tax_id_missing"

        if "bank account number" in q and not _has_any(("bank account number", "account number")):
            return "bank_account_missing"

        if "shipped via" in q or "shipping method" in q:
            if not self._has_shipped_via_value(evidence_text):
                return "shipped_via_missing"

        if ("mold" in q or "mildew" in q) and ("clause" in q or "coverage" in q):
            if not _has_any(("mold", "mildew")):
                return "mold_clause_missing"

        return None

    def _has_shipped_via_value(self, evidence_text: str) -> bool:
        if not evidence_text:
            return False
        header_tokens = (
            "salesperson",
            "p.o.",
            "po",
            "p.o",
            "number",
            "requisitioner",
            "due date",
            "terms",
        )
        for match in re.finditer(r"shipped\s+via\s*[:\-]?\s*([^\n\r]+)", evidence_text, re.IGNORECASE):
            value = (match.group(1) or "").strip()
            if not value:
                continue
            value_norm = value.casefold()
            if any(tok in value_norm for tok in header_tokens):
                continue
            if re.fullmatch(r"[\W_]+", value):
                continue
            return True
        return False

    def _has_governing_law_jurisdiction(self, evidence_text: str, jurisdiction: str) -> bool:
        if not evidence_text or not jurisdiction:
            return False
        j = re.escape(jurisdiction.strip())
        patterns = (
            rf"governed\s+by\s+(the\s+)?(laws?|law)\s+of\s+(the\s+state\s+of\s+)?{j}",
            rf"governing\s+law[^\n\r]{0,80}{j}",
        )
        for pattern in patterns:
            if re.search(pattern, evidence_text, re.IGNORECASE):
                return True
        return False
    
    async def synthesize(
        self,
        query: str,
        evidence_nodes: List[Tuple[str, float]],
        response_type: str = "detailed_report",
        sub_questions: Optional[List[str]] = None,
        intermediate_context: Optional[List[Dict[str, Any]]] = None,
        coverage_chunks: Optional[List[Dict[str, Any]]] = None,
        prompt_variant: Optional[str] = None,
        synthesis_model: Optional[str] = None,
        include_context: bool = False,
        language_spans_by_doc: Optional[Dict[str, List[Dict[str, Any]]]] = None,
        pre_fetched_chunks: Optional[List[Dict[str, Any]]] = None,
    ) -> Dict[str, Any]:
        """
        Generate a comprehensive response with evidence citations.
        
        Args:
            query: The original user query.
            evidence_nodes: List of (entity_name, score) from Stage 2.
            response_type: "detailed_report" | "summary" | "audit_trail" | "nlp_audit" | "nlp_connected"
            sub_questions: Optional list of sub-questions (Route 3 DRIFT).
            intermediate_context: Optional intermediate results from sub-questions.
            coverage_chunks: Optional list of pre-retrieved chunks (e.g., from coverage retrieval).
            
        Returns:
            Dictionary containing:
            - response: The generated text response.
            - citations: List of source citations.
            - evidence_path: The nodes used to generate the response.
        """
        # Step 1: Retrieve raw text chunks for evidence nodes
        # _retrieve_text_chunks returns (deduped_chunks, entity_scores, retrieval_stats) after Feb 9 de-noising.
        retrieval_stats: Dict[str, Any] = {}
        if pre_fetched_chunks is not None:
            text_chunks = pre_fetched_chunks
            entity_scores: Dict[str, float] = {name: score for name, score in evidence_nodes}
        else:
            text_chunks, entity_scores, retrieval_stats = await self._retrieve_text_chunks(evidence_nodes, query=query, is_drift=sub_questions is not None)
        
        # Step 1.5: Merge coverage chunks if provided — with dedup against entity-retrieved chunks.
        # Coverage chunks bypass _retrieve_text_chunks() so they miss the denoising stack.
        # Apply MD5 dedup + semantic dedup here to prevent duplicate content injection.
        if coverage_chunks:
            import hashlib as _hl
            import re as _re_cov

            # Build hash set of existing entity-retrieved chunks
            existing_hashes: set = set()
            existing_word_sets: list = []
            for ec in text_chunks:
                t = ec.get("text", "")
                existing_hashes.add(_hl.md5(t.encode("utf-8")).hexdigest())
                existing_word_sets.append(set(_re_cov.findall(r'[a-z0-9]+', t.lower())))

            semantic_dedup_threshold_cov = float(os.environ.get("SEMANTIC_DEDUP_THRESHOLD", "0.92"))
            coverage_added = 0
            coverage_deduped = 0

            for cov_chunk in coverage_chunks:
                cov_text = cov_chunk.get("text", "")
                cov_hash = _hl.md5(cov_text.encode("utf-8")).hexdigest()

                # 1. Exact MD5 dedup
                if cov_hash in existing_hashes:
                    coverage_deduped += 1
                    continue

                # 2. Semantic near-dedup (Jaccard) against existing chunks
                cov_words = set(_re_cov.findall(r'[a-z0-9]+', cov_text.lower()))
                is_near_dup = False
                if cov_words:
                    for ws in existing_word_sets:
                        if ws:
                            inter = len(cov_words & ws)
                            union = len(cov_words | ws)
                            if union > 0 and inter / union >= semantic_dedup_threshold_cov:
                                is_near_dup = True
                                coverage_deduped += 1
                                break

                if not is_near_dup:
                    # Stamp coverage chunks with a score for token-budget ordering.
                    # If the chunk already has a score (e.g., sentence evidence with
                    # rerank scores), preserve it — only set a default for raw
                    # coverage gap-fill chunks.
                    if "_entity_score" not in cov_chunk:
                        min_entity_score = min(
                            (c.get("_entity_score", 0.0) for c in text_chunks),
                            default=0.0,
                        )
                        # Coverage chunks rank below entity-retrieved chunks
                        cov_chunk["_entity_score"] = min_entity_score * 0.5 if min_entity_score > 0 else 0.01
                    cov_chunk["_source_entity"] = cov_chunk.get("_source_entity", "__coverage_gap_fill__")

                    text_chunks.append(cov_chunk)
                    existing_hashes.add(cov_hash)
                    existing_word_sets.append(cov_words)
                    coverage_added += 1

            if coverage_deduped > 0 or coverage_added > 0:
                logger.info(
                    "coverage_chunks_merged_with_dedup",
                    original=len(coverage_chunks),
                    added=coverage_added,
                    deduped=coverage_deduped,
                )

        # Route 4 often produces generic "evidence" strings when seed entities don't resolve.
        # If entity-based retrieval returns nothing, fall back to query-based chunk retrieval.
        if not text_chunks and self.text_store and hasattr(self.text_store, "get_chunks_for_query"):
            try:
                text_chunks = await self.text_store.get_chunks_for_query(query)
                logger.info(
                    "text_chunks_query_fallback",
                    num_chunks=len(text_chunks),
                )
            except Exception as e:
                logger.warning("text_chunks_query_fallback_failed", error=str(e))

        refusal_reason = self._detect_missing_field_refusal(query, text_chunks, response_type)
        if refusal_reason:
            logger.info(
                "synthesis_refusal_missing_field",
                query=query,
                reason=refusal_reason,
                response_type=response_type,
            )
            return {
                "response": self._REFUSAL_MESSAGE,
                "citations": [],
                "evidence_path": [node for node, _ in evidence_nodes],
                "text_chunks_used": len(text_chunks),
                "sub_questions_addressed": sub_questions or [],
            }
        
        # nlp_audit mode: deterministic extraction only, no LLM synthesis
        if response_type == "nlp_audit":
            return await self._nlp_audit_extract(query, text_chunks, evidence_nodes)
        
        # nlp_connected mode: deterministic extraction + rephrasing with temperature=0
        if response_type == "nlp_connected":
            return await self._nlp_connected_extract(query, text_chunks, evidence_nodes)
        
        # comprehensive mode: 2-pass extraction (structured extraction → LLM enrichment)
        # This mode solves the LLM fact-dropping problem by extracting facts FIRST,
        # then using LLM only for comparison/explanation. Achieves 100% ground truth coverage.
        if response_type == "comprehensive":
            return await self._comprehensive_two_pass_extract(query, text_chunks, evidence_nodes)
        
        # comprehensive_sentence mode: sentence-level extraction using Azure DI language spans
        # This provides more precise context by traversing graph to extract individual sentences
        if response_type == "comprehensive_sentence":
            return await self._comprehensive_sentence_level_extract(query, text_chunks, evidence_nodes)
        
        # Step 2: Build context with citations (now groups by document for better reasoning)
        # Pass entity_scores so _build_cited_context can enforce token budget by PPR rank.
        context, citation_map, sentence_citation_map, context_stats = self._build_cited_context(
            text_chunks, language_spans_by_doc=language_spans_by_doc,
            entity_scores=entity_scores,
        )

        # ── Structural fix: strip sentence-level markers ──────────────
        # Sentence markers [1a], [1b] … cause citation explosion — the LLM
        # over-cites when presented with hundreds of fine-grained targets.
        # Strip to block-level [N] only; sentence text is preserved.
        context = strip_sentence_markers(context)
        sentence_citation_map.clear()

        # ── Metadata cleanup: strip skeleton retrieval tags ───────────
        # Tags like [Skeleton: paragraph, sim=0.751] are useful for logging
        # but cause gpt-4.1-mini to leak them into responses.  Strip before
        # LLM injection; metadata is preserved in coverage_chunks dicts.
        context = strip_skeleton_tags(context)

        # Step 2.5: Inject global document overview when retrieval is sparse.
        # This fixes Q-D7 (dates) and Q-D8 (comparisons) where PPR returns few/no entities.
        sparse_retrieval = len(text_chunks) < 3 or len(evidence_nodes) == 0
        if sparse_retrieval and self.text_store and hasattr(self.text_store, "get_workspace_document_overviews"):
            try:
                doc_overviews = await self.text_store.get_workspace_document_overviews(limit=20)
                if doc_overviews:
                    overview_section = self._format_document_overview(doc_overviews)
                    context = overview_section + "\n\n" + context
                    logger.info("injected_global_document_overview", num_docs=len(doc_overviews))
            except Exception as e:
                logger.warning("global_document_overview_injection_failed", error=str(e))
        
        # Step 3: For Route 3, add sub-question context
        if sub_questions and intermediate_context:
            context = self._enrich_context_for_drift(
                context, sub_questions, intermediate_context
            )
        
        # Step 3.5: Add sentence-level citation guidance when language spans are available
        # NOTE: After the structural strip above, sentence_citation_map is empty
        # so has_sentence_citations is False and this block is skipped.
        has_sentence_citations = bool(sentence_citation_map)
        if has_sentence_citations:
            sentence_hint = (
                "\n\nSENTENCE-LEVEL CITATIONS: Evidence is segmented at sentence granularity with markers "
                "like [1a], [1b], [2a], etc. Cite the specific sentence marker (e.g., [1a]) for each "
                "factual claim. When referencing numeric values, dates, or critical terms, quote exact "
                "wording from the cited sentence. You may also cite [N] to reference an entire evidence block."
            )
            context = context + sentence_hint
        
        # Step 4: Generate response with citation requirements
        response = await self._generate_response(
            query=query,
            context=context,
            response_type=response_type,
            sub_questions=sub_questions,
            prompt_variant=prompt_variant,
            synthesis_model=synthesis_model,
        )
        
        # Step 4b: Post-process v1_concise — strip any residual bracket
        # references the LLM may still emit despite "no citations" rule.
        effective_variant = (prompt_variant or "v0").lower().strip()
        if effective_variant == "v1_concise":
            response = re.sub(r"\s*\[\d+[a-z]?\]", "", response).strip()

        # Step 5: Build citations
        if effective_variant == "v1_concise" and citation_map:
            # v1_concise: LLM does pure extraction, no citation markup.
            # Attach top retrieval chunks directly — they ARE the source sentences.
            citations = []
            for cite_key in sorted(citation_map.keys(),
                                   key=lambda k: int(k.strip("[]")))[:3]:
                citations.append({
                    "citation": cite_key,
                    "citation_type": "retrieval",
                    **citation_map[cite_key],
                })
        else:
            # v0 / other variants: parse LLM-produced [N] markers
            citations = self._extract_citations(
                response, citation_map,
                sentence_citation_map=sentence_citation_map if sentence_citation_map else None,
            )
        
        # Measure final context (after sparse-retrieval injection + sentence hints)
        final_context_chars = len(context)
        final_context_tokens = self._estimate_tokens(context)
        context_stats["final_context_chars"] = final_context_chars
        context_stats["final_context_tokens"] = final_context_tokens
        # Merge retrieval-stage stats for per-measure attribution
        if retrieval_stats:
            context_stats["retrieval"] = retrieval_stats

        logger.info("synthesis_complete",
                   query=query,
                   num_citations=len(citations),
                   response_length=len(response),
                   response_type=response_type,
                   is_drift_mode=sub_questions is not None,
                   **context_stats)
        
        return {
            "response": response,
            "citations": citations,
            "evidence_path": [node for node, _ in evidence_nodes],
            "text_chunks_used": len(text_chunks),
            "sub_questions_addressed": sub_questions or [],
            "llm_context": context if include_context else None,
            "context_stats": context_stats,
        }
    
    async def synthesize_with_graph_context(
        self,
        query: str,
        evidence_nodes: List[Tuple[str, float]],
        graph_context: EnhancedGraphContext,
        response_type: str = "detailed_report",
        language_spans_by_doc: Optional[Dict[str, List[Dict[str, Any]]]] = None,
        community_data: Optional[List[Dict[str, Any]]] = None,
        include_context: bool = False,
    ) -> Dict[str, Any]:
        """
        Enhanced synthesis using full graph context (Route 3 v2.0).
        
        This method uses:
        1. Source chunks from MENTIONS edges (real citations!)
        2. Relationship context from RELATED_TO edges
        3. Entity descriptions for richer understanding
        4. Sentence-level segmentation from Azure DI language_spans (when available)
        
        Args:
            query: The original user query.
            evidence_nodes: List of (entity_name, score) from PPR.
            graph_context: EnhancedGraphContext with chunks, relationships.
            response_type: "detailed_report" | "summary" | "audit_trail"
            language_spans_by_doc: Optional dict mapping doc_id -> language span groups
                from Azure DI LANGUAGES feature for sentence-level citation granularity.
            
        Returns:
            Dictionary with response, citations, and evidence path.
        """
        # Special response types that need custom handling
        # IMPORTANT: Route 3 uses synthesize_with_graph_context(), so we must
        # handle these here (not only in synthesize()).
        if response_type in {"nlp_audit", "nlp_connected", "comprehensive"}:
            text_chunks: List[Dict[str, Any]] = []
            for i, chunk in enumerate(graph_context.source_chunks or []):
                section_str = " > ".join(chunk.section_path) if chunk.section_path else "General"
                source = chunk.document_source or chunk.document_title or "Unknown"
                text_chunks.append(
                    {
                        "id": chunk.chunk_id or f"chunk_{i}",
                        "source": source,
                        "section": section_str,
                        "entity": chunk.entity_name,
                        "text": chunk.text or "",
                        "document_id": chunk.document_id or "",
                        "document_title": chunk.document_title or "Unknown",
                        "document_source": chunk.document_source or "",
                        "metadata": {
                            "document_id": chunk.document_id or "",
                            "document_title": chunk.document_title or "Unknown",
                            "section_path_key": section_str,
                        }
                    }
                )

            if response_type == "nlp_audit":
                result = await self._nlp_audit_extract(query, text_chunks, evidence_nodes)
            elif response_type == "nlp_connected":
                result = await self._nlp_connected_extract(query, text_chunks, evidence_nodes)
            elif response_type == "comprehensive":
                result = await self._comprehensive_two_pass_extract(query, text_chunks, evidence_nodes)

            # Preserve graph context flags for downstream consumers.
            result.setdefault("graph_context_used", True)
            result.setdefault("relationships_used", len(graph_context.relationships))
            return result

        # ================================================================
        # Route 3 Context Distillation (Phase 0)
        # Each measure gated by env toggle for ablation testing.
        # ================================================================
        import hashlib as _hashlib
        _distill_stats = {"dedup_removed": 0, "noise_filtered": 0}

        # --- Step D1: Hash-based exact dedup ---
        # Same chunk fetched via multiple entities (56.5% measured duplication).
        # Default ON — proven 46% token reduction. Set ROUTE3_DENOISE_DEDUP=0 to disable.
        _enable_dedup = os.getenv("ROUTE3_DENOISE_DEDUP", "1").strip().lower() in {"1", "true", "yes"}
        if _enable_dedup:
            _seen_hashes: set = set()
            _deduped: list = []
            for chunk in graph_context.source_chunks:
                h = _hashlib.sha256((chunk.text or "").strip().encode()).hexdigest()
                if h not in _seen_hashes:
                    _seen_hashes.add(h)
                    _deduped.append(chunk)
            _distill_stats["dedup_removed"] = len(graph_context.source_chunks) - len(_deduped)
            graph_context.source_chunks = _deduped
            logger.info("route3_distill_dedup",
                       before=_distill_stats["dedup_removed"] + len(_deduped),
                       after=len(_deduped),
                       removed=_distill_stats["dedup_removed"])

        # --- Step D2: Content quality noise filter ---
        # Filter out non-informative fragments: form labels, bare headings, tiny snippets.
        # Default ON — safety net for noisy datasets. Set ROUTE3_DENOISE_NOISE_FILTER=0 to disable.
        _enable_noise_filter = os.getenv("ROUTE3_DENOISE_NOISE_FILTER", "1").strip().lower() in {"1", "true", "yes"}
        if _enable_noise_filter:
            import re as _re_noise
            _before_noise = len(graph_context.source_chunks)
            _filtered: list = []
            for chunk in graph_context.source_chunks:
                txt = (chunk.text or "").strip()
                txt_len = len(txt)
                # Rule 1: Form labels — short text ending with colon
                if txt_len < 40 and txt.endswith(":"):
                    continue
                # Rule 2: Bare headings — short, no sentence punctuation
                if txt_len < 50 and not _re_noise.search(r'[.?!]', txt):
                    continue
                # Rule 3: Tiny fragments — too short to be useful
                if txt_len < 15:
                    continue
                _filtered.append(chunk)
            _distill_stats["noise_filtered"] = _before_noise - len(_filtered)
            graph_context.source_chunks = _filtered
            logger.info("route3_distill_noise_filter",
                       before=_before_noise,
                       after=len(_filtered),
                       removed=_distill_stats["noise_filtered"])

        # --- Step D3: PPR-based chunk scoring and re-ranking ---
        # Use PPR entity scores to rank chunks. Each chunk has entity_name
        # indicating which entity retrieved it. Map PPR score → chunk score,
        # then sort descending so highest-relevance chunks come first.
        # Default ON — recovered containment to 0.815. Set ROUTE3_DENOISE_PPR_SCORING=0 to disable.
        _enable_ppr_scoring = os.getenv("ROUTE3_DENOISE_PPR_SCORING", "1").strip().lower() in {"1", "true", "yes"}
        _ppr_stats = {"enabled": _enable_ppr_scoring, "scored": 0, "unscored": 0}
        if _enable_ppr_scoring and evidence_nodes:
            # Build entity → PPR score lookup
            _entity_scores: Dict[str, float] = {}
            for ent_name, ent_score in evidence_nodes:
                _entity_scores[ent_name] = max(_entity_scores.get(ent_name, 0.0), ent_score)
            
            # Score each chunk via its entity_name
            for chunk in graph_context.source_chunks:
                ent = getattr(chunk, 'entity_name', '') or ''
                # BM25/coverage-fill chunks use marker names; give them a baseline score
                if ent in _entity_scores:
                    chunk.relevance_score = _entity_scores[ent]
                    _ppr_stats["scored"] += 1
                else:
                    # Non-entity chunks (bm25_phrase, coverage_fill, etc.)
                    # Keep existing relevance_score (from RRF) or assign floor
                    if chunk.relevance_score <= 0:
                        chunk.relevance_score = 0.01  # floor so they sort last
                    _ppr_stats["unscored"] += 1
            
            # Sort by PPR score descending
            graph_context.source_chunks.sort(
                key=lambda c: c.relevance_score, reverse=True
            )
            logger.info("route3_distill_ppr_scoring",
                       total=len(graph_context.source_chunks),
                       scored=_ppr_stats["scored"],
                       unscored=_ppr_stats["unscored"],
                       top_score=graph_context.source_chunks[0].relevance_score if graph_context.source_chunks else 0)

        # --- Step D4: Token budget enforcement ---
        # After PPR scoring and sorting, drop lowest-scored chunks that exceed
        # the token budget. Chunks are already sorted by relevance (D3), so we
        # keep top-scored chunks and trim from the tail.
        # Default ON — safety net for larger datasets. Set ROUTE3_DENOISE_TOKEN_BUDGET=0 to disable.
        _enable_token_budget = os.getenv("ROUTE3_DENOISE_TOKEN_BUDGET", "1").strip().lower() in {"1", "true", "yes"}
        _budget_stats = {"enabled": _enable_token_budget, "dropped": 0, "budget": 0}
        if _enable_token_budget and graph_context.source_chunks:
            _token_budget = self._get_token_budget()
            _budget_stats["budget"] = _token_budget
            _tokens_used = 0
            _budgeted: list = []
            for chunk in graph_context.source_chunks:
                chunk_tokens = self._estimate_tokens(chunk.text or "")
                if _tokens_used + chunk_tokens > _token_budget and _budgeted:
                    # Budget exceeded — stop adding chunks
                    break
                _tokens_used += chunk_tokens
                _budgeted.append(chunk)
            _budget_stats["dropped"] = len(graph_context.source_chunks) - len(_budgeted)
            _budget_stats["tokens_used"] = _tokens_used
            if _budget_stats["dropped"] > 0:
                logger.info("route3_distill_token_budget",
                           budget=_token_budget,
                           before=len(graph_context.source_chunks),
                           after=len(_budgeted),
                           dropped=_budget_stats["dropped"],
                           tokens_used=_tokens_used)
            graph_context.source_chunks = _budgeted

        # Step 1: Build citation context from source chunks (MENTIONS-derived)
        # Group chunks by document_id to ensure proper document attribution
        from collections import defaultdict
        doc_groups: Dict[str, List[Tuple[int, Any]]] = defaultdict(list)
        
        for i, chunk in enumerate(graph_context.source_chunks):
            # Use document_id as primary grouping key (graph ground truth)
            doc_key = chunk.document_id or chunk.document_source or chunk.document_title or "Unknown"
            doc_groups[doc_key].append((i, chunk))
        
        logger.info(
            "route3_document_grouping",
            num_chunks=len(graph_context.source_chunks),
            num_doc_groups=len(doc_groups),
            doc_keys=list(doc_groups.keys())[:10],
        )
        
        context_parts = []
        citation_map: Dict[str, Dict[str, Any]] = {}
        sentence_citation_map: Dict[str, Dict[str, Any]] = {}
        
        # Pre-compute sentence spans per chunk for sentence-level citations
        # language_spans_by_doc: {doc_id -> [{locale, confidence, spans: [{offset, length}]}]}
        _spans_by_doc = language_spans_by_doc or {}
        _sentence_segmentation_enabled = bool(_spans_by_doc)
        
        # Sentence segmentation is handled per-chunk in _get_sentences_for_chunk().
        # Each chunk is checked individually for offset availability — chunks
        # without offsets (e.g. BM25-sourced) gracefully fall back to chunk-level
        # citations while chunks WITH offsets get sentence-level granularity.
        
        def _get_sentences_for_chunk(chunk) -> List[Dict[str, Any]]:
            """Filter language spans that fall within this chunk's offset range."""
            if not _sentence_segmentation_enabled:
                return []
            doc_id = chunk.document_id or ""
            if doc_id not in _spans_by_doc:
                return []
            if chunk.start_offset is None or chunk.end_offset is None:
                return []
            
            chunk_start = chunk.start_offset
            chunk_end = chunk.end_offset
            sentences = []
            
            for lang_group in _spans_by_doc[doc_id]:
                confidence = lang_group.get("confidence", 1.0)
                locale = lang_group.get("locale", "en")
                for span in lang_group.get("spans", []):
                    s_offset = span.get("offset", 0)
                    s_length = span.get("length", 0)
                    # Span must fall entirely within the chunk's range
                    if s_offset >= chunk_start and s_offset + s_length <= chunk_end:
                        # Extract text from chunk by adjusting offset relative to chunk
                        relative_offset = s_offset - chunk_start
                        chunk_text = chunk.text or ""
                        if relative_offset >= 0 and relative_offset + s_length <= len(chunk_text):
                            sent_text = chunk_text[relative_offset:relative_offset + s_length].strip()
                        else:
                            # Offset mismatch (content may have been trimmed during chunking)
                            sent_text = ""
                        if sent_text and len(sent_text) >= 5:
                            sentences.append({
                                "text": sent_text,
                                "offset": s_offset,
                                "length": s_length,
                                "confidence": confidence,
                                "locale": locale,
                            })
            return sentences
        
        # Add unique document count header to help LLM with document-counting questions
        # This prevents LLM from counting sections/chunks as separate documents
        unique_doc_names = [
            (chunks[0][1].document_title or chunks[0][1].document_source or key)
            for key, chunks in doc_groups.items()
        ]
        context_parts.append(f"## Retrieved from {len(doc_groups)} unique source document(s): {', '.join(unique_doc_names)}\n")
        
        # Build context grouped by document
        for doc_key, chunks_with_idx in doc_groups.items():
            first_chunk = chunks_with_idx[0][1]
            doc_title = first_chunk.document_title or first_chunk.document_source or doc_key
            
            # Add document header for clearer LLM reasoning
            context_parts.append(f"=== DOCUMENT: {doc_title} ===")
            
            for original_idx, chunk in chunks_with_idx:
                citation_id = f"[{original_idx + 1}]"
                section_str = " > ".join(chunk.section_path) if chunk.section_path else "General"
                
                citation_map[citation_id] = {
                    "source": chunk.document_source or chunk.document_title,
                    "chunk_id": chunk.chunk_id,
                    "document": doc_title,  # Add document for proper attribution
                    "document_id": chunk.document_id or "",  # Include document ID from SourceChunk
                    "document_title": chunk.document_title or doc_title,  # Include document title
                    "section": section_str,
                    "entity": chunk.entity_name,
                    "text_preview": chunk.text[:150] + "..." if len(chunk.text) > 150 else chunk.text,
                    **({"page_number": chunk.page_number} if chunk.page_number is not None else {}),
                    **({"start_offset": chunk.start_offset} if chunk.start_offset is not None else {}),
                    **({"end_offset": chunk.end_offset} if chunk.end_offset is not None else {}),
                }
                
                # Try sentence-level segmentation from Azure DI language_spans
                chunk_sentences = _get_sentences_for_chunk(chunk)
                
                logger.info(
                    "sentence_segmentation_per_chunk",
                    chunk_id=chunk.chunk_id,
                    doc_id=chunk.document_id,
                    has_offsets=chunk.start_offset is not None and chunk.end_offset is not None,
                    start_offset=chunk.start_offset,
                    end_offset=chunk.end_offset,
                    doc_in_spans=bool(chunk.document_id and chunk.document_id in _spans_by_doc),
                    sentences_found=len(chunk_sentences),
                    segmentation_enabled=_sentence_segmentation_enabled,
                )
                
                if chunk_sentences:
                    # Cap sentences per chunk to prevent citation explosion.
                    # With 18 chunks × 20 sentences = 360 markers, the LLM over-cites.
                    # Default 8 keeps the most important opening sentences per chunk.
                    _max_sent = int(os.getenv("ROUTE3_MAX_SENTENCES_PER_CHUNK", "8"))
                    if _max_sent > 0 and len(chunk_sentences) > _max_sent:
                        chunk_sentences = chunk_sentences[:_max_sent]
                    
                    # Format as individually-citable sentences: [1a], [1b], [1c]...
                    chunk_num = original_idx + 1
                    entry_lines = [f"{citation_id} [Section: {section_str}] [Entity: {chunk.entity_name}]"]
                    for s_idx, sent in enumerate(chunk_sentences):
                        suffix = chr(ord('a') + s_idx) if s_idx < 26 else str(s_idx)
                        sent_citation_id = f"[{chunk_num}{suffix}]"
                        entry_lines.append(f"{sent_citation_id} {sent['text']}")
                        # Build sentence citation map entry
                        sentence_citation_map[sent_citation_id] = {
                            "source": chunk.document_source or chunk.document_title,
                            "chunk_id": chunk.chunk_id,
                            "document": doc_title,
                            "document_id": chunk.document_id or "",
                            "document_title": chunk.document_title or doc_title,
                            "section": section_str,
                            "entity": chunk.entity_name,
                            "text_preview": sent["text"][:150],
                            "sentence_text": sent["text"],
                            "sentence_offset": sent["offset"],
                            "sentence_length": sent["length"],
                            "sentence_confidence": sent["confidence"],
                            "sentence_locale": sent["locale"],
                            **({"page_number": chunk.page_number} if chunk.page_number is not None else {}),
                        }
                    entry = "\n".join(entry_lines)
                else:
                    # Fallback: standard chunk-level citation
                    entry = f"{citation_id} [Section: {section_str}] [Entity: {chunk.entity_name}]\n{chunk.text}"
                context_parts.append(entry)
            
            context_parts.append("")  # Blank line between documents
        
        # Step 2: Add relationship context
        relationship_context = graph_context.get_relationship_context()
        
        # Step 3: Add entity descriptions
        entity_context = ""
        if graph_context.entity_descriptions:
            entity_lines = ["## Entity Descriptions:"]
            for name, desc in list(graph_context.entity_descriptions.items())[:10]:
                if desc:
                    entity_lines.append(f"- **{name}**: {desc[:200]}")
            entity_context = "\n".join(entity_lines)
        
        # Step 4: Combine all context
        full_context = "\n\n".join(context_parts)
        if relationship_context:
            full_context = relationship_context + "\n\n" + full_context
        if entity_context:
            full_context = entity_context + "\n\n" + full_context

        # ── Structural fix: strip sentence-level markers ──────────────
        # Route 3 global search produces large context (18+ chunks × 20+
        # sentences = 360+ individually-citable [Na] markers).  The LLM
        # over-cites, emitting 60+ inline markers per claim.  Stripping
        # converts to block-level [N] citations only while preserving the
        # sentence text for full information coverage.
        full_context = strip_sentence_markers(full_context)
        sentence_citation_map.clear()
        logger.info("route3_sentence_markers_stripped")

        # Validate that we have source chunks from MENTIONS edges
        if not graph_context.source_chunks:
            logger.error("no_source_chunks_from_mentions",
                        hub_entities=graph_context.hub_entities,
                        num_relationships=len(graph_context.relationships))
            raise RuntimeError(
                f"No source chunks found via MENTIONS edges for hub entities: {graph_context.hub_entities}. "
                "This indicates the group may not have been properly indexed with entity extraction, "
                "or the entities don't have MENTIONS relationships to TextChunks."
            )
        
        # Step 5: Generate response
        response = await self._generate_graph_response(
            query=query,
            context=full_context,
            hub_entities=graph_context.hub_entities,
            response_type=response_type,
            has_sentence_citations=bool(sentence_citation_map),
            has_community_context=False,
        )
        
        # Step 6: Extract citations from response
        citations = self._extract_citations(
            response, citation_map,
            sentence_citation_map=sentence_citation_map if sentence_citation_map else None,
        )
        
        logger.info("synthesis_with_graph_context_complete",
                   query=query[:50],
                   num_source_chunks=len(graph_context.source_chunks),
                   num_relationships=len(graph_context.relationships),
                   num_citations=len(citations),
                   num_sentence_citations=len(sentence_citation_map),
                   response_length=len(response),
                   final_context_chars=len(full_context),
                   final_context_tokens=self._estimate_tokens(full_context))
        
        context_stats = {
            "chunks_before_budget": len(graph_context.source_chunks) + _distill_stats["dedup_removed"] + _distill_stats["noise_filtered"] + _budget_stats["dropped"],
            "chunks_after_budget": len(graph_context.source_chunks),
            "chunks_dropped": _budget_stats["dropped"],
            "dedup_removed": _distill_stats["dedup_removed"],
            "noise_filtered": _distill_stats["noise_filtered"],
            "ppr_scoring": _ppr_stats,
            "token_budget_enforcement": _budget_stats,
            "context_tokens": self._estimate_tokens(full_context),
            "context_chars": len(full_context),
            "final_context_chars": len(full_context),
            "final_context_tokens": self._estimate_tokens(full_context),
            "token_budget": self._get_token_budget(),
            "num_doc_groups": len(doc_groups),
            "community_prompt": False,
        }

        return {
            "response": response,
            "citations": citations,
            "evidence_path": [node for node, _ in evidence_nodes],
            "text_chunks_used": len(graph_context.source_chunks),
            "graph_context_used": True,
            "relationships_used": len(graph_context.relationships),
            "llm_context": full_context if include_context else None,
            "context_stats": context_stats,
        }
    
    async def _generate_graph_response(
        self,
        query: str,
        context: str,
        hub_entities: List[str],
        response_type: str,
        has_sentence_citations: bool = False,
        has_community_context: bool = False,
    ) -> str:
        """Generate response with graph-aware prompting."""
        if self.llm is None:
            logger.error("llm_not_configured")
            return "Error: LLM client not configured"
        
        hub_str = ", ".join(hub_entities[:5]) if hub_entities else "various"

        ql = (query or "").lower()
        reporting_hint = ""
        if any(k in ql for k in ["reporting", "record-keeping", "record keeping", "recordkeeping"]):
            reporting_hint = """

    Additional requirements for reporting/record-keeping questions:
    - Enumerate distinct reporting/record-keeping obligations as bullet points.
    - For each bullet, cite at least one source chunk that explicitly states it.
    - If the evidence uses specific wording (e.g., periodic statements, income/expenses, volumes/pumper/county), quote those phrases verbatim and cite them.
    """

        termination_hint = ""
        if any(k in ql for k in ["termination", "terminate", "cancel", "cancellation"]):
            termination_hint = """

        Additional requirements for termination/cancellation questions:
        - Explicitly list each distinct notice period and cancellation window using digits (e.g., "3 business days", "60 days", "10 business days") when present.
        - State the refund/forfeiture outcome for each cancellation window (e.g., full refund vs deposit forfeited).
        - If a document has no termination/cancellation mechanism, state that explicitly.
        """
        
        # Sentence-level citation guidance when Azure DI language_spans are available
        sentence_hint = ""
        if has_sentence_citations:
            sentence_hint = """

SENTENCE-LEVEL CITATIONS: Evidence is segmented at sentence granularity with markers
like [1a], [1b], [2a], etc. Cite the specific sentence marker (e.g., [1a]) for each
factual claim. When referencing numeric values, dates, or critical terms, quote exact
wording from the cited sentence. You may also cite [N] to reference an entire evidence block."""
        
        citation_instruction = "Cites specific sources for EVERY claim using [N] notation"
        if has_sentence_citations:
            citation_instruction = "Cites specific sources for EVERY claim using sentence markers [Na] (e.g., [1a], [2b]) or block markers [N]"
        
        prompt = f"""You are an expert analyst generating a response using knowledge graph evidence.

CRITICAL REQUIREMENT: You MUST cite your sources using the citation markers (e.g., [1], [2], [1a], [1b])
for EVERY factual claim. Citations link to source documents via entity relationships.
{sentence_hint}

Query: {query}

Hub Entities (Key Topics): {hub_str}

Evidence Context (organized by entity relationships and document sections):
{context}

{reporting_hint}

{termination_hint}

Generate a comprehensive {response_type.replace('_', ' ')} that:
1. Directly answers the query
2. {citation_instruction}
3. Leverages the entity relationships to explain connections
4. Organizes information by document sections where relevant
5. Highlights cross-references between different sources
6. Includes any explicit numeric values found in evidence (e.g., dollar amounts, time periods/deadlines, percentages, counts) verbatim

Use this output format:

## Answer

[Direct answer with citations]

## Supporting Details

- [Key detail 1 with citation]
- [Additional details as needed, covering ALL source documents]

## Cross-References

[Connections between different sources or entities, with citations]

Response:"""

        try:
            response = await self.llm.acomplete(prompt)
            return response.text.strip()
        except Exception as e:
            logger.error("graph_response_generation_failed", error=str(e))
            return f"Error generating response: {str(e)}"
    
    def _enrich_context_for_drift(
        self,
        base_context: str,
        sub_questions: List[str],
        intermediate_context: List[Dict[str, Any]]
    ) -> str:
        """Add structured sub-question context for DRIFT-style synthesis."""
        drift_section = "\n\n## Sub-Question Analysis:\n"
        
        for i, (sub_q, result) in enumerate(zip(sub_questions, intermediate_context), 1):
            drift_section += f"\n### Q{i}: {sub_q}\n"
            drift_section += f"- Entities identified: {', '.join(result.get('entities', []))}\n"
            drift_section += f"- Evidence points: {result.get('evidence_count', 0)}\n"
        
        return base_context + drift_section

    def _format_document_overview(self, doc_overviews: List[Dict[str, Any]]) -> str:
        """Format global document overview for corpus-level reasoning.
        
        This enables the LLM to answer questions like:
        - "What is the latest date across all documents?"
        - "Which document contains more X?"
        - "Compare document A and document B"
        
        Args:
            doc_overviews: List of document metadata dicts.
            
        Returns:
            Formatted overview string to prepend to context.
        """
        lines = ["## Available Documents in Corpus:\n"]
        for i, doc in enumerate(doc_overviews, 1):
            title = doc.get("title", "Untitled")
            date = doc.get("date", "")
            summary = doc.get("summary", "")
            chunk_count = doc.get("chunk_count", 0)
            
            # Build a concise entry
            entry = f"{i}. **{title}**"
            if date:
                entry += f" (Date: {date})"
            if chunk_count:
                entry += f" [{chunk_count} sections]"
            if summary:
                # Truncate long summaries
                summary_preview = summary[:200] + "..." if len(summary) > 200 else summary
                entry += f"\n   Summary: {summary_preview}"
            lines.append(entry)
        
        lines.append("\n---\n")
        return "\n".join(lines)

    # ------------------------------------------------------------------
    # Document-Scoped Retrieval helpers  (February 10, 2026)
    # ------------------------------------------------------------------

    @staticmethod
    def _resolve_target_documents(
        seed_entities: List[str],
        doc_coverage: List[Dict[str, Any]],
    ) -> Tuple[Optional[List[str]], Dict[str, Any]]:
        """IDF-weighted entity→document voting to determine target document(s).

        Each seed entity votes for every document it appears in, but its vote
        is inversely proportional to how many documents it spans (IDF-like).
        A single-doc entity votes 1.0; a 4-doc super-connector votes 0.25.

        Returns:
            (target_document_ids, stats_dict)
            target_document_ids is None when scoping is inconclusive.
        """
        min_score = float(os.environ.get("DOC_SCOPE_MIN_SCORE", "1.5"))

        if not doc_coverage:
            return None, {"reason": "no_coverage_data"}

        # Build per-entity doc-count map from the first record's metadata
        entity_doc_counts: Dict[str, int] = doc_coverage[0].get("entity_doc_counts", {})

        # Score each document with IDF-weighted entity votes
        doc_scores: Dict[str, float] = {}
        doc_titles: Dict[str, str] = {}
        doc_seeds: Dict[str, List[str]] = {}

        for rec in doc_coverage:
            doc_id = rec["doc_id"]
            doc_titles[doc_id] = rec.get("doc_title", "?")
            doc_seeds[doc_id] = rec.get("matching_seeds", [])

            score = 0.0
            for seed_name in rec.get("matching_seeds", []):
                n_docs = entity_doc_counts.get(seed_name, 1)
                score += 1.0 / max(n_docs, 1)  # IDF weight
            doc_scores[doc_id] = score

        if not doc_scores:
            return None, {"reason": "empty_scores"}

        ranked = sorted(doc_scores.items(), key=lambda kv: kv[1], reverse=True)
        top_id, top_score = ranked[0]
        second_score = ranked[1][1] if len(ranked) > 1 else 0.0

        stats: Dict[str, Any] = {
            "enabled": True,
            "scores": {doc_titles.get(d, d): round(s, 3) for d, s in ranked[:5]},
            "top_doc": doc_titles.get(top_id, top_id),
            "top_score": round(top_score, 3),
            "second_score": round(second_score, 3),
            "min_score_threshold": min_score,
            "entity_doc_counts": entity_doc_counts,
        }

        # Decision logic
        if top_score < min_score:
            # Low confidence — likely cross-document question or vague query
            stats["decision"] = "skip_low_confidence"
            logger.info("doc_scope_skip", reason="low_confidence", top_score=round(top_score, 3))
            return None, stats

        total_seeds = len(seed_entities)
        if total_seeds > 0 and top_score / total_seeds < 0.5:
            # No dominant document — cross-document question
            stats["decision"] = "skip_cross_document"
            logger.info("doc_scope_skip", reason="cross_document", ratio=round(top_score / total_seeds, 3))
            return None, stats

        if len(ranked) > 1 and second_score > 0 and top_score / second_score < 1.5:
            # Two nearly-equal documents — include both
            second_id = ranked[1][0]
            target = [top_id, second_id]
            stats["decision"] = "dual_document"
            logger.info(
                "doc_scope_resolved",
                target_docs=[doc_titles.get(d, d) for d in target],
                scores=[round(doc_scores[d], 3) for d in target],
            )
            return target, stats

        # Single dominant document
        stats["decision"] = "single_document"
        logger.info(
            "doc_scope_resolved",
            target_doc=doc_titles.get(top_id, top_id),
            score=round(top_score, 3),
        )
        return [top_id], stats
    
    async def _retrieve_text_chunks(
        self, 
        evidence_nodes: List[Tuple[str, float]],
        query: Optional[str] = None,
        is_drift: bool = False,
    ) -> Tuple[List[Dict[str, Any]], Dict[str, float], Dict[str, Any]]:
        """Retrieve raw text chunks for the evidence nodes.
        
        Performance optimization: Uses batched Neo4j query to fetch all entities
        in a single round-trip instead of sequential queries (4-10x faster).
        
        De-noising (February 9, 2026):
        - Content-hash dedup:  identical chunk text → keep first occurrence only.
        - Cross-entity dedup:  when entities A and B reference the same chunk,
          keep the one associated with the highest-scored entity.
        - Returns entity_scores dict so callers can rank/budget by PPR score.
        
        Returns:
            Tuple of (deduplicated_chunks, entity_scores_dict, retrieval_stats)
            retrieval_stats contains: chunks_raw, duplicates_removed, dedup_ratio,
            noise_filter_counts (per-filter breakdown), entities_selected.
        """
        if not self.text_store:
            logger.warning("no_text_store_available")
            return [], {}, {}
        
        chunks: List[Dict[str, Any]] = []
        def _clean_entity_name(name: str) -> str:
            cleaned = (name or "").strip()
            while len(cleaned) >= 2 and cleaned[0] == cleaned[-1] and cleaned[0] in ('"', "'", "`"):
                cleaned = cleaned[1:-1].strip()
            return cleaned

        # Build entity_scores dict — propagate PPR scores instead of discarding them.
        entity_scores: Dict[str, float] = {}
        seen_entities: set[str] = set()
        entity_names: List[str] = []
        for name, score in evidence_nodes:
            cleaned = _clean_entity_name(name)
            if not cleaned:
                continue
            if cleaned not in seen_entities:
                seen_entities.add(cleaned)
                entity_names.append(cleaned)
            # Keep highest score if entity appears multiple times
            if cleaned not in entity_scores or score > entity_scores[cleaned]:
                entity_scores[cleaned] = score
        
        # Apply relevance budget (limit entities processed)
        budget_limit = int(len(entity_names) * self.relevance_budget) + 1
        selected_entities = entity_names[:budget_limit]

        # Common Route 4 case: no entity seeds. Don't emit a misleading "0 chunks" log here;
        # the caller may fall back to query-based retrieval.
        if len(selected_entities) == 0:
            return [], entity_scores, {"chunks_raw": 0, "entities_selected": 0}

        # --- Document-scoped retrieval (February 10, 2026) ---
        # Resolve which document(s) the query targets using IDF-weighted
        # entity→document voting.  Super-connector entities (appearing in
        # many docs) get proportionally less voting power.
        # Toggle: DOC_SCOPE_ENABLED=0 to disable.
        doc_scope_enabled = os.environ.get("DOC_SCOPE_ENABLED", "1") == "1"
        target_document_ids: Optional[List[str]] = None
        doc_scope_stats: Dict[str, Any] = {"enabled": doc_scope_enabled}

        if doc_scope_enabled and self.text_store and hasattr(self.text_store, 'get_entity_document_coverage'):
            try:
                doc_coverage = await self.text_store.get_entity_document_coverage(selected_entities)
                if doc_coverage:
                    target_document_ids, doc_scope_stats = self._resolve_target_documents(
                        selected_entities, doc_coverage
                    )
            except Exception as exc:
                logger.warning("doc_scope_resolution_failed", error=str(exc))
                doc_scope_stats["error"] = str(exc)

        # --- Community-aware entity scoring (February 10, 2026 — Improvement #1.5) ---
        # PPR expands to entities across multiple Louvain communities.  Entities outside
        # the seed community (the "target" cluster) are often noise.  This step looks up
        # each entity's community_id and applies a score penalty to off-community
        # entities, which cascades into lower chunk budgets via score-weighted allocation.
        # Toggle: set DENOISE_COMMUNITY_FILTER=0 to disable.
        # Disable community filter for DRIFT multi-hop queries (Route 4).
        # DRIFT intentionally spans multiple Louvain communities to answer
        # cross-document comparative questions. Penalising cross-community
        # entities by 0.3× starves the synthesis of evidence it needs.
        community_filter_enabled = os.environ.get("DENOISE_COMMUNITY_FILTER", "1") == "1"
        if is_drift:
            community_filter_enabled = False
        community_stats: Dict[str, Any] = {"enabled": community_filter_enabled, "drift_override": is_drift}

        if community_filter_enabled and self.text_store and hasattr(self.text_store, 'get_entity_communities'):
            try:
                entity_communities = await self.text_store.get_entity_communities(selected_entities)

                # Identify target community from the top-scoring entities.
                # Top-3 entities by PPR score are most likely seeds/near-seeds.
                top_entities = sorted(selected_entities, key=lambda e: entity_scores.get(e, 0), reverse=True)[:3]
                top_cids = [entity_communities.get(e) for e in top_entities if entity_communities.get(e) is not None]

                if top_cids:
                    # Majority vote — the most common community among top entities is the target.
                    from collections import Counter
                    target_community = Counter(top_cids).most_common(1)[0][0]

                    COMMUNITY_PENALTY = float(os.environ.get("COMMUNITY_PENALTY", "0.3"))
                    penalised_count = 0
                    for ent in selected_entities:
                        ent_cid = entity_communities.get(ent)
                        if ent_cid is not None and ent_cid != target_community:
                            entity_scores[ent] = entity_scores.get(ent, 0.0) * COMMUNITY_PENALTY
                            penalised_count += 1

                    community_stats.update({
                        "target_community": target_community,
                        "entities_in_target": len(selected_entities) - penalised_count,
                        "entities_penalised": penalised_count,
                        "penalty_factor": COMMUNITY_PENALTY,
                        "top_entities_used": top_entities[:3],
                    })
                    logger.info(
                        "community_filter_applied",
                        target=target_community,
                        penalised=penalised_count,
                        total=len(selected_entities),
                    )
                else:
                    community_stats["skipped_reason"] = "no_community_ids_on_top_entities"
            except Exception as e:
                logger.warning("community_filter_failed error=%s", str(e))
                community_stats["error"] = str(e)

        # --- PPR Score-Gap Pruning (February 10, 2026 — Improvement #2) ---
        # Runs AFTER community penalty so it detects the amplified gaps between
        # in-community (full score) and out-of-community (0.3× score) entities.
        # Detects the largest relative score drop and prunes entities below it.
        # Toggle: set DENOISE_SCORE_GAP=0 to disable.
        # SCORE_GAP_THRESHOLD: minimum relative drop (fraction) to trigger pruning.
        # E.g. 0.5 means a 50%+ drop from entity[i] to entity[i+1] triggers pruning.
        # SCORE_GAP_MIN_KEEP: minimum number of entities to keep (never prune below this).
        score_gap_enabled = os.environ.get("DENOISE_SCORE_GAP", "1") == "1"
        score_gap_stats: Dict[str, Any] = {"enabled": score_gap_enabled}

        if score_gap_enabled and len(selected_entities) > 2:
            gap_threshold = float(os.environ.get("SCORE_GAP_THRESHOLD", "0.5"))
            min_keep = int(os.environ.get("SCORE_GAP_MIN_KEEP", "6"))

            # Sort entities by (potentially penalised) PPR score descending
            sorted_by_score = sorted(
                selected_entities,
                key=lambda e: entity_scores.get(e, 0.0),
                reverse=True,
            )
            scores_sorted = [entity_scores.get(e, 0.0) for e in sorted_by_score]

            # Find the largest relative drop
            best_gap_idx = None
            best_gap_ratio = 0.0
            for i in range(max(min_keep - 1, 0), len(scores_sorted) - 1):
                if scores_sorted[i] > 0:
                    relative_drop = 1.0 - (scores_sorted[i + 1] / scores_sorted[i])
                    if relative_drop > gap_threshold and relative_drop > best_gap_ratio:
                        best_gap_ratio = relative_drop
                        best_gap_idx = i

            original_count = len(selected_entities)
            if best_gap_idx is not None:
                # Keep entities up to and including the gap index
                kept_entities = set(sorted_by_score[: best_gap_idx + 1])
                selected_entities = [e for e in selected_entities if e in kept_entities]
                score_gap_stats.update({
                    "gap_found": True,
                    "gap_index": best_gap_idx,
                    "gap_ratio": round(best_gap_ratio, 3),
                    "score_at_gap": round(scores_sorted[best_gap_idx], 6),
                    "score_below_gap": round(scores_sorted[best_gap_idx + 1], 6),
                    "entities_before": original_count,
                    "entities_after": len(selected_entities),
                    "entities_pruned": original_count - len(selected_entities),
                })
                logger.info(
                    "score_gap_pruning",
                    gap_idx=best_gap_idx,
                    gap_ratio=round(best_gap_ratio, 3),
                    before=original_count,
                    after=len(selected_entities),
                )
            else:
                score_gap_stats.update({
                    "gap_found": False,
                    "entities_before": original_count,
                    "entities_after": original_count,
                    "reason": f"no_drop_exceeds_{gap_threshold}",
                })

        # --- Score-weighted chunk allocation (February 10, 2026) ---
        # Instead of giving every entity uniform limit_per_entity=12, allocate
        # proportional to PPR score.  Top entity gets full limit; noise entities get fewer.
        # Toggle: set DENOISE_SCORE_WEIGHTED=0 to disable.
        score_weighted_enabled = os.environ.get("DENOISE_SCORE_WEIGHTED", "1") == "1"
        DEFAULT_LIMIT = 12  # matches text_store default limit_per_entity

        entity_budgets: Dict[str, int] = {}
        if score_weighted_enabled and selected_entities:
            scores_list = [entity_scores.get(e, 0.0) for e in selected_entities]
            max_score = max(scores_list) if scores_list else 0.0
            if max_score > 0:
                for ent in selected_entities:
                    ratio = entity_scores.get(ent, 0.0) / max_score
                    entity_budgets[ent] = max(1, round(ratio * DEFAULT_LIMIT))
            else:
                # All scores zero/equal — fall back to uniform
                entity_budgets = {e: DEFAULT_LIMIT for e in selected_entities}
            logger.info(
                "score_weighted_budgets",
                budgets={e: entity_budgets[e] for e in selected_entities[:10]},
                max_score=max_score,
            )
        else:
            entity_budgets = {e: DEFAULT_LIMIT for e in selected_entities}

        try:
            # Batch query: fetch all entities in one round-trip (major performance gain)
            raw_chunks: List[Tuple[str, Dict[str, Any]]] = []  # (entity_name, chunk)
            if hasattr(self.text_store, 'get_chunks_for_entities'):
                entity_chunks_map = await self.text_store.get_chunks_for_entities(
                    selected_entities,
                    target_document_ids=target_document_ids,
                )
                for entity_name in selected_entities:
                    budget = entity_budgets.get(entity_name, DEFAULT_LIMIT)
                    for chunk in entity_chunks_map.get(entity_name, [])[:budget]:
                        raw_chunks.append((entity_name, chunk))
            else:
                # Fallback to sequential queries (for HippoRAGTextUnitStore or old implementations)
                for entity_name in selected_entities:
                    entity_chunks = await self.text_store.get_chunks_for_entity(entity_name)
                    budget = entity_budgets.get(entity_name, DEFAULT_LIMIT)
                    for chunk in entity_chunks[:budget]:
                        raw_chunks.append((entity_name, chunk))
            
            # --- Vector chunk safety net (Improvement #5) ---
            # After entity-based retrieval, optionally add top-k vector-similar
            # chunks directly from the chunk embedding index.  This catches content
            # that the NER → PPR path misses (e.g., vocabulary mismatch, entity not
            # linked to the most relevant chunk).
            # Toggle: DENOISE_VECTOR_FALLBACK=1 to enable.
            vector_fallback_enabled = os.environ.get("DENOISE_VECTOR_FALLBACK", "") == "1"
            vector_fallback_top_k = int(os.environ.get("VECTOR_FALLBACK_TOP_K", "3"))
            vector_fallback_stats: Dict[str, Any] = {"enabled": vector_fallback_enabled}
            
            if vector_fallback_enabled and query and self.text_store and hasattr(self.text_store, 'search_chunks_by_vector'):
                try:
                    # Lazy import to avoid circular dependency
                    from src.worker.hybrid_v2.orchestrator import get_query_embedding, get_vector_index_name
                    
                    query_embedding = get_query_embedding(query)
                    index_name = get_vector_index_name()
                    
                    vector_results = await self.text_store.search_chunks_by_vector(
                        query_embedding,
                        top_k=vector_fallback_top_k,
                        index_name=index_name,
                    )
                    
                    # Collect IDs already in raw_chunks to count true additions
                    existing_ids = {c.get("id") for _, c in raw_chunks if c.get("id")}
                    vector_new = 0
                    vector_duplicate = 0
                    
                    # Assign a low entity score so vector chunks rank below PPR-derived
                    # ones but above zero.  Use min PPR score × 0.5 as a ceiling.
                    min_ppr = min(entity_scores.values()) if entity_scores else 0.01
                    fallback_score_cap = max(min_ppr * 0.5, 0.001)
                    entity_scores["__vector_fallback__"] = fallback_score_cap
                    
                    for chunk_dict, sim_score in vector_results:
                        chunk_id = chunk_dict.get("id", "")
                        if chunk_id in existing_ids:
                            vector_duplicate += 1
                            continue
                        # Scale similarity into entity-score space
                        chunk_dict["_vector_similarity"] = sim_score
                        raw_chunks.append(("__vector_fallback__", chunk_dict))
                        existing_ids.add(chunk_id)
                        vector_new += 1
                    
                    vector_fallback_stats.update({
                        "index": index_name,
                        "top_k": vector_fallback_top_k,
                        "candidates": len(vector_results),
                        "new_chunks_added": vector_new,
                        "already_in_entity_pool": vector_duplicate,
                        "fallback_score_cap": round(fallback_score_cap, 6),
                    })
                    if vector_new > 0:
                        logger.info("vector_fallback_added_chunks",
                                   new=vector_new, duplicate=vector_duplicate,
                                   top_k=vector_fallback_top_k)
                except Exception as exc:
                    logger.warning("vector_fallback_failed", error=str(exc))
                    vector_fallback_stats["error"] = str(exc)
            
            # --- Content-hash dedup + cross-entity dedup ---
            # Ablation toggle: set DENOISE_DISABLE_DEDUP=1 to skip dedup (measure its contribution)
            dedup_enabled = os.environ.get("DENOISE_DISABLE_DEDUP", "") != "1"
            # When the same chunk is retrieved via multiple entities, keep only the
            # copy associated with the highest-scored entity (best PPR rank).
            seen_hashes: Dict[str, float] = {}  # content_hash → best entity score
            hash_to_chunk: Dict[str, Dict[str, Any]] = {}
            all_chunks_no_dedup: List[Dict[str, Any]] = []  # used when dedup disabled
            duplicates_removed = 0
            
            for entity_name, chunk in raw_chunks:
                text = chunk.get("text", "")
                content_hash = hashlib.md5(text.encode("utf-8")).hexdigest()
                ent_score = entity_scores.get(entity_name, 0.0)
                
                # Stamp every chunk with its entity score for downstream ranking
                chunk["_entity_score"] = ent_score
                chunk["_source_entity"] = entity_name
                
                if not dedup_enabled:
                    # Ablation: keep ALL chunks including duplicates
                    all_chunks_no_dedup.append(chunk)
                elif content_hash not in seen_hashes:
                    seen_hashes[content_hash] = ent_score
                    hash_to_chunk[content_hash] = chunk
                else:
                    duplicates_removed += 1
                    # If this entity has a higher score, upgrade the chunk's score
                    if ent_score > seen_hashes[content_hash]:
                        seen_hashes[content_hash] = ent_score
                        hash_to_chunk[content_hash]["_entity_score"] = ent_score
                        hash_to_chunk[content_hash]["_source_entity"] = entity_name
            
            # Sort deduped chunks by entity score (highest first) for downstream ranking
            deduped_chunks = all_chunks_no_dedup if not dedup_enabled else list(hash_to_chunk.values())
            
            # --- Semantic near-dedup (Improvement #4) ---
            # After exact MD5 dedup, catch near-duplicate chunks that differ only in
            # whitespace, punctuation, or minor OCR artefacts.  Uses word-level Jaccard
            # similarity (no external API calls → zero added latency).
            # Toggle: DENOISE_SEMANTIC_DEDUP=0 to disable.
            semantic_dedup_enabled = os.environ.get("DENOISE_SEMANTIC_DEDUP", "1") == "1"
            semantic_dedup_threshold = float(os.environ.get("SEMANTIC_DEDUP_THRESHOLD", "0.92"))
            semantic_dedup_stats: Dict[str, Any] = {"enabled": semantic_dedup_enabled}
            
            if semantic_dedup_enabled and len(deduped_chunks) > 1:
                import re as _re
                
                def _normalize_words(text: str) -> set:
                    """Extract word-level token set for Jaccard comparison."""
                    # Lowercase, strip non-alphanumeric, split on whitespace
                    words = _re.findall(r'[a-z0-9]+', text.lower())
                    return set(words)
                
                # Pre-compute word sets for all chunks
                chunk_word_sets = [_normalize_words(c.get("text", "")) for c in deduped_chunks]
                
                # Greedily cluster: iterate in score order (highest first), mark
                # near-duplicates of already-kept chunks for removal.
                # Sort by score first so we always keep the highest-scored version.
                scored_indices = sorted(
                    range(len(deduped_chunks)),
                    key=lambda i: deduped_chunks[i].get("_entity_score", 0.0),
                    reverse=True,
                )
                
                kept_indices: List[int] = []
                removed_by_semantic = 0
                
                for idx in scored_indices:
                    words_idx = chunk_word_sets[idx]
                    if not words_idx:
                        kept_indices.append(idx)  # empty chunks pass through
                        continue
                    
                    is_near_dup = False
                    for kept_idx in kept_indices:
                        words_kept = chunk_word_sets[kept_idx]
                        if not words_kept:
                            continue
                        # Jaccard similarity
                        intersection = len(words_idx & words_kept)
                        union = len(words_idx | words_kept)
                        if union > 0 and intersection / union >= semantic_dedup_threshold:
                            is_near_dup = True
                            removed_by_semantic += 1
                            break
                    
                    if not is_near_dup:
                        kept_indices.append(idx)
                
                deduped_chunks = [deduped_chunks[i] for i in sorted(kept_indices)]
                semantic_dedup_stats.update({
                    "chunks_before": len(chunk_word_sets),
                    "chunks_after": len(deduped_chunks),
                    "near_duplicates_removed": removed_by_semantic,
                    "threshold": semantic_dedup_threshold,
                })
                if removed_by_semantic > 0:
                    logger.info("semantic_near_dedup_applied",
                               removed=removed_by_semantic,
                               before=len(chunk_word_sets),
                               after=len(deduped_chunks),
                               threshold=semantic_dedup_threshold)
            
            # --- Noise filtering (February 9, 2026) ---
            # Ablation toggle: set DENOISE_DISABLE_NOISE=1 to skip noise filters
            noise_enabled = os.environ.get("DENOISE_DISABLE_NOISE", "") != "1"
            # Penalise form-label, bare-heading, and low-content chunks by reducing
            # their _entity_score.  This pushes noisy chunks below the token budget
            # cutoff without hard-deleting them.
            if noise_enabled:
                noise_stats = apply_noise_filters(deduped_chunks, score_key="_entity_score")
            else:
                noise_stats = {"total_penalised": 0, "form_label": 0, "bare_heading": 0, "min_content": 0, "disabled": True}
            
            # --- Document-coherence denoising (February 10, 2026 — Pass 7) ---
            # Safety-net: even after upstream document scoping, penalise any
            # remaining off-target-document chunks so they sink below the token
            # budget cutoff.  Toggle: DENOISE_DOC_COHERENCE=0 to disable.
            doc_coherence_enabled = os.environ.get("DENOISE_DOC_COHERENCE", "1") == "1"
            doc_coherence_penalty = float(os.environ.get("DOC_COHERENCE_PENALTY", "0.2"))
            doc_coherence_stats: Dict[str, Any] = {"enabled": doc_coherence_enabled}

            if doc_coherence_enabled and target_document_ids and deduped_chunks:
                penalised = 0
                for chunk in deduped_chunks:
                    meta = chunk.get("metadata", {})
                    cdoc = meta.get("document_id") or ""
                    if cdoc and cdoc not in target_document_ids:
                        chunk["_entity_score"] = chunk.get("_entity_score", 0.0) * doc_coherence_penalty
                        penalised += 1
                doc_coherence_stats.update({
                    "target_docs": target_document_ids,
                    "chunks_penalised": penalised,
                    "penalty": doc_coherence_penalty,
                })
                if penalised > 0:
                    logger.info(
                        "doc_coherence_denoising",
                        penalised=penalised,
                        total=len(deduped_chunks),
                        target_docs=target_document_ids,
                    )

            # Now sort by (potentially penalised) entity score
            chunks = sorted(
                deduped_chunks,
                key=lambda c: c.get("_entity_score", 0.0),
                reverse=True,
            )
            
            # Build retrieval stats for per-measure attribution
            retrieval_stats = {
                "chunks_raw": len(raw_chunks),
                "chunks_after_dedup": len(chunks),
                "duplicates_removed": duplicates_removed,
                "dedup_ratio": round(duplicates_removed / len(raw_chunks) * 100, 1) if raw_chunks else 0.0,
                "noise_filters": noise_stats,
                "entities_selected": len(selected_entities),
                "dedup_enabled": dedup_enabled,
                "noise_enabled": noise_enabled,
                "score_weighted_enabled": score_weighted_enabled,
                "entity_budgets": {e: entity_budgets.get(e, DEFAULT_LIMIT) for e in selected_entities[:10]} if score_weighted_enabled else None,
                "community_filter": community_stats,
                "score_gap": score_gap_stats,
                "semantic_dedup": semantic_dedup_stats,
                "vector_fallback": vector_fallback_stats,
                "doc_scope": doc_scope_stats,
                "doc_coherence": doc_coherence_stats,
            }
            
            logger.info("text_chunks_retrieved", 
                       num_chunks_raw=len(raw_chunks),
                       num_chunks_deduped=len(chunks),
                       duplicates_removed=duplicates_removed,
                       noise_stats=noise_stats,
                       dedup_ratio=f"{duplicates_removed / len(raw_chunks) * 100:.1f}%" if raw_chunks else "0%",
                       num_entities=len(selected_entities),
                       batched=hasattr(self.text_store, 'get_chunks_for_entities'))

            if len(chunks) == 0 and len(selected_entities) > 0:
                logger.warning(
                    "text_chunks_retrieved_zero",
                    num_entities=len(selected_entities),
                    sample_entities=selected_entities[:5],
                    hint="Likely entity/chunk label or MENTIONS direction mismatch, or chunks not linked to entities for this group_id",
                )
            return chunks, entity_scores, retrieval_stats
            
        except Exception as e:
            logger.error("text_chunk_retrieval_failed", error=str(e))
            return [], entity_scores, {"error": str(e)}
    
    @staticmethod
    def _estimate_tokens(text: str) -> int:
        """Fast token estimate: ~4 chars per token (GPT-family heuristic)."""
        return len(text) // 4 + 1

    def _get_token_budget(self) -> int:
        """Return the configured token budget (env override or default)."""
        env_val = os.environ.get("SYNTHESIS_TOKEN_BUDGET", "")
        if env_val.isdigit():
            return int(env_val)
        return self.DEFAULT_TOKEN_BUDGET

    def _build_cited_context(
        self, 
        text_chunks: List[Dict[str, Any]],
        language_spans_by_doc: Optional[Dict[str, List[Dict[str, Any]]]] = None,
        entity_scores: Optional[Dict[str, float]] = None,
    ) -> Tuple[str, Dict[str, Dict[str, str]], Dict[str, Dict[str, Any]], Dict[str, Any]]:
        """
        Build a context string with citation markers, grouped by document.
        
        De-noising (February 9, 2026):
        - Chunks arrive **pre-sorted by PPR entity score** (from _retrieve_text_chunks).
        - A configurable **token budget** (default 32K) caps context size.
          Chunks beyond the budget are dropped (lowest-scored first since list is
          score-sorted).
        
        Grouping by document enables the LLM to reason about:
        - Which document a fact comes from
        - Comparisons between documents
        - Document-level properties (dates, totals)
        
        Returns:
            Tuple of (context_string, citation_map, sentence_citation_map, context_stats)
            context_stats contains: chunks_before_budget, chunks_after_budget,
            chunks_dropped, context_tokens, context_chars, token_budget, num_doc_groups
        """
        citation_map: Dict[str, Dict[str, str]] = {}
        sentence_citation_map: Dict[str, Dict[str, Any]] = {}
        _spans_by_doc = language_spans_by_doc or {}
        _sentence_segmentation_enabled = bool(_spans_by_doc)

        def _get_sentences_for_dict_chunk(
            chunk: Dict[str, Any], meta: Dict[str, Any]
        ) -> List[Dict[str, Any]]:
            """Filter language spans that fall within this chunk's offset range."""
            if not _sentence_segmentation_enabled:
                return []
            doc_id = meta.get("document_id", "")
            if doc_id not in _spans_by_doc:
                return []
            start_offset = meta.get("start_offset")
            end_offset = meta.get("end_offset")
            if start_offset is None or end_offset is None:
                return []

            chunk_text = chunk.get("text", "")
            sentences = []
            for lang_group in _spans_by_doc[doc_id]:
                confidence = lang_group.get("confidence", 1.0)
                locale = lang_group.get("locale", "en")
                for span in lang_group.get("spans", []):
                    s_offset = span.get("offset", 0)
                    s_length = span.get("length", 0)
                    if s_offset >= start_offset and s_offset + s_length <= end_offset:
                        relative_offset = s_offset - start_offset
                        if relative_offset >= 0 and relative_offset + s_length <= len(chunk_text):
                            sent_text = chunk_text[relative_offset:relative_offset + s_length].strip()
                        else:
                            sent_text = ""
                        if sent_text and len(sent_text) >= 5:
                            sentences.append({
                                "text": sent_text,
                                "offset": s_offset,
                                "length": s_length,
                                "confidence": confidence,
                                "locale": locale,
                            })
            return sentences

        def _normalize_doc_key(doc_key: str) -> str:
            """
            Normalize document keys to merge sub-parts with parent documents.
            
            Removes common sub-part prefixes/patterns:
            - "Document Name - Exhibit A" -> "Document Name"
            - "Document Name - Appendix B" -> "Document Name"
            - "Document Name - Schedule 1" -> "Document Name"
            - "Agreement (Section 3: Arbitration)" -> "Agreement"
            """
            if not doc_key or not isinstance(doc_key, str):
                return doc_key
            
            import re
            
            # Pattern 1: Remove " - Exhibit/Appendix/Schedule/Attachment/Annex ..."
            patterns = [
                r'\s*[-–—]\s*(Exhibit|Appendix|Schedule|Attachment|Annex|Section)\s+[A-Z0-9].*$',
                r'\s*[-–—]\s*(Exhibit|Appendix|Schedule|Attachment|Annex)$',
                # Pattern 2: Remove parenthetical sections like "(Section 3: ...)"
                r'\s*\(Section\s+\d+:?\s*[^)]*\)$',
                r'\s*\([^)]*Arbitration[^)]*\)$',
            ]
            
            for pattern in patterns:
                doc_key = re.sub(pattern, '', doc_key, flags=re.IGNORECASE)
            
            return doc_key.strip()
        
        # Group chunks by document for clearer context boundaries
        from collections import defaultdict
        doc_groups: Dict[str, List[Tuple[int, Dict[str, Any]]]] = defaultdict(list)
        
        for i, chunk in enumerate(text_chunks):
            # Extract document identity from metadata or top-level keys.
            # Graph-retrieved chunks store info in chunk["metadata"][...],
            # while sentence-search chunks use top-level keys.
            meta = chunk.get("metadata", {})
            # Primary: use document_id from graph (authoritative, no normalization needed)
            doc_key = meta.get("document_id") or chunk.get("document_id")
            if not doc_key:
                # Fallback: normalize document_title or source to merge sub-parts
                raw_doc_key = (
                    meta.get("document_title")
                    or chunk.get("document_title")
                    or chunk.get("source", "Unknown")
                )
                doc_key = _normalize_doc_key(raw_doc_key)
            doc_groups[doc_key].append((i, chunk))
        
        # --- Token budget enforcement (February 9, 2026) ---
        # Ablation toggle: set DENOISE_DISABLE_BUDGET=1 to skip token budget
        budget_enabled = os.environ.get("DENOISE_DISABLE_BUDGET", "") != "1"
        # Chunks arrive pre-sorted by PPR entity score (highest first from
        # _retrieve_text_chunks).  We accumulate tokens and drop chunks that
        # would exceed the budget.  Dropped chunks are the lowest-scored.
        token_budget = self._get_token_budget()
        tokens_used = 0
        budgeted_chunks: List[Tuple[int, Dict[str, Any]]] = []
        chunks_dropped = 0
        
        # Flatten doc_groups back into score-sorted order for budget enforcement
        all_indexed_chunks = sorted(
            [(i, chunk) for group in doc_groups.values() for i, chunk in group],
            key=lambda ic: ic[1].get("_entity_score", 0.0),
            reverse=True,
        )
        
        for idx, chunk in all_indexed_chunks:
            chunk_tokens = self._estimate_tokens(chunk.get("text", ""))
            if budget_enabled and tokens_used + chunk_tokens > token_budget and budgeted_chunks:
                chunks_dropped += 1
                continue
            tokens_used += chunk_tokens
            budgeted_chunks.append((idx, chunk))
        
        # Re-group by document after budget enforcement
        doc_groups_budgeted: Dict[str, List[Tuple[int, Dict[str, Any]]]] = defaultdict(list)
        for i, chunk in budgeted_chunks:
            meta = chunk.get("metadata", {})
            doc_key = meta.get("document_id") or chunk.get("document_id")
            if not doc_key:
                raw_doc_key = (
                    meta.get("document_title")
                    or chunk.get("document_title")
                    or chunk.get("source", "Unknown")
                )
                doc_key = _normalize_doc_key(raw_doc_key)
            doc_groups_budgeted[doc_key].append((i, chunk))

        # --- Document-group gap pruning (February 14, 2026) ---
        # Score each document group by sum of chunk _entity_score values.
        # Drop document groups whose total score is below a gap threshold
        # relative to the top-scoring document.  This removes irrelevant
        # documents that leaked through PPR (e.g., "Owner" appearing in
        # multiple docs pulls in unrelated contracts).
        # Toggle: DOC_GROUP_PRUNING_ENABLED=0 to disable.
        doc_group_pruning_enabled = os.environ.get("DOC_GROUP_PRUNING_ENABLED", "1") == "1"
        doc_group_pruning_stats: Dict[str, Any] = {"enabled": doc_group_pruning_enabled}

        if doc_group_pruning_enabled and len(doc_groups_budgeted) > 1:
            # Score each document group by *unique entity PPR coverage*.
            # Only entity-retrieved chunks carry signal — skeleton/coverage chunks
            # (tagged __coverage_gap_fill__) are excluded from scoring because
            # they all share a uniform low _entity_score that masks real
            # document relevance differences.
            #
            # For each doc group we collect the set of unique _source_entity
            # names and sum their PPR scores from entity_scores.  This way a
            # document that attracted chunks from 3 high-PPR entities scores
            # higher than one that shares a single common entity.
            #
            # Fallback: if NO entity-retrieved chunks exist in any doc group
            # (i.e., all chunks are __coverage_gap_fill__ from skeleton
            # enrichment), fall back to summing _entity_score directly.
            _entity_scores_dict = entity_scores or {}
            doc_group_scores: Dict[str, float] = {}
            doc_group_entity_detail: Dict[str, Dict[str, float]] = {}
            has_entity_chunks = False
            for dkey, dchunks in doc_groups_budgeted.items():
                # Collect unique entities (not coverage gap-fill) per doc group
                entity_set: Dict[str, float] = {}
                for _, c in dchunks:
                    src_ent = c.get("_source_entity", "")
                    if src_ent and src_ent != "__coverage_gap_fill__":
                        has_entity_chunks = True
                        # Use the PPR score from entity_scores dict (canonical);
                        # fall back to chunk-level _entity_score if not in dict.
                        ppr = _entity_scores_dict.get(src_ent, c.get("_entity_score", 0.0))
                        # Keep highest score per entity (same entity might
                        # map to multiple chunks with different scores).
                        if src_ent not in entity_set or ppr > entity_set[src_ent]:
                            entity_set[src_ent] = ppr
                doc_group_scores[dkey] = sum(entity_set.values())
                doc_group_entity_detail[dkey] = entity_set

            # Fallback: if all chunks are coverage/skeleton (no entity-retrieved),
            # score by raw _entity_score sum instead.
            scoring_method = "entity_ppr"
            if not has_entity_chunks:
                scoring_method = "entity_score_sum_fallback"
                for dkey, dchunks in doc_groups_budgeted.items():
                    doc_group_scores[dkey] = sum(
                        c.get("_entity_score", 0.0) for _, c in dchunks
                    )

            ranked_docs = sorted(doc_group_scores.items(), key=lambda kv: kv[1], reverse=True)
            top_doc_key, top_doc_score = ranked_docs[0]

            # Only prune when the top document has meaningful signal
            doc_group_min_ratio = float(os.environ.get("DOC_GROUP_MIN_RATIO", "0.25"))
            if top_doc_score > 0:
                docs_to_keep = {top_doc_key}
                docs_pruned = []
                for dkey, dscore in ranked_docs[1:]:
                    ratio = dscore / top_doc_score if top_doc_score > 0 else 0.0
                    if ratio >= doc_group_min_ratio:
                        docs_to_keep.add(dkey)
                    else:
                        docs_pruned.append((dkey, round(dscore, 4), round(ratio, 3)))

                if docs_pruned:
                    # Remove pruned document groups
                    chunks_before_prune = sum(len(v) for v in doc_groups_budgeted.values())
                    for dkey, _, _ in docs_pruned:
                        del doc_groups_budgeted[dkey]
                    chunks_after_prune = sum(len(v) for v in doc_groups_budgeted.values())

                    doc_group_pruning_stats.update({
                        "top_doc": top_doc_key,
                        "top_score": round(top_doc_score, 4),
                        "min_ratio": doc_group_min_ratio,
                        "docs_kept": len(docs_to_keep),
                        "docs_pruned": len(docs_pruned),
                        "pruned_details": [(d, s, r) for d, s, r in docs_pruned],
                        "entity_detail": {
                            dk: {e: round(s, 4) for e, s in ev.items()}
                            for dk, ev in doc_group_entity_detail.items()
                        },
                        "chunks_before": chunks_before_prune,
                        "chunks_after": chunks_after_prune,
                    })
                    logger.info(
                        "doc_group_pruning",
                        kept=len(docs_to_keep),
                        pruned=len(docs_pruned),
                        top_doc=top_doc_key,
                        top_score=round(top_doc_score, 4),
                    )
                else:
                    doc_group_pruning_stats.update({
                        "decision": "all_above_threshold",
                        "ranked": [(dk, round(ds, 4)) for dk, ds in ranked_docs],
                        "entity_detail": {
                            dk: {e: round(s, 4) for e, s in ev.items()}
                            for dk, ev in doc_group_entity_detail.items()
                        },
                        "min_ratio": doc_group_min_ratio,
                    })
            else:
                doc_group_pruning_stats["decision"] = "no_signal"

        # Log document grouping for debugging
        logger.info(
            "build_cited_context_grouping",
            num_chunks_input=len(text_chunks),
            num_chunks_budgeted=len(budgeted_chunks),
            chunks_dropped_by_budget=chunks_dropped,
            tokens_used=tokens_used,
            token_budget=token_budget,
            num_doc_groups=len(doc_groups_budgeted),
            doc_keys=list(doc_groups_budgeted.keys())[:10],
            doc_group_pruning=doc_group_pruning_stats,
        )
        
        context_parts = []
        
        # Build context with document headers
        for doc_key, chunks_with_idx in doc_groups_budgeted.items():
            # Extract document metadata from first chunk
            first_chunk = chunks_with_idx[0][1]
            meta = first_chunk.get("metadata", {})
            doc_title = meta.get("document_title") or doc_key
            doc_date = meta.get("document_date", "")
            
            # Add document header
            header = f"=== DOCUMENT: {doc_title}"
            if doc_date:
                header += f" (Date: {doc_date})"
            header += " ==="
            context_parts.append(header)
            
            # Add chunks under this document
            for original_idx, chunk in chunks_with_idx:
                citation_id = f"[{original_idx + 1}]"
                source = chunk.get("source", "Unknown")
                text = chunk.get("text", "")
                
                # Extract section information from metadata
                meta = chunk.get("metadata", {})
                section_path = meta.get("section_path") or meta.get("di_section_path")
                section_str = "General"
                if isinstance(section_path, list) and section_path:
                    section_str = " > ".join(str(x) for x in section_path if x)
                elif isinstance(section_path, str) and section_path:
                    section_str = section_path
                
                # Extract document_id from metadata (set by Neo4jTextUnitStore from IN_DOCUMENT edge)
                document_id = meta.get("document_id", "")
                
                # Extract document URL from multiple sources
                document_url = (
                    chunk.get("document_source", "")  # From IN_DOCUMENT→Document.source
                    or chunk.get("document_url", "")  # Direct field
                    or meta.get("url", "")  # From chunk metadata
                    or ""
                )
                
                # Extract location metadata for precise citations
                page_number = meta.get("page_number")
                start_offset = meta.get("start_offset")
                end_offset = meta.get("end_offset")
                
                citation_map[citation_id] = {
                    "source": source,
                    "chunk_id": chunk.get("id", f"chunk_{original_idx}"),
                    "document": doc_title,
                    "document_id": document_id,  # Graph node ID for citation attribution
                    "document_title": doc_title,  # Explicit title for Route 2/3 citation format
                    "document_url": document_url,  # Blob storage URL for clickable links
                    "section": section_str,
                    "text_preview": text[:100] + "..." if len(text) > 100 else text,
                    # Location metadata for precise citations
                    **({"page_number": page_number} if page_number is not None else {}),
                    **({"start_offset": start_offset} if start_offset is not None else {}),
                    **({"end_offset": end_offset} if end_offset is not None else {}),
                }
                
                # Try sentence-level segmentation from Azure DI language_spans
                chunk_sentences = _get_sentences_for_dict_chunk(chunk, meta)
                if chunk_sentences:
                    # Cap sentences per chunk to prevent citation explosion.
                    _max_sent = int(os.getenv("ROUTE2_MAX_SENTENCES_PER_CHUNK", "8"))
                    if _max_sent > 0 and len(chunk_sentences) > _max_sent:
                        chunk_sentences = chunk_sentences[:_max_sent]
                    
                    chunk_num = original_idx + 1
                    entry_lines = [f"{citation_id} [Section: {section_str}]"]
                    for s_idx, sent in enumerate(chunk_sentences):
                        suffix = chr(ord('a') + s_idx) if s_idx < 26 else str(s_idx)
                        sent_citation_id = f"[{chunk_num}{suffix}]"
                        entry_lines.append(f"{sent_citation_id} {sent['text']}")
                        sentence_citation_map[sent_citation_id] = {
                            "source": source,
                            "chunk_id": chunk.get("id", f"chunk_{original_idx}"),
                            "document": doc_title,
                            "document_id": document_id,
                            "document_title": doc_title,
                            "document_url": document_url,
                            "section": section_str,
                            "text_preview": sent["text"][:150],
                            "sentence_text": sent["text"],
                            "sentence_offset": sent["offset"],
                            "sentence_length": sent["length"],
                            "sentence_confidence": sent["confidence"],
                            "sentence_locale": sent["locale"],
                            **({"page_number": page_number} if page_number is not None else {}),
                        }
                    context_parts.append("\n".join(entry_lines))
                else:
                    context_parts.append(f"{citation_id} {text}")
            
            context_parts.append("")  # Blank line between documents
        
        context_string = "\n\n".join(context_parts)
        context_stats = {
            "chunks_before_budget": len(text_chunks),
            "chunks_after_budget": len(budgeted_chunks),
            "chunks_dropped": chunks_dropped,
            "context_tokens": tokens_used,
            "context_chars": len(context_string),
            "token_budget": token_budget,
            "budget_enabled": budget_enabled,
            "num_doc_groups": len(doc_groups_budgeted),
            "doc_group_pruning": doc_group_pruning_stats,
        }
        return context_string, citation_map, sentence_citation_map, context_stats
    
    async def _generate_response(
        self,
        query: str,
        context: str,
        response_type: str,
        sub_questions: Optional[List[str]] = None,
        prompt_variant: Optional[str] = None,
        synthesis_model: Optional[str] = None,
    ) -> str:
        """Generate the final response with citation requirements."""
        
        if self.llm is None:
            logger.error("llm_not_configured_cannot_generate_response")
            return "Error: LLM client not configured"
        
        # Model override: create a temporary LLM client if a different model is requested
        llm = self.llm
        if synthesis_model:
            try:
                from src.worker.services.llm_service import LLMService
                llm_service = LLMService()
                llm = llm_service._create_llm_client(synthesis_model)
                logger.info(
                    "synthesis_model_override",
                    requested_model=synthesis_model,
                    default_model="HYBRID_SYNTHESIS_MODEL",
                )
            except Exception as e:
                logger.warning(
                    "synthesis_model_override_failed_using_default",
                    requested_model=synthesis_model,
                    error=str(e),
                )
                llm = self.llm
        
        # Different prompts for different response types
        if sub_questions:
            # DRIFT mode: Use multi-question synthesis prompt
            prompt = self._get_drift_synthesis_prompt(query, context, sub_questions, prompt_variant=prompt_variant)
        else:
            # v1_concise always uses the summary/extraction prompt builder
            # regardless of response_type (Route 2 fact extraction).
            effective_variant = (prompt_variant or "").lower().strip()
            if effective_variant == "v1_concise":
                prompt = self._get_summary_prompt(query, context, prompt_variant=prompt_variant)
            else:
                prompts = {
                    "detailed_report": self._get_detailed_report_prompt(query, context),
                    "summary": self._get_summary_prompt(query, context, prompt_variant=prompt_variant),
                    "audit_trail": self._get_audit_trail_prompt(query, context)
                }
                prompt = prompts.get(response_type, prompts["detailed_report"])
        
        try:
            response = await llm.acomplete(prompt)
            return response.text.strip()
        except Exception as e:
            logger.error("response_generation_failed", error=str(e))
            return f"Error generating response: {str(e)}"
    
    def _get_drift_synthesis_prompt(
        self, 
        query: str, 
        context: str,
        sub_questions: List[str],
        prompt_variant: Optional[str] = None,
    ) -> str:
        """Prompt for DRIFT-style multi-question synthesis.
        
        Supports multiple prompt variants for A/B testing:
        - None / "v0": Current production prompt (verbose, structured)
        - "v1_concise": Query-focused, concise output
        - "v2_adaptive": Format adapts to query type (compare/list/explain)
        - "v3_budget": Explicit word budget constraint
        """
        sub_q_list = "\n".join(f"  {i+1}. {q}" for i, q in enumerate(sub_questions))
        
        variant = (prompt_variant or "v0").lower().strip()
        
        logger.info(
            "drift_synthesis_prompt_variant",
            variant=variant,
            query=query[:60],
            num_sub_questions=len(sub_questions),
            context_length=len(context),
        )
        
        # ---- V0: Current production prompt (no change) ----
        if variant == "v0":
            return f"""You are analyzing a complex query that was decomposed into multiple sub-questions.

Original Query: {query}

Sub-questions explored:
{sub_q_list}

Evidence Context (with citation markers):
{context}

Instructions:
1. Synthesize findings from ALL sub-questions into a coherent analysis
2. Show how the answers connect to address the original query
3. EVERY factual claim must include a citation [n] to the evidence
4. Structure your response to follow the logical flow of the sub-questions
5. Include a final synthesis section that ties everything together

Format:
## Analysis

[Your comprehensive analysis addressing each sub-question]

## Key Connections

[How the findings relate to each other]

## Conclusion

[Final answer to the original query]

Your response:"""
        
        # ---- V1: Query-focused, concise ----
        # Answers the original query directly instead of walking through sub-questions.
        # Omits irrelevant sub-question findings. No mandatory sections.
        if variant == "v1_concise":
            return f"""Answer the following query using the evidence gathered from multiple sources.

Query: {query}

Evidence (with citation markers [n]):
{context}

Rules:
- Answer the query DIRECTLY — do not walk through sub-questions one by one
- Include ONLY findings relevant to what was asked — omit tangential information
- Cite every factual claim with [n]
- Keep the response focused: 2-4 paragraphs unless the query explicitly asks for a list or table
- No section headers unless the query asks for structured output (e.g., "list", "compare side by side")

Your response:"""
        
        # ---- V2: Task-adaptive ----
        # Detects query intent (compare, list, explain) and adjusts format accordingly.
        # Still concise but allows structured output when the query demands it.
        if variant == "v2_adaptive":
            return f"""You are a document analysis expert. Answer the query below using ONLY the evidence provided.

Query: {query}

Evidence (citations marked as [n]):
{context}

Instructions:
- Determine what the query is asking for and respond in the most natural format:
  • If COMPARING items: use a concise side-by-side analysis highlighting differences
  • If LISTING items: use a compact numbered/bulleted list
  • If EXPLAINING a concept: give a focused explanation (2-3 paragraphs max)
  • If IDENTIFYING a specific fact: state it directly in 1-2 sentences
- Cite every claim with [n] — no unsupported statements
- Include ONLY information that directly answers the query
- Do NOT repeat evidence text verbatim — synthesize and reason about it

Your response:"""
        
        # ---- V3: Word-budget constrained ----
        # Explicit word limit to force brevity. Still requires citations.
        if variant == "v3_budget":
            return f"""Query: {query}

Evidence (citations [n]):
{context}

Answer the query in 150-300 words. Cite sources with [n]. Focus only on what was asked. If comparing, highlight key differences concisely. Skip background context the user didn't request.

Your response:"""
        
        # Unknown variant — fall back to v0
        logger.warning("unknown_prompt_variant_falling_back_to_v0", variant=variant)
        return self._get_drift_synthesis_prompt(query, context, sub_questions, prompt_variant="v0")
    
    def _get_detailed_report_prompt(self, query: str, context: str) -> str:
        return f"""You are an expert analyst generating a detailed report based on document evidence.

REQUIREMENTS:
1. Answer the question using ONLY information from the Evidence Context below.
2. Cite sources with [N] markers for EVERY factual claim.
3. REFUSE only for specific lookups where the exact data point is absent:
   - Question asks for a "bank routing number" but evidence has no routing number → REFUSE
   - Question asks for "VAT/Tax ID" but evidence has no VAT number → REFUSE
   - Question asks for "governed by California law" but evidence shows a different state → REFUSE
   When refusing, respond ONLY with: "The requested information was not found in the available documents."
4. For general questions (warranty terms, agreement details, obligations, etc.),
   synthesize all relevant information from the evidence even if the text is
   fragmentary or OCR-imperfect. Do NOT refuse when partial evidence is available.
5. Do NOT fabricate information that is not in the evidence.

Question: {query}

Evidence Context:
{context}

Respond using this format:

## Answer

[Direct answer to the question with citations [N] for every factual claim]

## Details

- [Specific detail with citation [N]]
- [Connection between entities with citation [N]]
- [Important highlights from source documents with citation [N]]

Response:"""

    def _get_summary_prompt(self, query: str, context: str, prompt_variant: Optional[str] = None) -> str:
        """Summary synthesis prompt with variant support for A/B testing.
        
        Variants:
        - None / "v0": Current production prompt (structured sections, verbose)
        - "v1_concise": Direct-answer style, no forced sections, precision-optimised
        """
        variant = (prompt_variant or "v0").lower().strip()
        
        # Detect document-counting and per-document queries
        # These patterns trigger document consolidation guidance
        q_lower = query.lower()
        
        # Simple string patterns (exact match)
        simple_patterns = [
            "each document", "every document", "all documents",
            "different documents", "how many documents", "most documents",
        ]
        # Regex patterns (for flexible matching)
        regex_patterns = [
            r"summarize.*document", r"list.*document",
            r"appears?\s+in.*documents",  # "appears in X documents"
            r"which.*documents?",  # "which document(s)"
        ]
        
        is_per_document_query = (
            any(pattern in q_lower for pattern in simple_patterns) or
            any(re.search(pattern, q_lower) for pattern in regex_patterns)
        )
        
        document_guidance = ""
        if is_per_document_query:
            document_guidance = """
IMPORTANT for Per-Document Queries:
- The Evidence Context contains chunks grouped by "=== DOCUMENT: <title> ===" headers.
- Count UNIQUE top-level documents only - do NOT create separate summaries for:
  * Document sections (e.g., "Section 2: Arbitration" belongs to parent document)
  * Exhibits, Appendices, Schedules (e.g., "Exhibit A" belongs to parent contract)
  * Repeated excerpts from the same document
- If you see "Builder's Warranty" and "Builder's Warranty - Section 3", combine into ONE summary.
- If you see "Purchase Contract" and "Exhibit A - Scope of Work", combine into ONE summary.
"""
        
        logger.info(
            "summary_prompt_variant",
            variant=variant,
            query=query[:60],
            is_per_document_query=is_per_document_query,
            context_length=len(context),
        )
        
        # ---- V1: Concise, precision-optimised ----
        # Matches the skeleton benchmark prompt style that produced 42-130 char
        # responses with gpt-4.1-mini (Feb 11 benchmark).  Key elements:
        # - "Lead with a direct answer" — prevents padding/preamble
        # - Evidence-then-Question order (matches benchmark layout)
        # - Minimal rules to avoid LLM over-compliance verbosity
        # - Explicit refusal template for negative queries
        if variant == "v1_concise":
            return f"""You are a precise document extraction assistant. Extract the answer from the evidence below.

RULES:
1. State the answer value directly — do NOT repeat or paraphrase the question.
2. Quote exact values (numbers, dates, names) verbatim.
3. One sentence is ideal. Two sentences maximum unless the question asks for a list.
4. No citations, no bracket references like [1] or [2a], no section headers, no background context, no hedging, no preamble.
5. If the answer is NOT in the evidence, respond ONLY with: "Not found in the provided documents."
{document_guidance}
EVIDENCE:
{context}

QUESTION: {query}

ANSWER:"""
        
        # ---- V0: Current production prompt ----
        return f"""You are an expert analyst generating a concise summary.

Question: {query}

Evidence Context:
{context}

Instructions:
1. Answer the question using ONLY information from the Evidence Context.
2. REFUSE only for specific lookups where the exact data point is absent:
   - Question asks for "bank routing number" but evidence has no routing number → Refuse
   - Question asks for "SWIFT code" but evidence has no SWIFT/IBAN → Refuse
   - Question asks for "California law" but evidence shows a different state → Refuse
   When refusing, respond ONLY with: "The requested information was not found in the available documents."
3. For general questions (warranty terms, agreement details, fees, obligations, etc.),
   synthesize all relevant information from the evidence even if the text is
   fragmentary or OCR-imperfect. Do NOT refuse when partial evidence is available.
4. **RESPECT ALL QUALIFIERS** in the question. If the question asks for a specific type, category, or unit:
   - Include ONLY items matching that qualifier
   - EXCLUDE items that don't match, even if they seem related
5. Include citations [N] for factual claims (aim for every sentence that states a fact).
6. If the evidence contains explicit numeric values (e.g., dollar amounts, time periods/deadlines, percentages, counts), include them verbatim.
7. Prefer concrete obligations/thresholds over general paraphrases.
8. If the question is asking for obligations, reporting/record-keeping, remedies, default/breach, or dispute-resolution: enumerate each distinct obligation/mechanism that is explicitly present in the Evidence Context; do not omit items just because another item is more prominent.
{document_guidance}

Respond using this format:

## Summary

[Summary with citations [N] for every factual claim. Include explicit numeric values verbatim. Cover provisions from ALL source documents, not just the most prominent one.]

## Key Points

- [Distinct item/obligation 1 with citation [N]]
- [Distinct item/obligation 2 with citation [N]]
- [Additional items from each source document as needed]

Response:"""

    def _get_audit_trail_prompt(self, query: str, context: str) -> str:
        return f"""You are generating an audit trail for compliance purposes.

CRITICAL: Every statement MUST be cited. This is for legal/compliance review.

Question: {query}

Evidence Context:
{context}

Respond using this format:

## Findings

- **Finding 1:** [Statement with exact source citation [N]]
- **Finding 2:** [Statement with exact source citation [N]]
- [Additional findings as needed]

## Evidence Chain

[Logical chain showing how evidence connects, with citations [N]]

## Gaps and Confidence

- **Gaps:** [Any missing information or uncertainties]
- **Confidence:** [High/Medium/Low with justification]

Audit Trail:"""

    async def _nlp_connected_extract(
        self,
        query: str,
        text_chunks: List[Dict[str, Any]],
        evidence_nodes: List[Tuple[str, float]]
    ) -> Dict[str, Any]:
        """
        Deterministic extraction + sentence connection (rephrasing with temperature=0).
        
        Uses ExtractionService from V3 for sentence extraction and optional rephrasing.
        Deterministic: same input produces consistent output (minor LLM variance possible).
        """
        extraction = ExtractionService(llm=self.llm)
        
        # Prepare communities (treat each text chunk as a "community")
        communities = [
            {
                "id": chunk.get("id", f"chunk_{i}"),
                "title": chunk.get("source", "Unknown"),
                "summary": chunk.get("text", ""),
            }
            for i, chunk in enumerate(text_chunks[:20])  # Limit to avoid excessive tokens
        ]
        
        # Extract and rephrase
        result = extraction.audit_summary(
            communities=communities,
            query=query,
            top_k=5,
            include_rephrased=True,  # Enable sentence connection
        )
        
        # Build citations from extracted sentences
        citations = []
        for sent in result.get("extracted_sentences", []):
            citations.append({
                "text": sent["text"],
                "source": sent.get("source_community_title", "Unknown"),
                "rank_score": sent.get("rank_score", 0.0),
            })
        
        return {
            "response": result.get("rephrased_narrative", result.get("audit_summary", "")),
            "citations": citations,
            "evidence_path": [node for node, _ in evidence_nodes],
            "text_chunks_used": len(text_chunks),
            "processing_deterministic": True,
            "extraction_mode": "nlp_connected",
        }

    async def _nlp_audit_extract(
        self,
        query: str,
        text_chunks: List[Dict[str, Any]],
        evidence_nodes: List[Tuple[str, float]]
    ) -> Dict[str, Any]:
        """
        Deterministic NLP extraction (no LLM) for 100% repeatability.
        
        Returns all retrieved chunk content with citations, preserving
        the full context that the LLM synthesis would have access to.
        This enables fair comparison between LLM and deterministic modes.
        """
        import re
        
        if not text_chunks:
            return {
                "response": "Not specified in the provided documents.",
                "citations": [],
                "evidence_path": [node[0] for node in evidence_nodes],
                "text_chunks_used": 0,
                "processing_deterministic": True,
            }
        
        # Build response with all chunks, each with citation marker
        response_parts = []
        citations = []
        
        for i, chunk in enumerate(text_chunks, 1):
            text = chunk.get("text", "")
            if not text or not isinstance(text, str):
                continue
            
            # Get metadata
            source = chunk.get("source", chunk.get("document_source", "unknown"))
            section = chunk.get("section", "")
            if not section:
                section_path = chunk.get("section_path", [])
                if section_path:
                    section = " > ".join(str(s) for s in section_path if s)
            
            # Extract document title from source URL or use section
            doc_title = "Unknown"
            if source and "/" in source:
                doc_title = source.split("/")[-1].replace(".pdf", "").replace("_", " ")
            
            citation_id = f"[{i}]"
            
            # Add to response with citation marker
            response_parts.append(f"{citation_id} {text.strip()}")
            
            # Build citation entry with full metadata
            citations.append({
                "citation": citation_id,
                "source": source,
                "document": doc_title,
                "section": section,
                "chunk_id": chunk.get("id", chunk.get("chunk_id", f"chunk_{i}")),
                "text_preview": text[:200] + "..." if len(text) > 200 else text,
            })
        
        # Join all chunks with double newline for readability
        audit_response = "\n\n".join(response_parts)
        
        logger.info(
            "nlp_audit_extraction_complete",
            query=query[:50],
            chunks_returned=len(citations),
            processing_deterministic=True,
        )
        
        return {
            "response": audit_response,
            "citations": citations,
            "evidence_path": [node[0] for node in evidence_nodes],
            "text_chunks_used": len(text_chunks),
            "processing_deterministic": True,
        }

    async def _comprehensive_two_pass_extract(
        self,
        query: str,
        text_chunks: List[Dict[str, Any]],
        evidence_nodes: List[Tuple[str, float]]
    ) -> Dict[str, Any]:
        """
        Graph-aware comprehensive extraction for 100% fact coverage.
        
        IMPROVED APPROACH: Instead of regex extraction, we leverage the GRAPH STRUCTURE:
        - Azure DI already extracted KVPs at indexing time → stored as KeyValuePair nodes
        - Tables are stored as Table nodes with markdown/headers
        - Edges connect Chunks → Sections → KVPs/Tables
        
        By traversing these edges, we get DETERMINISTIC structured facts without LLM re-extraction.
        
        PASS 1: Graph Structure Retrieval (No LLM)
        - Query KeyValuePair nodes for each document
        - Query Table nodes for each document
        - Use pre-extracted structured facts (deterministic, layout-aware)
        - Supplement with regex extraction for values not in KVPs
        
        PASS 2: LLM Comparison
        - Input: Structured facts from graph + regex
        - LLM compares and identifies inconsistencies
        - Has full context: original text + structured KVPs + tables
        
        Returns:
            dict with:
            - response: Rich comparison narrative
            - raw_extractions: Structured JSON facts per document (from graph)
            - citations: Full citation metadata
        """
        import json
        import re
        from collections import defaultdict
        
        # =====================================================================
        # STEP 0: Try to get GRAPH-AWARE chunks with KVPs and Tables
        # =====================================================================
        graph_docs: List[Dict[str, Any]] = []
        
        # Debug: log text_store state
        logger.info(
            "comprehensive_text_store_state",
            text_store_type=type(self.text_store).__name__ if self.text_store else None,
            text_store_group_id=getattr(self.text_store, '_group_id', None) if self.text_store else None,
            has_method=hasattr(self.text_store, "get_chunks_with_graph_structure") if self.text_store else False,
        )
        
        if self.text_store and hasattr(self.text_store, "get_chunks_with_graph_structure"):
            try:
                graph_docs = await self.text_store.get_chunks_with_graph_structure(limit=50)
                logger.info("comprehensive_graph_structure_loaded",
                           num_docs=len(graph_docs),
                           total_kvps=sum(len(d.get("kvps", [])) for d in graph_docs),
                           total_tables=sum(len(d.get("tables", [])) for d in graph_docs),
                           total_entities=sum(len(d.get("entities", [])) for d in graph_docs))
            except Exception as e:
                logger.warning("comprehensive_graph_structure_failed", error=str(e))
        
        # NO FALLBACK - require graph structure or fail explicitly
        if not graph_docs:
            logger.error(
                "comprehensive_no_graph_docs",
                text_store_type=type(self.text_store).__name__ if self.text_store else None,
                text_store_group_id=getattr(self.text_store, '_group_id', None) if self.text_store else None,
                has_get_chunks_method=hasattr(self.text_store, "get_chunks_with_graph_structure") if self.text_store else False,
                evidence_nodes_count=len(evidence_nodes),
            )
            return {
                "response": "No documents found to analyze.",
                "raw_extractions": [],
                "citations": [],
                "evidence_path": [node[0] for node in evidence_nodes],
                "text_chunks_used": 0,
            }
        
        # =====================================================================
        # PASS 1: Build Structured Extractions from GRAPH + Regex
        # =====================================================================
        raw_extractions = []
        citations = []
        citation_idx = 1
        
        for doc in graph_docs:
            doc_title = doc.get("document_title", "Unknown")
            doc_id = doc.get("document_id", "")
            doc_text = doc.get("combined_text", "")
            kvps = doc.get("kvps", [])
            tables = doc.get("tables", [])
            entities = doc.get("entities", [])
            
            # Start with KVPs from graph (deterministic, layout-aware)
            extraction = {
                "document_title": doc_title,
                "_document_id": doc_id,
                "_document_title": doc_title,
                "_citation_idx": citation_idx,
                "_source": "graph",  # Track that this came from graph
                
                # KVPs grouped by type
                "kvp_amounts": [],
                "kvp_parties": [],
                "kvp_dates": [],
                "kvp_identifiers": [],
                "kvp_other": [],
                
                # Tables
                "tables": tables,
                
                # Entities from graph
                "entities": entities,
                
                # Will also add regex extractions for values not in KVPs
                "regex_amounts": [],
                "regex_parties": [],
                "regex_dates": [],
                "all_fields": [],  # Combined for comparison
            }
            
            # Categorize KVPs by type
            for kvp in kvps:
                key = (kvp.get("key") or "").lower()
                value = kvp.get("value") or ""
                
                if not value.strip():
                    continue
                
                field_entry = {
                    "field": f"kvp_{key.replace(' ', '_')[:30]}",
                    "value": value,
                    "key": kvp.get("key"),
                    "confidence": kvp.get("confidence", 0.0),
                    "source": "azure_di"
                }
                
                # Categorize by content
                if any(word in key for word in ["amount", "total", "price", "cost", "payment", "fee", "$"]):
                    extraction["kvp_amounts"].append(field_entry)
                elif any(word in key for word in ["date", "effective", "expir", "due"]):
                    extraction["kvp_dates"].append(field_entry)
                elif any(word in key for word in ["name", "party", "buyer", "seller", "customer", "vendor", "company", "representative"]):
                    extraction["kvp_parties"].append(field_entry)
                elif any(word in key for word in ["number", "id", "invoice", "contract", "po", "ref"]):
                    extraction["kvp_identifiers"].append(field_entry)
                else:
                    extraction["kvp_other"].append(field_entry)
                
                extraction["all_fields"].append(field_entry)
            
            # Supplement with regex extraction for values KVPs might miss
            regex_extraction = self._regex_extract_fields(doc_text, doc_title)
            
            # Add regex-found amounts that aren't already in KVPs
            kvp_values = {f.get("value", "").strip() for f in extraction["all_fields"]}
            for amt in regex_extraction.get("amounts", []):
                if amt.get("value", "").strip() not in kvp_values:
                    field_entry = {**amt, "source": "regex"}
                    extraction["regex_amounts"].append(field_entry)
                    extraction["all_fields"].append(field_entry)
            
            for party in regex_extraction.get("parties", []):
                if party.get("value", "").strip() not in kvp_values:
                    field_entry = {**party, "source": "regex"}
                    extraction["regex_parties"].append(field_entry)
                    extraction["all_fields"].append(field_entry)
            
            for date in regex_extraction.get("dates", []):
                if date.get("value", "").strip() not in kvp_values:
                    field_entry = {**date, "source": "regex"}
                    extraction["regex_dates"].append(field_entry)
                    extraction["all_fields"].append(field_entry)
            
            # Also add regex-found identifiers (invoice#, PO#, etc)
            for identifier in regex_extraction.get("identifiers", []):
                if identifier.get("value", "").strip() not in kvp_values:
                    field_entry = {**identifier, "source": "regex"}
                    # Add to kvp_identifiers (not a separate regex_identifiers list)
                    extraction["kvp_identifiers"].append(field_entry)
                    extraction["all_fields"].append(field_entry)
            
            raw_extractions.append(extraction)
            
            citations.append({
                "citation": f"[{citation_idx}]",
                "chunk_id": doc.get("chunks", [{}])[0].get("chunk_id", "") if doc.get("chunks") else "",
                "document_id": doc_id,
                "document_title": doc_title,
                "document_url": "",
                "page_number": None,
                "section": "",
                "text_preview": doc_text[:200] + "..." if len(doc_text) > 200 else doc_text,
                "kvp_count": len(kvps),
                "table_count": len(tables),
            })
            citation_idx += 1
        
        logger.info("pass1_graph_extraction_complete",
                   num_docs=len(raw_extractions),
                   total_kvp_fields=sum(len(e.get("kvp_amounts", [])) + len(e.get("kvp_parties", [])) + 
                                        len(e.get("kvp_dates", [])) + len(e.get("kvp_identifiers", [])) +
                                        len(e.get("kvp_other", [])) for e in raw_extractions),
                   total_regex_fields=sum(len(e.get("regex_amounts", [])) + len(e.get("regex_parties", [])) +
                                          len(e.get("regex_dates", [])) for e in raw_extractions))
        
        # =====================================================================
        # PASS 2: LLM COMPARISON (with reduced context - only HippoRAG output)
        # =====================================================================
        comparison_context = self._build_graph_aware_comparison_context(raw_extractions, graph_docs)
        
        comparison_prompt = f"""You are comparing documents to identify inconsistencies.

QUERY: "{query}"

{comparison_context}

TASK: Identify ALL inconsistencies between these documents.

For each inconsistency, provide:
1. FIELD: What field/value is inconsistent
2. DOCUMENTS: Which documents disagree and what each says
3. SIGNIFICANCE: Why this inconsistency matters

Include ALL discrepancies:
- Amounts/prices (different totals, payment terms)
- Party names (different company names, entities)
- Product/model descriptions (different specs)
- Dates (different effective dates, due dates)
- Terms and conditions (different warranty, payment terms)
- Any other factual disagreements

Use citation markers [1], [2], etc. to reference each document.

BEGIN ANALYSIS:"""

        # Guard against None LLM
        if not self.llm:
            logger.error("llm_not_available", error="LLM is None")
            narrative = "## Comparison Failed: LLM not available\n\n"
            for ext in raw_extractions:
                narrative += f"### Document [{ext.get('_citation_idx', '?')}]: {ext.get('_document_title', 'Unknown')}\n"
                if '_error' in ext:
                    narrative += f"- Error: {ext['_error']}\n\n"
                else:
                    narrative += f"- Fields extracted: {len([k for k in ext.keys() if not k.startswith('_')])}\n\n"
        else:
            try:
                comparison_result = await self.llm.acomplete(comparison_prompt)
                narrative = comparison_result.text.strip()
            except Exception as e:
                logger.error("llm_comparison_failed", error=str(e))
                # Fallback: List the extracted facts
                narrative = "## Comparison Failed\n\n"
                for ext in raw_extractions:
                    narrative += f"### Document [{ext.get('_citation_idx', '?')}]: {ext.get('_document_title', 'Unknown')}\n"
                    if '_error' in ext:
                        narrative += f"- Error: {ext['_error']}\n\n"
                    else:
                        narrative += f"- Fields extracted: {len([k for k in ext.keys() if not k.startswith('_')])}\n\n"
        
        logger.info("comprehensive_graph_aware_complete",
                   query=query[:50],
                   num_docs=len(graph_docs),
                   num_extractions=len(raw_extractions))
        
        return {
            "response": narrative,
            "raw_extractions": raw_extractions,
            "citations": citations,
            "evidence_path": [node[0] for node in evidence_nodes],
            "text_chunks_used": sum(len(d.get("chunks", [])) for d in graph_docs),
            "processing_mode": "comprehensive_graph_aware_reduced_context",
            "kvp_source": "azure_di",
        }

    async def _comprehensive_sentence_level_extract(
        self,
        query: str,
        text_chunks: List[Dict[str, Any]],
        evidence_nodes: List[Tuple[str, float]],
    ) -> Dict[str, Any]:
        """
        Sentence-level comprehensive extraction using Azure DI language spans.
        
        Provides RAW evidence to LLM without pre-extracted interpretations:
        1. Sentences (from Azure DI language_spans) - precise text boundaries
        2. Tables (headers + rows) - raw structured data
        3. Original HippoRAG chunks - the evidence PPR traversal found
        
        NO KVPs - they are pre-extracted interpretations that interfere with LLM judgment.
        The LLM should work from raw evidence only.
        
        Args:
            query: User's comparison query
            text_chunks: HippoRAG evidence chunks (original PPR output)
            evidence_nodes: List of (entity_id, ppr_score) from HippoRAG
            
        Returns:
            Dict with response, raw_extractions, citations, etc.
        """
        import json
        from collections import defaultdict
        
        # =====================================================================
        # STEP 1: Get sentence-level context from ALL documents directly
        # NOTE: We do NOT use entity traversal here because Route 4's entity
        # resolution often returns generic terms (e.g., "Contract") that don't
        # exist as Entity nodes. Instead, query all documents with sentences.
        # =====================================================================
        sentence_docs: List[Dict[str, Any]] = []
        
        # Debug logging for text_store
        logger.info(
            "comprehensive_sentence_text_store_debug",
            has_text_store=self.text_store is not None,
            text_store_type=type(self.text_store).__name__ if self.text_store else None,
            has_get_all_docs_method=hasattr(self.text_store, "get_all_documents_with_sentences") if self.text_store else False,
            text_store_group_id=getattr(self.text_store, '_group_id', None) if self.text_store else None,
        )
        
        if self.text_store and hasattr(self.text_store, "get_all_documents_with_sentences"):
            try:
                sentence_docs = await self.text_store.get_all_documents_with_sentences(
                    top_k_docs=10,
                    max_sentences_per_doc=50,
                )
                logger.info(
                    "comprehensive_sentence_level_all_docs",
                    num_docs=len(sentence_docs),
                    total_sentences=sum(len(d.get("sentences", [])) for d in sentence_docs),
                )
            except Exception as e:
                logger.warning("comprehensive_sentence_level_failed", error=str(e))
        
        # =====================================================================
        # STEP 2: Get Tables ONLY (no KVPs - they interfere with LLM judgment)
        # =====================================================================
        tables_by_doc: Dict[str, List[Dict[str, Any]]] = defaultdict(list)
        
        if self.text_store and hasattr(self.text_store, "get_chunks_with_graph_structure"):
            try:
                graph_docs = await self.text_store.get_chunks_with_graph_structure(limit=50)
                for graph_doc in graph_docs:
                    doc_title = graph_doc.get("document_title", "Unknown")
                    tables = graph_doc.get("tables", [])
                    if tables:
                        tables_by_doc[doc_title].extend(tables)
            except Exception as e:
                logger.warning("comprehensive_tables_retrieval_failed", error=str(e))
        
        # =====================================================================
        # STEP 3: Build RAW EVIDENCE context (sentences + tables + HippoRAG chunks)
        # =====================================================================
        combined_context_parts = []
        citations = []
        citation_idx = 1
        
        # --- SECTION A: Sentence-level evidence from Azure DI ---
        if sentence_docs:
            combined_context_parts.append("=" * 60)
            combined_context_parts.append("SECTION A: SENTENCE-LEVEL EVIDENCE")
            combined_context_parts.append("(Extracted using Azure Document Intelligence language spans)")
            combined_context_parts.append("=" * 60)
            
            for doc in sentence_docs:
                doc_title = doc.get("document_title", "Unknown")
                sentences = doc.get("sentences", [])
                
                if not sentences:
                    continue
                
                combined_context_parts.append(f"\n### Document [{citation_idx}]: {doc_title}")
                combined_context_parts.append("-" * 40)
                
                # Collect sentence data with spans for citations
                sentence_spans = []
                for i, sent in enumerate(sentences, 1):
                    text = sent.get("text", "").strip()
                    if len(text) < 5:
                        continue
                    combined_context_parts.append(f"  S{i}: {text}")
                    
                    # Store span metadata for this sentence
                    sentence_data = {
                        "text": text,
                        "offset": sent.get("offset", 0),
                        "length": sent.get("length", len(text)),
                        "confidence": sent.get("confidence", 1.0),
                        "locale": sent.get("locale", "en"),
                        "sentence_index": i,
                    }
                    
                    # Include polygon geometry for pixel-accurate highlighting (if available)
                    if sent.get("page") is not None:
                        sentence_data["page"] = sent["page"]
                    if sent.get("polygons"):
                        sentence_data["polygons"] = sent["polygons"]
                    
                    sentence_spans.append(sentence_data)
                
                # Build citation with page dimensions for coordinate transformation
                citation_data = {
                    "citation": f"[{citation_idx}]",
                    "document_id": doc.get("document_id", ""),
                    "document_title": doc_title,
                    "sentence_count": len(sentences),
                    "source": "azure_di_sentences",
                    "sentences": sentence_spans,  # Add detailed span data for frontend
                }
                
                # Include page dimensions from document metadata (for normalized→pixel conversion)
                if doc.get("page_dimensions"):
                    citation_data["page_dimensions"] = doc["page_dimensions"]
                
                citations.append(citation_data)
                citation_idx += 1
        
        # --- SECTION B: Table data (raw headers + rows) ---
        if tables_by_doc:
            combined_context_parts.append("\n" + "=" * 60)
            combined_context_parts.append("SECTION B: TABLE DATA")
            combined_context_parts.append("(Raw table headers and rows from documents)")
            combined_context_parts.append("=" * 60)
            
            for doc_title, tables in tables_by_doc.items():
                # Find or create citation for this document
                existing = next((c for c in citations if c.get("document_title") == doc_title), None)
                if existing:
                    cite_ref = existing["citation"]
                else:
                    cite_ref = f"[{citation_idx}]"
                    citations.append({
                        "citation": cite_ref,
                        "document_id": "",
                        "document_title": doc_title,
                        "source": "tables",
                    })
                    citation_idx += 1
                
                combined_context_parts.append(f"\n### Tables from {cite_ref}: {doc_title}")
                combined_context_parts.append("-" * 40)
                
                for t_idx, table in enumerate(tables, 1):
                    headers = table.get("headers", [])
                    rows = table.get("rows", [])
                    
                    if headers:
                        combined_context_parts.append(f"\nTable {t_idx} Headers: | {' | '.join(str(h) for h in headers)} |")
                    
                    if rows and isinstance(rows, list):
                        for r_idx, row in enumerate(rows, 1):
                            if isinstance(row, dict):
                                row_vals = [str(row.get(h, "")) for h in headers] if headers else list(row.values())
                                combined_context_parts.append(f"  Row {r_idx}: | {' | '.join(row_vals)} |")
                            elif isinstance(row, list):
                                combined_context_parts.append(f"  Row {r_idx}: | {' | '.join(str(v) for v in row)} |")
        
        # --- SECTION C: Original HippoRAG evidence chunks ---
        if text_chunks:
            combined_context_parts.append("\n" + "=" * 60)
            combined_context_parts.append("SECTION C: HIPPORAG EVIDENCE CHUNKS")
            combined_context_parts.append("(Original text chunks from graph traversal)")
            combined_context_parts.append("=" * 60)
            
            # Group chunks by document
            chunks_by_doc: Dict[str, List[Dict[str, Any]]] = defaultdict(list)
            for chunk in text_chunks:
                doc_title = chunk.get("document_title") or chunk.get("metadata", {}).get("document_title") or chunk.get("source", "Unknown")
                chunks_by_doc[doc_title].append(chunk)
            
            for doc_title, chunks in chunks_by_doc.items():
                # Find or create citation
                existing = next((c for c in citations if c.get("document_title") == doc_title), None)
                if existing:
                    cite_ref = existing["citation"]
                else:
                    cite_ref = f"[{citation_idx}]"
                    citations.append({
                        "citation": cite_ref,
                        "document_id": "",
                        "document_title": doc_title,
                        "source": "hipporag_chunks",
                    })
                    citation_idx += 1
                
                combined_context_parts.append(f"\n### Chunks from {cite_ref}: {doc_title}")
                combined_context_parts.append("-" * 40)
                
                for c_idx, chunk in enumerate(chunks[:10], 1):  # Limit to 10 chunks per doc
                    text = chunk.get("text", "")[:1500]  # Truncate very long chunks
                    if text:
                        combined_context_parts.append(f"\nChunk {c_idx}:\n{text}")
        
        combined_context = "\n".join(combined_context_parts)
        
        # =====================================================================
        # STEP 4: NO FALLBACK - fail explicitly if no evidence
        # =====================================================================
        if not sentence_docs and not tables_by_doc and not text_chunks:
            logger.error(
                "comprehensive_sentence_level_no_data",
                sentence_docs_count=len(sentence_docs),
                tables_count=len(tables_by_doc),
                text_chunks_count=len(text_chunks),
                text_store_type=type(self.text_store).__name__ if self.text_store else None,
                text_store_group_id=getattr(self.text_store, '_group_id', None) if self.text_store else None,
            )
            return {
                "response": "ERROR: comprehensive_sentence found no data. Check text_store initialization and group_id.",
                "raw_extractions": [],
                "citations": [],
                "evidence_path": [node[0] for node in evidence_nodes],
                "text_chunks_used": 0,
                "debug": {
                    "sentence_docs": len(sentence_docs),
                    "tables_by_doc": len(tables_by_doc),
                    "text_chunks": len(text_chunks),
                    "text_store_type": type(self.text_store).__name__ if self.text_store else None,
                    "text_store_group_id": getattr(self.text_store, '_group_id', None) if self.text_store else None,
                },
            }
        
        # =====================================================================
        # STEP 5: LLM Analysis with RAW evidence only
        # =====================================================================
        comparison_prompt = f"""You are analyzing documents to find inconsistencies using RAW EVIDENCE.

QUERY: "{query}"

{combined_context}

TASK: Using the raw evidence above, identify ALL inconsistencies between documents.

The evidence is organized in three sections:
- SECTION A: Sentence-level text (most precise - from Azure DI)
- SECTION B: Table data (headers and rows)
- SECTION C: HippoRAG chunks (broader context from graph traversal)

For each inconsistency found, provide:
1. FIELD: What field/value is inconsistent (e.g., "Total Amount", "Product Model")
2. DOCUMENTS: Which documents disagree (use citations [1], [2], etc.)
3. EVIDENCE: Quote the exact text from each document showing the discrepancy
4. SIGNIFICANCE: Why this inconsistency matters

Look for ALL discrepancies in:
- Amounts, prices, totals
- Product/service descriptions
- Party names, addresses
- Dates, terms, conditions
- Quantities, specifications
- Any other factual disagreements

BE THOROUGH - list every inconsistency you find, even minor ones.

BEGIN ANALYSIS:"""

        if not self.llm:
            logger.error("llm_not_available")
            narrative = "## Analysis Failed: LLM not available\n\nRaw evidence was retrieved but cannot be analyzed."
        else:
            try:
                comparison_result = await self.llm.acomplete(comparison_prompt)
                narrative = comparison_result.text.strip()
            except Exception as e:
                logger.error("llm_sentence_analysis_failed", error=str(e))
                narrative = f"## Analysis Failed: {str(e)}\n\n### Raw Evidence Summary:\n"
                narrative += f"- Sentences extracted: {sum(len(d.get('sentences', [])) for d in sentence_docs)}\n"
                narrative += f"- Tables found: {sum(len(t) for t in tables_by_doc.values())}\n"
                narrative += f"- HippoRAG chunks: {len(text_chunks)}\n"
        
        logger.info(
            "comprehensive_sentence_level_complete",
            query=query[:50],
            num_sentence_docs=len(sentence_docs),
            total_sentences=sum(len(d.get("sentences", [])) for d in sentence_docs),
            total_tables=sum(len(t) for t in tables_by_doc.values()),
            hipporag_chunks=len(text_chunks),
        )
        
        # Build enhanced citations with reference tracking
        enhanced_citations = []
        for cite in citations:
            # Track which findings reference this citation
            cite_ref = cite.get("citation", "")
            referenced_in = []
            
            # Simple heuristic: check if citation appears in narrative
            if cite_ref in narrative:
                referenced_in.append({
                    "type": "llm_finding",
                    "context": "Referenced in comprehensive analysis",
                })
            
            enhanced_citations.append({
                **cite,  # Preserve all original data including sentences with spans
                "referenced_in_findings": referenced_in,
            })
        
        return {
            "response": narrative,  # Full LLM output with all inconsistencies
            "raw_extractions": [
                {
                    "document_title": d.get("document_title"),
                    "sentences": d.get("sentences", []),
                    "total_spans": d.get("total_spans", 0),
                }
                for d in sentence_docs
            ],
            "citations": enhanced_citations,  # Enhanced with span data for highlighting
            "evidence_path": [node[0] for node in evidence_nodes],
            "text_chunks_used": len(text_chunks) + sum(len(d.get("sentences", [])) for d in sentence_docs),
            "processing_mode": "comprehensive_sentence_level",
            "sentence_based": True,
            "metadata": {
                "sentence_extraction": "azure_di_language_spans",
                "total_documents": len(sentence_docs),
                "total_sentences": sum(len(d.get("sentences", [])) for d in sentence_docs),
                "total_tables": sum(len(tables) for tables in tables_by_doc.values()),
                "hipporag_chunks": len(text_chunks),
                "citation_strategy": "sentence_level_with_spans",
            },
        }

    def _build_graph_aware_comparison_context(
        self, 
        extractions: List[Dict[str, Any]], 
        graph_docs: List[Dict[str, Any]]
    ) -> str:
        """Build section-level context for retrieved KVPs/Tables using section_path.
        
        RETRIEVAL-BASED APPROACH:
        1. Only show sections that contain retrieved KVPs/tables
        2. Deduplicate sections across the document
        3. No arbitrary limits - show everything retrieved
        4. Group by section to show context once per section
        """
        parts = []
        
        parts.append("=" * 60)
        parts.append("ENRICHED CONTEXT (Retrieval-based)")
        parts.append("Showing only sections with retrieved structured data")
        parts.append("=" * 60)
        
        # Process each document's extractions
        for ext in extractions:
            idx = ext.get("_citation_idx", "?")
            title = ext.get("_document_title", "Unknown")
            parts.append(f"\n### Document [{idx}]: {title}")
            parts.append("-" * 40)
            
            # Get the document's KVPs and group by section
            doc = next((d for d in graph_docs if d.get("document_title") == title), None)
            if not doc:
                continue
            
            kvps = doc.get("kvps", [])
            
            # Group KVPs by section for efficiency and deduplication
            kvps_by_section: Dict[str, List[Dict]] = {}
            for kvp in kvps:
                section_path = kvp.get("section_path", [])
                if section_path:
                    section_key = section_path[0]
                    if section_key not in kvps_by_section:
                        kvps_by_section[section_key] = []
                    kvps_by_section[section_key].append(kvp)
            
            # If no KVPs grouped by section, show raw chunks
            if not kvps_by_section:
                chunks = doc.get("chunks", [])
                if chunks:
                    parts.append("\n**Document Content (no KVPs found):**")
                    # Show ALL retrieved chunks - no arbitrary limit
                    for i, chunk in enumerate(chunks, 1):
                        chunk_text = chunk.get("text", "")
                        # Only truncate if truly massive (>3000 chars)
                        if len(chunk_text) > 3000:
                            chunk_text = chunk_text[:3000] + "..."
                        parts.append(f"\nChunk {i}: {chunk_text}")
            else:
                # Build section context mapping once per document (deduplication)
                section_to_context: Dict[str, str] = {}
                for section_key in kvps_by_section.keys():
                    # Find chunk containing this section
                    for chunk in doc.get("chunks", []):
                        chunk_text = chunk.get("text", "")
                        # Simple heuristic: if section title appears in chunk, it's likely the right one
                        if section_key.lower() in chunk_text.lower()[:500]:
                            # Only truncate if truly massive (>3000 chars)
                            if len(chunk_text) > 3000:
                                chunk_text = chunk_text[:3000] + "..."
                            section_to_context[section_key] = chunk_text
                            break
                
                # Show ALL sections with retrieved KVPs - no arbitrary limit
                for section_key, section_kvps in kvps_by_section.items():
                    parts.append(f"\n**Section: {section_key}**")
                    
                    # Show section context if available
                    if section_key in section_to_context:
                        parts.append(f"Context: {section_to_context[section_key]}")
                        parts.append("")
                    
                    # Show ALL KVPs from this section - no arbitrary limit
                    parts.append("Fields extracted:")
                    for kvp in section_kvps:
                        key = kvp.get("key", "")
                        value = kvp.get("value", "")
                        parts.append(f"  • {key}: {value}")
            
            # Show ALL tables for this document - no arbitrary limit
            tables = doc.get("tables", [])
            if tables:
                parts.append(f"\n**Tables in Document:**")
                for i, table in enumerate(tables, 1):
                    headers = table.get("headers", [])
                    rows = table.get("rows", [])
                    
                    if headers:
                        parts.append(f"\nTable {i} Headers: {', '.join(headers)}")
                    
                    # Show ALL rows - no arbitrary limit
                    if isinstance(rows, list):
                        for j, row in enumerate(rows, 1):
                            if isinstance(row, dict):
                                # Show all columns in the row
                                row_str = ", ".join(f"{k}={v}" for k, v in row.items())
                                parts.append(f"  Row {j}: {row_str}")
        
        return "\n".join(parts)

    def _regex_extract_fields(self, text: str, doc_title: str) -> Dict[str, Any]:
        """
        PASS 1: Deterministic field extraction using regex patterns.
        
        This is NOT NLP - it's pure pattern matching for:
        - Amounts: $X,XXX.XX patterns
        - Dates: MM/DD/YYYY, YYYY-MM-DD, Month DD, YYYY
        - Identifiers: Invoice #, PO #, Contract #
        - Parties: Company names with Inc/LLC/Ltd suffixes
        - URLs, percentages, line items
        
        Why regex instead of NLP?
        - Domain-specific terms (Savaria V1504, WR-500 lock) aren't in NER models
        - We need exact literal values, not semantic labels
        - 100% reproducible - same input always gives same output
        - Fast, no model loading required
        """
        import re
        
        extraction = {
            "document_title": doc_title,
            "amounts": [],
            "dates": [],
            "identifiers": [],
            "parties": [],
            "percentages": [],
            "urls": [],
            "line_items": [],
            "terms": [],
            "all_fields": [],  # Flattened list for comparison
        }
        
        # Amount patterns (currency)
        amount_pattern = r'\$[\d,]+(?:\.\d{2})?|\d{1,3}(?:,\d{3})*(?:\.\d{2})?\s*(?:USD|dollars?)'
        for match in re.finditer(amount_pattern, text, re.IGNORECASE):
            value = match.group().strip()
            # Get context (20 chars before and after)
            start = max(0, match.start() - 30)
            end = min(len(text), match.end() + 30)
            context = text[start:end].replace('\n', ' ').strip()
            extraction["amounts"].append({"value": value, "context": context})
            extraction["all_fields"].append({"field": f"amount_{len(extraction['amounts'])}", "value": value, "context": context})
        
        # Date patterns
        date_pattern = r'\b(?:\d{1,2}[-/]\d{1,2}[-/]\d{2,4}|\d{4}[-/]\d{1,2}[-/]\d{1,2}|(?:Jan|Feb|Mar|Apr|May|Jun|Jul|Aug|Sep|Oct|Nov|Dec)[a-z]*\.?\s+\d{1,2},?\s+\d{4}|\d{1,2}\s+(?:Jan|Feb|Mar|Apr|May|Jun|Jul|Aug|Sep|Oct|Nov|Dec)[a-z]*\.?\s+\d{4})\b'
        for match in re.finditer(date_pattern, text, re.IGNORECASE):
            value = match.group().strip()
            start = max(0, match.start() - 30)
            end = min(len(text), match.end() + 30)
            context = text[start:end].replace('\n', ' ').strip()
            extraction["dates"].append({"value": value, "context": context})
            extraction["all_fields"].append({"field": f"date_{len(extraction['dates'])}", "value": value, "context": context})
        
        # Invoice/PO/Contract numbers - require specific format with at least one digit
        id_patterns = [
            r'(?:Invoice|INV)\s*(?:#|No\.?|Number)?\s*:?\s*([A-Z0-9][A-Z0-9\-]+\d[A-Z0-9\-]*)',
            r'(?:PO|Purchase\s+Order)\s*(?:#|No\.?|Number)?\s*:?\s*([A-Z0-9][A-Z0-9\-]+\d[A-Z0-9\-]*)',
            r'(?:Contract)\s*(?:#|No\.?|Number)?\s*:?\s*([A-Z0-9][A-Z0-9\-]+\d[A-Z0-9\-]*)',
            r'(?:Order)\s*(?:#|No\.?|Number)?\s*:?\s*([A-Z0-9][A-Z0-9\-]+\d[A-Z0-9\-]*)',
        ]
        seen_ids = set()
        for pattern in id_patterns:
            for match in re.finditer(pattern, text, re.IGNORECASE):
                value = match.group(1).strip() if match.groups() else match.group().strip()
                if value and len(value) >= 3 and value.lower() not in seen_ids:
                    seen_ids.add(value.lower())
                    context = match.group()
                    extraction["identifiers"].append({"value": value, "context": context})
                    extraction["all_fields"].append({"field": f"identifier_{len(extraction['identifiers'])}", "value": value, "context": context})
        
        # Party names (look for common patterns)
        party_patterns = [
            r'(?:Bill\s+To|Ship\s+To|Sold\s+To|Customer|Buyer|Seller|Vendor|Contractor)\s*:?\s*([A-Z][A-Za-z\s&,\.]+?)(?:\n|$|,\s*(?:Inc|LLC|Ltd|Corp))',
            r'([A-Z][A-Za-z\s]+(?:Inc|LLC|Ltd|Corp|Company|Co)\.?)',
        ]
        seen_parties = set()
        for pattern in party_patterns:
            for match in re.finditer(pattern, text):
                value = match.group(1).strip() if match.groups() else match.group().strip()
                if value and len(value) > 3 and value.lower() not in seen_parties:
                    seen_parties.add(value.lower())
                    extraction["parties"].append({"value": value})
                    extraction["all_fields"].append({"field": f"party_{len(extraction['parties'])}", "value": value})
        
        # URLs
        url_pattern = r'https?://[^\s<>"{}|\\^`\[\]]+'
        for match in re.finditer(url_pattern, text):
            value = match.group().strip()
            extraction["urls"].append({"value": value})
            extraction["all_fields"].append({"field": f"url_{len(extraction['urls'])}", "value": value})
        
        # Percentages
        pct_pattern = r'\d+(?:\.\d+)?%'
        for match in re.finditer(pct_pattern, text):
            value = match.group().strip()
            start = max(0, match.start() - 30)
            end = min(len(text), match.end() + 30)
            context = text[start:end].replace('\n', ' ').strip()
            extraction["percentages"].append({"value": value, "context": context})
            extraction["all_fields"].append({"field": f"percentage_{len(extraction['percentages'])}", "value": value, "context": context})
        
        # Product/model patterns
        model_pattern = r'(?:Model|Product|Item|Part)\s*#?\s*:?\s*([A-Z0-9\-]+)'
        for match in re.finditer(model_pattern, text, re.IGNORECASE):
            value = match.group(1).strip() if match.groups() else match.group().strip()
            extraction["all_fields"].append({"field": f"model_{len([f for f in extraction['all_fields'] if f['field'].startswith('model_')])}", "value": value})
        
        # Line items (look for quantity x description x price patterns)
        line_item_pattern = r'(\d+)\s+(.+?)\s+\$?([\d,]+(?:\.\d{2})?)'
        for match in re.finditer(line_item_pattern, text):
            qty, desc, price = match.groups()
            if len(desc) > 5 and len(desc) < 200:  # Filter noise
                extraction["line_items"].append({
                    "quantity": qty,
                    "description": desc.strip(),
                    "price": price
                })
                extraction["all_fields"].append({
                    "field": f"line_item_{len(extraction['line_items'])}",
                    "value": f"{qty}x {desc.strip()} @ ${price}"
                })
        
        # Payment terms patterns
        term_patterns = [
            r'(?:Payment|Due)\s+(?:Terms?|upon)\s*:?\s*(.+?)(?:\n|$)',
            r'(?:payable|due)\s+(?:in|within|on)\s+(.+?)(?:\n|$|\.|,)',
            r'(\d+)\s+(?:days?|weeks?)\s+(?:from|after|upon)',
        ]
        for pattern in term_patterns:
            for match in re.finditer(pattern, text, re.IGNORECASE):
                value = match.group(1).strip() if match.groups() else match.group().strip()
                if value and len(value) > 3:
                    extraction["terms"].append({"value": value})
                    extraction["all_fields"].append({"field": f"term_{len(extraction['terms'])}", "value": value})
        
        return extraction

    def _build_field_comparison_pairs(self, extractions: List[Dict[str, Any]], max_pairs: int = 100) -> List[Dict[str, Any]]:
        """Build pairs of fields to compare across documents.
        
        Compares ALL document pairs - lets LLM decide relevance in Pass 2.
        No document-type filtering to avoid domain-specific assumptions.
        """
        if len(extractions) < 2:
            return []
        
        pairs = []
        
        # Compare ALL document pairs - no smart filtering
        doc_pairs_to_compare = []
        for i in range(len(extractions)):
            for j in range(i + 1, len(extractions)):
                doc_pairs_to_compare.append((extractions[i], extractions[j]))
        
        logger.info("document_pairs_to_compare", num_pairs=len(doc_pairs_to_compare),
                   doc_titles=[e.get("_document_title", "")[:30] for e in extractions])
        
        # Key field types to compare (ignore noisy identifiers)
        important_field_types = {"amount", "party", "date", "url", "percentage", "model", "term", "line_item"}
        
        for doc1, doc2 in doc_pairs_to_compare:
            doc1_by_type = {}
            doc2_by_type = {}
            
            for f in doc1.get("all_fields", []):
                field_type = f["field"].rsplit("_", 1)[0]
                if field_type in important_field_types:
                    doc1_by_type.setdefault(field_type, []).append(f)
            
            for f in doc2.get("all_fields", []):
                field_type = f["field"].rsplit("_", 1)[0]
                if field_type in important_field_types:
                    doc2_by_type.setdefault(field_type, []).append(f)
            
            all_types = set(doc1_by_type.keys()) | set(doc2_by_type.keys())
            
            for field_type in all_types:
                doc1_vals = doc1_by_type.get(field_type, [])
                doc2_vals = doc2_by_type.get(field_type, [])
                
                # For amounts and percentages - compare all combinations (they might appear in different order)
                if field_type in ("amount", "percentage"):
                    doc1_set = {v.get("value", "").strip() for v in doc1_vals}
                    doc2_set = {v.get("value", "").strip() for v in doc2_vals}
                    
                    # Only flag if there are values unique to one doc
                    only_in_doc1 = doc1_set - doc2_set
                    only_in_doc2 = doc2_set - doc1_set
                    
                    if only_in_doc1 or only_in_doc2:
                        pairs.append({
                            "field": f"{field_type}_comparison",
                            "doc1_id": doc1.get("_document_id"),
                            "doc1_title": doc1.get("_document_title", "Doc1")[:30],
                            "doc1_value": ", ".join(sorted(only_in_doc1)[:5]) if only_in_doc1 else "N/A",
                            "doc1_citation": f"[{doc1.get('_citation_idx', '?')}]",
                            "doc2_id": doc2.get("_document_id"),
                            "doc2_title": doc2.get("_document_title", "Doc2")[:30],
                            "doc2_value": ", ".join(sorted(only_in_doc2)[:5]) if only_in_doc2 else "N/A",
                            "doc2_citation": f"[{doc2.get('_citation_idx', '?')}]",
                            "notes": f"Values unique to each document"
                        })
                
                # For parties - compare names
                elif field_type == "party":
                    doc1_names = {v.get("value", "").strip().lower() for v in doc1_vals}
                    doc2_names = {v.get("value", "").strip().lower() for v in doc2_vals}
                    
                    # Compare each pair of party names
                    for v1 in doc1_vals[:5]:  # Limit to first 5
                        for v2 in doc2_vals[:5]:
                            name1 = v1.get("value", "").strip()
                            name2 = v2.get("value", "").strip()
                            # Skip obvious matches
                            if name1.lower() == name2.lower():
                                continue
                            # Check for potential variants (e.g., "Contoso Ltd" vs "Contoso LLC")
                            if self._might_be_same_entity(name1, name2):
                                pairs.append({
                                    "field": "party_variant",
                                    "doc1_id": doc1.get("_document_id"),
                                    "doc1_title": doc1.get("_document_title", "Doc1")[:30],
                                    "doc1_value": name1[:100],
                                    "doc1_citation": f"[{doc1.get('_citation_idx', '?')}]",
                                    "doc2_id": doc2.get("_document_id"),
                                    "doc2_title": doc2.get("_document_title", "Doc2")[:30],
                                    "doc2_value": name2[:100],
                                    "doc2_citation": f"[{doc2.get('_citation_idx', '?')}]",
                                    "notes": "Potential entity name variant"
                                })
                
                # For URLs - direct comparison
                elif field_type == "url":
                    for v1 in doc1_vals:
                        for v2 in doc2_vals:
                            url1 = v1.get("value", "").strip()
                            url2 = v2.get("value", "").strip()
                            if url1 != url2:
                                pairs.append({
                                    "field": "url_difference",
                                    "doc1_id": doc1.get("_document_id"),
                                    "doc1_title": doc1.get("_document_title", "Doc1")[:30],
                                    "doc1_value": url1[:100],
                                    "doc1_citation": f"[{doc1.get('_citation_idx', '?')}]",
                                    "doc2_id": doc2.get("_document_id"),
                                    "doc2_title": doc2.get("_document_title", "Doc2")[:30],
                                    "doc2_value": url2[:100],
                                    "doc2_citation": f"[{doc2.get('_citation_idx', '?')}]",
                                })
                
                # For line items - compare descriptions
                elif field_type == "line_item":
                    for k, v1 in enumerate(doc1_vals[:10]):
                        v2 = doc2_vals[k] if k < len(doc2_vals) else {"value": "NOT FOUND"}
                        if v1.get("value") != v2.get("value"):
                            pairs.append({
                                "field": f"line_item_{k+1}",
                                "doc1_id": doc1.get("_document_id"),
                                "doc1_title": doc1.get("_document_title", "Doc1")[:30],
                                "doc1_value": str(v1.get("value", ""))[:100],
                                "doc1_citation": f"[{doc1.get('_citation_idx', '?')}]",
                                "doc2_id": doc2.get("_document_id"),
                                "doc2_title": doc2.get("_document_title", "Doc2")[:30],
                                "doc2_value": str(v2.get("value", ""))[:100],
                                "doc2_citation": f"[{doc2.get('_citation_idx', '?')}]",
                            })
            
            # Stop if we have enough pairs
            if len(pairs) >= max_pairs:
                break
        
        logger.info("field_comparison_pairs_built", total_pairs=len(pairs))
        return pairs[:max_pairs]

    def _might_be_same_entity(self, name1: str, name2: str) -> bool:
        """Check if two names might refer to the same entity (e.g., Contoso Ltd vs Contoso LLC)."""
        # Normalize: lowercase, remove common suffixes
        suffixes = ["inc", "inc.", "llc", "ltd", "ltd.", "corp", "corp.", "company", "co", "co."]
        
        def normalize(name):
            n = name.lower().strip()
            for suffix in suffixes:
                if n.endswith(" " + suffix):
                    n = n[:-len(suffix)-1].strip()
            return n
        
        n1 = normalize(name1)
        n2 = normalize(name2)
        
        # If base names match, they might be variants
        if n1 == n2:
            return True
        
        # Check if one is substring of other
        if len(n1) > 3 and len(n2) > 3:
            if n1 in n2 or n2 in n1:
                return True
        
        return False

    def _parse_comparison_results(self, comparison_text: str, field_pairs: List[Dict]) -> List[Dict]:
        """Parse LLM comparison output into structured results."""
        import re
        
        results = []
        lines = comparison_text.split('\n')
        
        for line in lines:
            # Match pattern: [1]: MISMATCH - explanation
            match = re.match(r'\[(\d+)\]\s*:?\s*(MATCH|MISMATCH|MISSING)\s*[-–]\s*(.+)', line, re.IGNORECASE)
            if match:
                idx = int(match.group(1)) - 1
                decision = match.group(2).upper()
                explanation = match.group(3).strip()
                
                if 0 <= idx < len(field_pairs):
                    results.append({
                        **field_pairs[idx],
                        "decision": decision,
                        "explanation": explanation,
                    })
        
        # Add any pairs that weren't matched
        matched_indices = {r.get("field") for r in results}
        for fp in field_pairs:
            if fp.get("field") not in matched_indices:
                results.append({
                    **fp,
                    "decision": "REVIEW",
                    "explanation": "LLM did not provide explicit comparison"
                })
        
        return results

    def _extract_citations(
        self, 
        response: str, 
        citation_map: Dict[str, Dict[str, str]],
        sentence_citation_map: Optional[Dict[str, Dict[str, Any]]] = None,
    ) -> List[Dict[str, str]]:
        """Extract and validate citations from the response.
        
        Supports both chunk-level [N] and sentence-level [Na] citations.
        Sentence citations (e.g., [1a], [2b]) are matched first, then chunk
        citations [N] are matched with a negative lookahead to avoid
        double-counting [1] inside [1a].
        """
        import re
        
        citations = []
        _sent_map = sentence_citation_map or {}
        
        # Step 1: Extract sentence-level citations [Na] (e.g., [1a], [2b])
        if _sent_map:
            sent_pattern = r'\[(\d+[a-z])\]'
            used_sentence_cites = set(re.findall(sent_pattern, response))
            for cite_key_raw in sorted(used_sentence_cites):
                cite_key = f"[{cite_key_raw}]"
                if cite_key in _sent_map:
                    citations.append({
                        "citation": cite_key,
                        "citation_type": "sentence",
                        **_sent_map[cite_key],
                    })
        
        # Step 2: Extract chunk-level citations [N] (negative lookahead avoids [1] in [1a])
        # Use negative lookahead (?![a-z]) so [1] doesn't match if followed by a letter
        chunk_pattern = r'\[(\d+)\](?![a-z])'
        used_citations = set(re.findall(chunk_pattern, response))
        
        for cite_num in sorted(used_citations, key=int):
            cite_key = f"[{cite_num}]"
            if cite_key in citation_map:
                citations.append({
                    "citation": cite_key,
                    "citation_type": "chunk",
                    **citation_map[cite_key]
                })
        
        return citations
