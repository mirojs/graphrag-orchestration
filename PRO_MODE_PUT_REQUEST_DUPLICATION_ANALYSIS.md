# Pro Mode PUT Request Internal Code Duplication Analysis üîç

## Overview
Analyzed the `create_or_replace_content_analyzer` function (~1250 lines) for internal code duplication patterns.

## Major Duplication Categories Found

### 1. üîÅ **JSON Serialization Pattern Duplication** (10+ occurrences)
**Pattern**: Safe JSON logging with exception handling
```python
try:
    safe_X_log = json.dumps(data, indent=2, ensure_ascii=True)
    print(f"[AnalyzerCreate] Content: {safe_X_log}")
except Exception as e:
    print(f"[AnalyzerCreate] Content (serialization failed): {str(data)[:500]}...")
```

**Duplicated in**:
- Schema config logging (line ~1861)
- Schema object logging (line ~1869) 
- Cosmos field logging (line ~1956)
- Azure storage logging (line ~2007)
- Field validation logging (line ~2098)
- Original schema logging (line ~2280)
- Azure schema logging (line ~2293)

### 2. üîÅ **JSONL Sources Creation Duplication** (2 major blocks)
**Pattern**: Creating sources.jsonl with entry formatting and upload

**Block 1**: Selected reference files (lines ~2520-2560)
```python
sources_entries = []
for file_name in valid_selected_files:
    sources_entries.append({
        "file": file_name,
        "resultFile": f"{file_name}.result.json"
    })
sources_jsonl_content = "\n".join(json.dumps(entry) for entry in sources_entries)
# Upload logic...
```

**Block 2**: All reference files (lines ~2590-2610)  
```python
sources_entries = []
for file_name in reference_files:
    sources_entries.append({
        "file": file_name,
        "resultFile": f"{file_name}.result.json"
    })
sources_jsonl_content = "\n".join(json.dumps(entry) for entry in sources_entries)
# Upload logic...
```

### 3. üîÅ **Error Handling and Logging Pattern Duplication** (15+ occurrences)
**Pattern**: Exception catching with detailed logging
```python
except Exception as e:
    print(f"[AnalyzerCreate][ERROR] Operation failed: {e}")
    print(f"[AnalyzerCreate][ERROR] Context details...")
    # Various error handling approaches
```

**Duplicated across**:
- Schema fetch operations
- Blob upload operations  
- Field validation loops
- API request preparation
- Database operations

### 4. üîÅ **Field Validation Loop Duplication** (Multiple sections)
**Pattern**: Iterating through fields with validation
```python
for field_name, field_def in fields.items():
    # Validation logic
    validated_field = {}
    # Type checking
    # Property copying
    validated_fields[field_name] = validated_field
```

**Found in**:
- Schema structure analysis (~line 1950)
- Field transformation for Azure API (~line 2650)  
- Field validation and cleanup (~line 2700)

### 5. üîÅ **Debug Header Printing Duplication** (20+ occurrences)
**Pattern**: Section headers with debug information
```python
print(f"[AnalyzerCreate][DEBUG] ===== SECTION NAME =====")
print(f"[AnalyzerCreate][DEBUG] Description of operation")
```

**Examples**:
- "===== SCHEMA DETECTION PHASE ====="
- "===== DATABASE SCHEMA FETCH ====="
- "===== COSMOS DB SCHEMA STRUCTURE ANALYSIS ====="
- "===== AZURE STORAGE FETCH ====="

### 6. üîÅ **Storage URL Processing Duplication** (3+ occurrences)
**Pattern**: Extracting base storage URL from configuration
```python
storage_url = app_config.app_storage_blob_url
if '/pro-reference-files' in storage_url:
    base_storage_url = storage_url.replace('/pro-reference-files', '')
else:
    base_storage_url = storage_url
```

## Refactoring Opportunities

### High Impact Refactoring
1. **Create Safe JSON Logger Utility**
   ```python
   def safe_log_json(data, label, max_length=500):
       try:
           return json.dumps(data, indent=2, ensure_ascii=True)
       except Exception:
           return str(data)[:max_length] + "..."
   ```

2. **Extract JSONL Creation Function**
   ```python
   def create_sources_jsonl(file_names, analysis_id, storage_helper):
       sources_entries = [
           {"file": name, "resultFile": f"{name}.result.json"} 
           for name in file_names
       ]
       content = "\\n".join(json.dumps(entry) for entry in sources_entries)
       # Upload logic...
       return sources_file_path
   ```

3. **Create Field Validation Utility**
   ```python
   def validate_field_for_azure_api(field_name, field_def):
       # Centralized validation logic
       # Type checking, property copying, etc.
       return validated_field
   ```

4. **Extract Storage URL Helper**
   ```python
   def get_base_storage_url(storage_blob_url):
       return storage_blob_url.replace('/pro-reference-files', '') if '/pro-reference-files' in storage_blob_url else storage_blob_url
   ```

### Medium Impact Refactoring
5. **Create Debug Logger Class**
   ```python
   class AnalyzerCreateLogger:
       @staticmethod
       def section_header(name):
           print(f"[AnalyzerCreate][DEBUG] ===== {name} =====")
       
       @staticmethod  
       def error(message, context=None):
           # Standardized error logging
   ```

6. **Extract Database Operations**
   ```python
   def fetch_schema_from_db(schema_id, app_config):
       # Centralized schema fetching with error handling
       return schema_doc
   ```

## Potential Code Reduction

| Category | Current Lines | Potential Reduction | Savings |
|----------|---------------|-------------------|---------|
| JSON Logging | ~70 lines | ~40 lines | 30 lines |
| JSONL Creation | ~80 lines | ~20 lines | 60 lines |
| Field Validation | ~120 lines | ~60 lines | 60 lines |
| Error Handling | ~100 lines | ~50 lines | 50 lines |
| Debug Headers | ~40 lines | ~10 lines | 30 lines |
| **TOTAL** | **~410 lines** | **~180 lines** | **~230 lines** |

## Implementation Priority

### Phase 1: High-Impact, Low-Risk
1. ‚úÖ Safe JSON logging utility (used 10+ times)
2. ‚úÖ Storage URL processing helper (used 3+ times)
3. ‚úÖ Debug logging standardization

### Phase 2: Moderate-Impact, Medium-Risk  
4. ‚úÖ JSONL creation function extraction
5. ‚úÖ Field validation utilities
6. ‚úÖ Error handling standardization

### Phase 3: Structural Improvements
7. ‚úÖ Break function into logical sub-functions:
   - `validate_and_fetch_schema()`
   - `configure_knowledge_sources()`
   - `prepare_azure_api_payload()`
   - `send_analyzer_request()`

## Benefits of Refactoring

### Code Quality
- **Maintainability**: Single source of truth for common operations
- **Readability**: Main function focuses on business logic flow
- **Testability**: Individual utility functions can be unit tested

### Performance
- **Reduced Memory**: Less duplicated code in memory
- **Faster Development**: Reusable utilities speed up future changes
- **Debugging**: Centralized error handling improves troubleshooting

### Risk Mitigation
- **Consistency**: Standardized error handling and logging
- **Bug Prevention**: Single implementation reduces chance of inconsistencies
- **Easier Updates**: Changes to utilities affect all usages automatically

## Summary

The `create_or_replace_content_analyzer` function contains **significant internal duplication** across 6 major categories, with potential for **~230 lines of code reduction** (~18% of the function). 

**Immediate Action Items**:
1. Extract safe JSON logging utility (highest impact, lowest risk)
2. Create JSONL generation helper function  
3. Standardize field validation logic
4. Implement debug logging utilities

This internal refactoring would make the complex function more maintainable while preserving all existing functionality.
