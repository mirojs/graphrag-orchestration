# GraphRAG Orchestration - Handover (January 8, 2026)

**Date:** January 8, 2026, 19:38 UTC  
**Status:** Reverted to baseline after regression attempt  
**Current Branch:** `main` at commit `1dcf16b`

---

## Most Successful Baseline

**Commit:** `1dcf16b` (Jan 8, 12:21 UTC)  
**Title:** feat: Add fulltext boost to Route 3 (Stage 3.3.5) for deterministic phrase matching

**Performance Metrics (Route 3 Global Search):**
- **Average Theme Coverage:** 94.2%
- **Questions at 100%:** 7/10
- **Questions below 100%:** 3/10
  - Q-G1: 88% (termination/cancellation rules)
  - Q-G7: 83% (arbitration/dispute resolution)
  - Q-G10: 71% (cross-document themes)
- **Benchmark:** `benchmarks/route3_global_search_20260108T125835Z.md`

**Key Features:**
- Stage 3.3.5: Fulltext Candidate Injection using Neo4j native fulltext index
- Section-based chunk retrieval with proximity scoring
- Graph-aware synthesis with community context
- Feature flag: `ROUTE3_FULLTEXT_BOOST=1` (enabled)

---

## Remaining Issues

### 1. Theme Coverage Gaps (6% Missing)

**Problem:** 3 questions not achieving 100% theme coverage

**Root Cause:** Retrieval not finding chunks containing specific expected terms:
- Specific time periods: "60 days", "10 business days"
- Formal language: "written notice", "certified mail"
- Specific dollar amounts and percentages
- Entity names and roles

**Evidence:**
- Hub entities show related concepts (e.g., "termination, amendment, or expiration")
- Citations present but missing specific phrases in text
- Not a synthesis issue - the expected terms are not in retrieved evidence chunks

**Example (Q-G1 - Termination/Cancellation):**
- Expected: "60 days", "written notice", "3 business days", "refund", "forfeited", "terminates"
- Retrieved: Hub entity "cancel after 3 business days" but phrase "3 business days" not in citation text
- Theme coverage: 88% (7/8 terms found)

### 2. Thematic Benchmark Lower Performance

**Metrics (route3_thematic_20260108T190712Z.json):**
- Average theme coverage: 88.5%
- 5/10 questions below 100%: T-1 (67%), T-2 (86%), T-7 (60%), X-1 (86%), X-2 (86%)

**Same root cause as above** - retrieval not finding chunks with specific expected terms.

---

## Today's Improvement Attempts (Documented for Reference)

**Note:** We attempted to improve theme coverage from 94.2% to 100% during today's session. Although the results were not better than baseline, **all efforts have been fully documented and preserved** in commit `b5a91be` on branch `regression-attempt-jan8` for future reference and learning.

**Branch:** `regression-attempt-jan8` (commit `b5a91be`)  
**Duration:** 7 hours (13:00-19:38 UTC)

**Improvements Attempted (Detailed):**

1. **Neo4j Connection Resilience** (`enhanced_graph_retriever.py`)
   - **Problem:** Neo4j connection resets causing `section_id_boost_chunks_failed` errors
   - **Solution:** Added exponential backoff retry logic with helper functions:
     - `_is_transient_neo4j_error()`: Detect connection resets, defunct sessions
     - `_run_neo4j_query_with_retry()`: Retry with 2^attempt second delays, max 3 attempts
   - **Applied to:** `get_section_id_boost_chunks()`, `_get_relationships()`, `_get_entity_descriptions()`
   - **Rationale:** Transient Neo4j errors should not fail entire queries

2. **Test Group ID Correction** (`last_test_group_id.txt`)
   - **Problem:** Benchmarks were using wrong/empty group ID, causing 0% theme coverage
   - **Solution:** Updated from empty to `test-5pdfs-1767429340223041632`
   - **Discovery:** Scanned historical benchmark JSONs to find known-good group
   - **Impact:** Fixed the "found 0 results" issue that started the investigation

3. **Benchmark Guardrails** (`benchmark_route3_global_search.py`)
   - **Problem:** Silent failures when wrong group ID is used
   - **Solution:** Added stats preflight check at benchmark start:
     - Query Neo4j for entity/chunk counts before running questions
     - Abort if counts are zero (indicates wrong/empty group)
   - **Purpose:** Prevent wasting time on invalid benchmark runs

4. **Synthesis Prompt Optimization** (`synthesis.py`) - **REVERTED**
   - **Problem:** Responses missing specific expected terms despite having evidence
   - **Attempted:** Add response_type-specific style instructions in `_generate_graph_response()`
     - Different prompting for list vs comparative vs summary questions
     - Emphasis on including specific terms, dates, amounts
   - **User Feedback:** "Document specific change" - rejected as question-specific tuning
   - **Outcome:** Reverted to original single comprehensive prompt

5. **Root Cause Analysis**
   - **Compared:** Thematic benchmark (88.5%) vs Global Search benchmark (94.2%)
   - **Checked:** Whether expected terms are in retrieved citations vs synthesized responses
   - **Finding:** Expected terms (e.g., "60 days", "written notice") are **NOT in citation text**
   - **Conclusion:** Gap is retrieval precision, not synthesis quality
   - **Evidence:** Hub entities show related concepts, but chunks missing specific phrases

6. **Benchmark Type Investigation**
   - **Discovered:** Two separate benchmark systems with different scoring:
     - `benchmark_route3_global_search.py`: Per-question theme coverage from EXPECTED_TERMS
     - `benchmark_route3_thematic.py`: Dedicated thematic evaluation with overall scores
   - **Comparison:** Identified thematic benchmark has different expected_themes dict
   - **Insight:** Same underlying retrieval issue affects both benchmark types

**Result:** **REGRESSION** - Theme coverage decreased from 94.2% to 92.9% (-1.3%)
- Q-G3: 100% → 88% (-12%)
- Q-G5: 100% → 67% (-33%)
- Overall: 4/10 questions below 100% (was 3/10)

**Conclusion:** Changes either had no positive effect or degraded retrieval quality. Reverted to baseline. However, the investigation revealed that the remaining 6% gap is a **retrieval precision issue** (missing specific terms in chunks), not a synthesis or extraction failure.

**Value of Documentation:** This commit preserves valuable diagnostic work, including analysis of which expected terms are missing from retrieved evidence, comparison between benchmark types, and verification that hub entities and citations are being generated correctly.

---

## Technical Context

### Architecture
- **System:** Hybrid GraphRAG (LazyHippo pattern)
- **Database:** Neo4j (remote, multi-tenant via group_id)
- **Tenant:** `test-5pdfs-1767429340223041632` (5 PDF documents)
- **Indexing:** Entity extraction + RAPTOR hierarchical summaries + community detection
- **Route 3 Pipeline:**
  1. Hybrid entity search (embedding + graph)
  2. Community matching (Leiden algorithm)
  3. Section boost (document structure awareness)
  4. Fulltext boost (Stage 3.3.5, exact phrase matching)
  5. Keyword proximity expansion (PPR)
  6. Graph-aware synthesis with community context

### Key Files
- Enhanced retrieval: `graphrag-orchestration/app/hybrid/pipeline/enhanced_graph_retriever.py`
- Synthesis: `graphrag-orchestration/app/hybrid/pipeline/synthesis.py`
- Orchestrator: `graphrag-orchestration/app/hybrid/orchestrator.py`
- Benchmarks: `scripts/benchmark_route3_global_search.py`, `scripts/benchmark_route3_thematic.py`

### Known Working Configuration
- Group ID: `test-5pdfs-1767429340223041632` (stored in `last_test_group_id.txt`)
- Feature flags: All Route 3 enhancements enabled
- Neo4j indices: entity, chunk, fulltext all operational
- No syntax errors, all queries executing successfully

---

## Next Steps / Recommendations

### To Reach 100% Theme Coverage

**Option 1: Improve Retrieval Precision**
- Investigate why chunks with specific terms (e.g., "60 days written notice") are not being retrieved
- Check Neo4j fulltext index tokenization and matching behavior
- Consider adding BM25 boosting for exact phrase matches
- Review chunk size/overlap during indexing

**Option 2: Expand Evidence Context**
- Increase `top_k` for chunk retrieval to cast wider net
- Add sliding window around matched entities to capture surrounding context
- Implement multi-hop chunk expansion based on document structure

**Option 3: Query Decomposition**
- For thematic questions, decompose into sub-queries targeting specific expected terms
- Aggregate evidence from multiple sub-queries before synthesis

**Constraint:** Must remain **route-level improvements** - no document/question-specific tuning allowed per user requirement.

---

## References

- **Baseline benchmark:** `benchmarks/route3_global_search_20260108T125835Z.md`
- **Regression benchmark:** `benchmarks/route3_global_search_20260108T182142Z.md`
- **Thematic benchmark:** `benchmarks/route3_thematic_20260108T190712Z.json`
- **Regression branch:** `regression-attempt-jan8` (commit `b5a91be`)
- **Previous handover:** Check git history for earlier handover documents

---

## Summary

Current system achieves **94.2% theme coverage** with 7/10 questions at 100%. The remaining 6% gap is due to **retrieval not finding chunks containing specific expected phrases**, not a synthesis or extraction issue. Today's improvement attempts resulted in regression and were reverted. The baseline commit `1dcf16b` represents the best known configuration.

**Recommendation:** Focus on improving retrieval precision for specific term matching rather than synthesis/orchestration changes.
